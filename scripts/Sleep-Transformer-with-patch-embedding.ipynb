{"cells":[{"cell_type":"code","source":["import os\n","import zipfile\n","import urllib.request\n","import numpy as np\n","import torch\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# Directory where datasets will be downloaded and extracted\n","DATA_DIR = 'datasets'\n","\n","# Ensure the dataset directory exists\n","os.makedirs(DATA_DIR, exist_ok=True)\n","\n","def download_dataset(dataset_name, url):\n","    \"\"\"\n","    Downloads and extracts a zip file containing the dataset.\n","    \"\"\"\n","    zip_path = os.path.join(DATA_DIR, f\"{dataset_name}.zip\")\n","    extract_path = os.path.join(DATA_DIR, dataset_name)\n","\n","    # Download the dataset\n","    print(f\"Downloading {dataset_name} from {url}...\")\n","    urllib.request.urlretrieve(url, zip_path)\n","\n","    # Extract the zip file\n","    print(f\"Extracting {dataset_name}...\")\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_path)\n","\n","    # Remove the zip file after extraction\n","    os.remove(zip_path)\n","    print(f\"Dataset {dataset_name} extracted to {extract_path}.\")\n","    return extract_path\n","\n","# Function to load the .ts file\n","def load_ts_file(file_path):\n","    data = []\n","    labels = []\n","    with open(file_path, 'r') as file:\n","        is_metadata = True\n","        for line in file:\n","            line = line.strip()\n","            if is_metadata:\n","                if line.lower() == \"@data\":\n","                    is_metadata = False\n","                continue\n","            if len(line) > 0:\n","                try:\n","                    series, label = line.split(':')\n","                    series = np.array(series.split(','), dtype=np.float32)\n","                    data.append(series)\n","                    labels.append(int(label))  # Convert the label to an integer\n","                except ValueError:\n","                    print(f\"Skipping invalid line: {line}\")\n","                    continue\n","    return np.array(data), np.array(labels)\n","\n","# Dataset information\n","dataset_name = 'Sleep'\n","dataset_url = 'https://timeseriesclassification.com/aeon-toolkit/Sleep.zip'\n","\n","# Download and extract the dataset\n","extract_path = download_dataset(dataset_name, dataset_url)\n","\n","# Paths to the .ts files\n","train_data_path = os.path.join(extract_path, 'Sleep_TRAIN.ts')\n","test_data_path = os.path.join(extract_path, 'Sleep_TEST.ts')\n","\n","# Load the train and test datasets\n","X_train, y_train = load_ts_file(train_data_path)\n","X_test, y_test = load_ts_file(test_data_path)\n","\n","# Normalize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Reshape the features to have a third dimension (samples, time_steps, 1)\n","X_train_scaled = X_train_scaled.reshape(-1, X_train_scaled.shape[1], 1)\n","X_test_scaled = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)\n","\n","# Split the test data into validation and test sets\n","X_valid_scaled, X_test_scaled, y_valid, y_test = train_test_split(X_test_scaled, y_test, test_size=0.50, random_state=42)\n","\n","# Convert data to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n","\n","X_valid_tensor = torch.tensor(X_valid_scaled, dtype=torch.float32)\n","y_valid_tensor = torch.tensor(y_valid, dtype=torch.int64)\n","\n","X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n","\n","# Create DataLoaders\n","batch_size = 64\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Calculate the number of unique classes\n","n_classes = len(np.unique(y_train))\n","\n","# Print the shapes and number of classes\n","print(f\"Number of classes: {n_classes}\")\n","print(f\"X_train shape: {X_train_tensor.shape}, y_train shape: {y_train_tensor.shape}\")\n","print(f\"X_valid shape: {X_valid_tensor.shape}, y_valid shape: {y_valid_tensor.shape}\")\n","print(f\"X_test shape: {X_test_tensor.shape}, y_test shape: {y_test_tensor.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K--i86wv_lY8","executionInfo":{"status":"ok","timestamp":1728493635170,"user_tz":300,"elapsed":42393,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"3e8f22de-b4d6-40c2-e2d2-ce4852b8f70b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading Sleep from https://timeseriesclassification.com/aeon-toolkit/Sleep.zip...\n","Extracting Sleep...\n","Dataset Sleep extracted to datasets/Sleep.\n","Number of classes: 5\n","X_train shape: torch.Size([478785, 178, 1]), y_train shape: torch.Size([478785])\n","X_valid shape: torch.Size([45157, 178, 1]), y_valid shape: torch.Size([45157])\n","X_test shape: torch.Size([45158, 178, 1]), y_test shape: torch.Size([45158])\n"]}]},{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NDT8m0OYWpQj","executionInfo":{"status":"ok","timestamp":1728494195135,"user_tz":300,"elapsed":7297,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"7f68506a-4c1b-44a5-ea52-d4d33e69fd15"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QAP9J-zwWMbt","executionInfo":{"status":"ok","timestamp":1728494195135,"user_tz":300,"elapsed":4,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import classification_report, confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchinfo import summary\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"j1tFm3BmWMbu","executionInfo":{"status":"ok","timestamp":1728494195135,"user_tz":300,"elapsed":3,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}}},"outputs":[],"source":["import math\n","\n","class TimeSeriesPatchEmbeddingLayer(nn.Module):\n","    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps):\n","        super().__init__()\n","        self.patch_size = patch_size\n","        self.embedding_dim = embedding_dim\n","        self.in_channels = in_channels\n","\n","        # Calculate the number of patches, adjusting for padding if necessary\n","        # Ceiling division to account for padding\n","        self.num_patches = -(-input_timesteps // patch_size)\n","        self.padding = (\n","            self.num_patches * patch_size\n","        ) - input_timesteps  # Calculate padding length\n","\n","        self.conv_layer = nn.Conv1d(\n","            in_channels=in_channels,\n","            out_channels=embedding_dim,\n","            kernel_size=patch_size,\n","            stride=patch_size,\n","        )\n","\n","        self.class_token_embeddings = nn.Parameter(\n","            torch.randn((1, 1, embedding_dim), requires_grad=True)\n","        )\n","        self.position_embeddings = PositionalEncoding(embedding_dim, dropout=0.1, max_len=input_timesteps)\n","\n","    def forward(self, x):\n","        # Pad the input sequence if necessary\n","        if self.padding > 0:\n","            x = nn.functional.pad(x, (0, 0, 0, self.padding))  # Pad the second to last dimension, which is input_timesteps\n","\n","        # We use a Conv1d layer to generate the patch embeddings\n","        x = x.permute(0, 2, 1)  # (batch, features, timesteps)\n","        conv_output = self.conv_layer(x)\n","        conv_output = conv_output.permute(0, 2, 1)  # (batch, timesteps, features)\n","\n","        batch_size = x.shape[0]\n","        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n","        output = torch.cat((class_tokens, conv_output), dim=1)\n","\n","        output = self.position_embeddings(output)\n","\n","        return output\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def pos_encoding(self, q_len, d_model, normalize=True):\n","        pe = torch.zeros(q_len, d_model)\n","        position = torch.arange(0, q_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        if normalize:\n","            pe = pe - pe.mean()\n","            pe = pe / (pe.std() * 10)\n","        return pe\n","\n","    def forward(self, x):\n","        x = x + self.pos_encoding(q_len = x.size(1), d_model = x.size(2))\n","        return self.dropout(x)\n","\n","class LearnedPositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(LearnedPositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1), :]\n","        return self.dropout(x)\n","\n","\n","def get_pos_encoder(pos_encoding):\n","    if pos_encoding == 'fixed':\n","        return PositionalEncoding\n","    elif pos_encoding == 'learned':\n","        return LearnedPositionalEncoding\n","    else:\n","        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d8MKozUTWMbu","executionInfo":{"status":"ok","timestamp":1728494195650,"user_tz":300,"elapsed":518,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"4590e712-f46f-4688-c370-61c2b55e03e9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["=================================================================================================================================================\n","Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n","=================================================================================================================================================\n","TimeSeriesPatchEmbeddingLayer (TimeSeriesPatchEmbeddingLayer)     [64, 178, 1]         [64, 24, 8]          8                    True\n","├─Conv1d (conv_layer)                                             [64, 1, 184]         [64, 8, 23]          72                   True\n","├─PositionalEncoding (position_embeddings)                        [64, 24, 8]          [64, 24, 8]          --                   --\n","│    └─Dropout (dropout)                                          [64, 24, 8]          [64, 24, 8]          --                   --\n","=================================================================================================================================================\n","Total params: 80\n","Trainable params: 80\n","Non-trainable params: 0\n","Total mult-adds (M): 0.11\n","=================================================================================================================================================\n","Input size (MB): 0.05\n","Forward/backward pass size (MB): 0.09\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 0.14\n","================================================================================================================================================="]},"metadata":{},"execution_count":6}],"source":["random_instances, random_labels = next(iter(train_loader))\n","random_instance = random_instances[0]\n","\n","BATCH_SIZE = random_instances.shape[0]\n","TIMESTEPS = random_instance.shape[0]\n","CHANNELS = random_instance.shape[1]\n","PATCH_SIZE = 8\n","\n","patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n","    in_channels=CHANNELS,\n","    patch_size=PATCH_SIZE,\n","    embedding_dim=CHANNELS * PATCH_SIZE,\n","    input_timesteps=TIMESTEPS,\n",")\n","\n","patch_embeddings = patch_embedding_layer(random_instances)\n","patch_embeddings.shape\n","\n","summary(\n","    model=patch_embedding_layer,\n","    # (batch_size, input_channels, input_timesteps)\n","    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n","    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","    col_width=20,\n","    row_settings=[\"var_names\"],\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"s30BKFOOWMbu","executionInfo":{"status":"ok","timestamp":1728494195896,"user_tz":300,"elapsed":1,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}}},"outputs":[],"source":["from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","class TimeSeriesTransformer(nn.Module):\n","    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2):\n","        super().__init__()\n","\n","        # Embedding layer\n","        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps)\n","\n","        # Calculate the number of patches\n","        self.num_patches = -(-input_timesteps // patch_size)\n","\n","        # Transformer Encoder\n","        # Setting batch_first=True to accommodate inputs with batch dimension first\n","        encoder_layers = TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n","        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n","\n","        # Feedforward layer\n","        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n","        # Classifier Head\n","        self.classifier = nn.Linear(dim_feedforward, num_classes)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, in_channels, input_timesteps)\n","\n","        # Get patch embeddings\n","        x = self.patch_embedding(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n","\n","        # Apply Transformer Encoder with batch first\n","        x = self.transformer_encoder(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n","\n","        # Use the output corresponding to the class token for classification\n","        class_token_output = x[:, 0, :]  # Select the class token for each item in the batch\n","\n","        # Feedforward layer\n","        x = self.ff_layer(class_token_output)\n","\n","        # Classifier head\n","        output = self.classifier(x)  # Output shape: (batch_size, num_classes)\n","\n","        return output\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3tGq9kmWMbu","executionInfo":{"status":"ok","timestamp":1728494202287,"user_tz":300,"elapsed":4066,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"b38ecef8-9a32-48bc-837c-e2a56eada27d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["=======================================================================================================================================\n","Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n","=======================================================================================================================================\n","TimeSeriesTransformer (TimeSeriesTransformer)           [64, 178, 1]         [64, 5]              --                   True\n","├─TimeSeriesPatchEmbeddingLayer (patch_embedding)       [64, 178, 1]         [64, 24, 32]         32                   True\n","│    └─Conv1d (conv_layer)                              [64, 1, 184]         [64, 32, 23]         288                  True\n","│    └─PositionalEncoding (position_embeddings)         [64, 24, 32]         [64, 24, 32]         --                   --\n","│    │    └─Dropout (dropout)                           [64, 24, 32]         [64, 24, 32]         --                   --\n","├─TransformerEncoder (transformer_encoder)              [64, 24, 32]         [64, 24, 32]         --                   True\n","│    └─ModuleList (layers)                              --                   --                   --                   True\n","│    │    └─TransformerEncoderLayer (0)                 [64, 24, 32]         [64, 24, 32]         12,704               True\n","│    │    └─TransformerEncoderLayer (1)                 [64, 24, 32]         [64, 24, 32]         12,704               True\n","│    │    └─TransformerEncoderLayer (2)                 [64, 24, 32]         [64, 24, 32]         12,704               True\n","│    │    └─TransformerEncoderLayer (3)                 [64, 24, 32]         [64, 24, 32]         12,704               True\n","├─Linear (ff_layer)                                     [64, 32]             [64, 128]            4,224                True\n","├─Linear (classifier)                                   [64, 128]            [64, 5]              645                  True\n","=======================================================================================================================================\n","Total params: 56,005\n","Trainable params: 56,005\n","Non-trainable params: 0\n","Total mult-adds (M): 0.74\n","=======================================================================================================================================\n","Input size (MB): 0.05\n","Forward/backward pass size (MB): 0.44\n","Params size (MB): 0.02\n","Estimated Total Size (MB): 0.51\n","======================================================================================================================================="]},"metadata":{},"execution_count":8}],"source":["# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Instantiate the model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=8,\n","    embedding_dim=32,\n","    num_transformer_layers=4,\n","    num_heads=4,\n","    dim_feedforward=128,\n","    dropout=0.2,\n","    num_classes=n_classes,\n",").to(device)\n","\n","# Define the loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Define the learning rate scheduler to reduce the learning rate by the specified step size and factor (gamma) every step_size epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n","\n","summary(\n","    model=model,\n","    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n","    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","    col_width=20,\n","    row_settings=[\"var_names\"],\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Smirk3qsWMbu","outputId":"604ae2d2-0b65-4d66-e3f2-1bbf1ee17072","executionInfo":{"status":"ok","timestamp":1728501950877,"user_tz":300,"elapsed":1592458,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: New best model saved with validation accuracy: 0.5805\n","Epoch 1, Train Loss: 0.9566, Train Acc: 0.6116, Val Loss: 0.9858, Val Acc: 0.5805\n","Epoch 2: New best model saved with validation accuracy: 0.5893\n","Epoch 2, Train Loss: 0.9040, Train Acc: 0.6340, Val Loss: 0.9620, Val Acc: 0.5893\n","Epoch 3: New best model saved with validation accuracy: 0.6169\n","Epoch 3, Train Loss: 0.8917, Train Acc: 0.6389, Val Loss: 0.9212, Val Acc: 0.6169\n","Epoch 4, Train Loss: 0.8838, Train Acc: 0.6417, Val Loss: 0.9261, Val Acc: 0.6160\n","Epoch 5, Train Loss: 0.8776, Train Acc: 0.6445, Val Loss: 0.9816, Val Acc: 0.5789\n","Epoch 6: New best model saved with validation accuracy: 0.6254\n","Epoch 6, Train Loss: 0.8725, Train Acc: 0.6461, Val Loss: 0.9022, Val Acc: 0.6254\n","Epoch 7: New best model saved with validation accuracy: 0.6260\n","Epoch 7, Train Loss: 0.8685, Train Acc: 0.6480, Val Loss: 0.9057, Val Acc: 0.6260\n","Epoch 8, Train Loss: 0.8660, Train Acc: 0.6489, Val Loss: 0.9422, Val Acc: 0.6000\n","Epoch 9, Train Loss: 0.8629, Train Acc: 0.6506, Val Loss: 0.9014, Val Acc: 0.6213\n","Epoch 10, Train Loss: 0.8608, Train Acc: 0.6511, Val Loss: 0.9115, Val Acc: 0.6192\n","Epoch 11, Train Loss: 0.8596, Train Acc: 0.6520, Val Loss: 0.8999, Val Acc: 0.6220\n","Epoch 12, Train Loss: 0.8575, Train Acc: 0.6525, Val Loss: 0.9149, Val Acc: 0.6146\n","Epoch 13: New best model saved with validation accuracy: 0.6304\n","Epoch 13, Train Loss: 0.8566, Train Acc: 0.6524, Val Loss: 0.8866, Val Acc: 0.6304\n","Epoch 14, Train Loss: 0.8544, Train Acc: 0.6536, Val Loss: 0.9001, Val Acc: 0.6252\n","Epoch 15: New best model saved with validation accuracy: 0.6324\n","Epoch 15, Train Loss: 0.8532, Train Acc: 0.6549, Val Loss: 0.8914, Val Acc: 0.6324\n","Epoch 16, Train Loss: 0.8522, Train Acc: 0.6546, Val Loss: 0.9013, Val Acc: 0.6281\n","Epoch 17, Train Loss: 0.8511, Train Acc: 0.6551, Val Loss: 0.9183, Val Acc: 0.6195\n","Epoch 18: New best model saved with validation accuracy: 0.6351\n","Epoch 18, Train Loss: 0.8501, Train Acc: 0.6559, Val Loss: 0.8887, Val Acc: 0.6351\n","Epoch 19: New best model saved with validation accuracy: 0.6373\n","Epoch 19, Train Loss: 0.8498, Train Acc: 0.6552, Val Loss: 0.8810, Val Acc: 0.6373\n","Epoch 20, Train Loss: 0.8481, Train Acc: 0.6561, Val Loss: 0.9022, Val Acc: 0.6207\n","Loaded best model for testing or further use.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-9c1bc2205eaa>:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(best_model_path))\n"]}],"source":["# Model, loss function, and optimizer\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","# Number of epochs\n","n_epochs = 20\n","\n","# Initialize variables for tracking the best model\n","best_validation_acc = 0.0\n","best_model_path = 'best_model_v2.pth'\n","\n","for epoch in range(n_epochs):\n","    model.train()  # Set model to training mode\n","    train_losses = []\n","    train_correct = 0\n","    total = 0\n","\n","    # Training loop\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()  # Zero the parameter gradients\n","\n","        predictions = model(inputs)  # Forward pass\n","        loss = criterion(predictions, labels)  # Calculate loss\n","\n","        loss.backward()  # Backward pass\n","        optimizer.step()  # Optimize\n","\n","        train_losses.append(loss.item())\n","\n","        # Count the number of correct predictions\n","        train_correct += (predictions.argmax(1) == labels).sum().item()\n","        total += labels.size(0)\n","\n","    train_loss = np.mean(train_losses)\n","    train_acc = train_correct / total\n","\n","    # Validation loop\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        validation_losses = []\n","        validation_correct = 0\n","        total_val = 0\n","\n","        for inputs, labels in valid_loader:\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            validation_losses.append(loss.item())\n","\n","            validation_correct += (predictions.argmax(1) == labels).sum().item()\n","            total_val += labels.size(0)\n","\n","        validation_loss = np.mean(validation_losses)\n","        validation_acc = validation_correct / total_val\n","\n","    # Check if this is the best model so far\n","    if validation_acc > best_validation_acc:\n","        best_validation_acc = validation_acc\n","        # Save the model\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {validation_acc:.4f}')\n","\n","    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {validation_loss:.4f}, Val Acc: {validation_acc:.4f}')\n","\n","# Loading the best model\n","model.load_state_dict(torch.load(best_model_path))\n","print('Loaded best model for testing or further use.')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iyotm-hrWMbv","executionInfo":{"status":"ok","timestamp":1728347451844,"user_tz":300,"elapsed":270,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"056351af-ae5b-4758-e0e9-d600322f8004"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.97      0.97      0.97       119\n","           1       1.00      0.70      0.83       135\n","           2       0.87      0.70      0.78       115\n","           3       0.95      0.93      0.94       112\n","           4       0.97      0.92      0.94       123\n","           5       0.98      0.96      0.97       128\n","           6       0.94      0.99      0.96       121\n","           7       0.83      0.62      0.71       102\n","           8       0.48      0.93      0.63        92\n","           9       0.92      0.95      0.93       113\n","\n","    accuracy                           0.87      1160\n","   macro avg       0.89      0.87      0.87      1160\n","weighted avg       0.90      0.87      0.87      1160\n","\n","Confusion matrix:\n","[[116   0   0   1   1   0   0   0   0   1]\n"," [  0  95   5   0   0   0   0   0  35   0]\n"," [  1   0  81   0   0   1   1  10  21   0]\n"," [  0   0   0 104   0   0   0   0   0   8]\n"," [  0   0   2   0 113   1   7   0   0   0]\n"," [  2   0   0   0   3 123   0   0   0   0]\n"," [  1   0   0   0   0   0 120   0   0   0]\n"," [  0   0   2   0   0   0   0  63  37   0]\n"," [  0   0   3   0   0   0   0   3  86   0]\n"," [  0   0   0   5   0   1   0   0   0 107]]\n"]}],"source":["# Prediction\n","model.eval()\n","with torch.no_grad():\n","    Y_pred_prob = model(X_test)\n","\n","Y_pred = Y_pred_prob.argmax(1)\n","\n","print(classification_report(y_test, Y_pred))\n","confusion = confusion_matrix(y_test, Y_pred)\n","print(f\"Confusion matrix:\\n{confusion}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"LhQeXQWnWMbv","executionInfo":{"status":"ok","timestamp":1728347482017,"user_tz":300,"elapsed":1901,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"0ff22672-51d9-4f55-fab2-fe90b5291a21"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# Visualize the confusion matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n","plt.xlabel(\"Prediction\")\n","plt.ylabel(\"Target\")\n","plt.title(\"Confusion Matrix\")\n","plt.show()"]},{"cell_type":"markdown","source":["## Positional Encodings"],"metadata":{"id":"PHiMwsYvVAzp"}},{"cell_type":"code","source":["!pip install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4rwyM_sG-KTN","executionInfo":{"status":"ok","timestamp":1728347495941,"user_tz":300,"elapsed":6234,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"577c74a3-5f3f-4101-af74-6a5ddab06417"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from einops import rearrange\n","import pandas as pd\n","\n","class Attention(nn.Module):\n","    def __init__(self, emb_size, num_heads, dropout):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.scale = emb_size ** -0.5\n","        self.key = nn.Linear(emb_size, emb_size, bias=False)\n","        self.value = nn.Linear(emb_size, emb_size, bias=False)\n","        self.query = nn.Linear(emb_size, emb_size, bias=False)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_out = nn.LayerNorm(emb_size)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.shape\n","        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","\n","        attn = torch.matmul(q, k) * self.scale\n","        attn = nn.functional.softmax(attn, dim=-1)\n","\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2)\n","        out = out.reshape(batch_size, seq_len, -1)\n","        out = self.to_out(out)\n","        return out\n","\n","\n","class Attention_Rel_Scl(nn.Module):\n","    def __init__(self, emb_size, num_heads, seq_len, dropout):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.num_heads = num_heads\n","        self.scale = emb_size ** -0.5\n","        self.key = nn.Linear(emb_size, emb_size, bias=False)\n","        self.value = nn.Linear(emb_size, emb_size, bias=False)\n","        self.query = nn.Linear(emb_size, emb_size, bias=False)\n","\n","        self.relative_bias_table = nn.Parameter(torch.zeros((2 * self.seq_len - 1), num_heads))\n","        coords = torch.meshgrid((torch.arange(1), torch.arange(self.seq_len)))\n","        coords = torch.flatten(torch.stack(coords), 1)\n","        relative_coords = coords[:, :, None] - coords[:, None, :]\n","        relative_coords[1] += self.seq_len - 1\n","        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n","        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n","        self.register_buffer(\"relative_index\", relative_index)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_out = nn.LayerNorm(emb_size)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.shape\n","        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","\n","        attn = torch.matmul(q, k) * self.scale\n","        attn = nn.functional.softmax(attn, dim=-1)\n","\n","        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, 8))\n","        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=1 * self.seq_len, w=1 * self.seq_len)\n","        attn = attn + relative_bias\n","\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2)\n","        out = out.reshape(batch_size, seq_len, -1)\n","        out = self.to_out(out)\n","        return out\n","\n","\n","class Attention_Rel_Vec(nn.Module):\n","    def __init__(self, emb_size, num_heads, seq_len, dropout):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.num_heads = num_heads\n","        self.scale = emb_size ** -0.5\n","        self.key = nn.Linear(emb_size, emb_size, bias=False)\n","        self.value = nn.Linear(emb_size, emb_size, bias=False)\n","        self.query = nn.Linear(emb_size, emb_size, bias=False)\n","\n","        self.Er = nn.Parameter(torch.randn(self.seq_len, int(emb_size / num_heads)))\n","\n","        self.register_buffer(\n","            \"mask\",\n","            torch.tril(torch.ones(self.seq_len, self.seq_len))\n","                .unsqueeze(0).unsqueeze(0)\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_out = nn.LayerNorm(emb_size)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.shape\n","        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","\n","        QEr = torch.matmul(q, self.Er.transpose(0, 1))\n","        Srel = self.skew(QEr)\n","\n","        attn = torch.matmul(q, k)\n","        attn = (attn + Srel) * self.scale\n","\n","        attn = nn.functional.softmax(attn, dim=-1)\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2)\n","        out = out.reshape(batch_size, seq_len, -1)\n","        out = self.to_out(out)\n","        return out\n","\n","    def skew(self, QEr):\n","        padded = nn.functional.pad(QEr, (1, 0))\n","        batch_size, num_heads, num_rows, num_cols = padded.shape\n","        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n","        Srel = reshaped[:, :, 1:, :]\n","        return Srel\n"],"metadata":{"id":"yD7K5F3D-Hji"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TUPEConfig:\n","    num_layers: int = 6\n","    num_heads: int = 8\n","    d_model: int = 128\n","    d_head: int = 0\n","    max_len: int = 256\n","    dropout: float = 0.1\n","    expansion_factor: int = 1\n","    relative_bias: bool = True\n","    bidirectional_bias: bool = True\n","    num_buckets: int = 32\n","    max_distance: int = 128\n","\n","    def __post_init__(self):\n","        d_head, remainder = divmod(self.d_model, self.num_heads)\n","        assert remainder == 0, \"`d_model` should be divisible by `num_heads`\"\n","        self.d_head = d_head"],"metadata":{"id":"B2e6n_Yj_38p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _get_relative_position_bucket(\n","    relative_position, bidirectional, num_buckets, max_distance\n","):\n","    \"\"\"\n","    from https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/modeling_t5.py\n","    \"\"\"\n","    relative_buckets = 0\n","    if bidirectional:\n","        num_buckets //= 2\n","        relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n","        relative_position = torch.abs(relative_position)\n","    else:\n","        relative_position = -torch.min(\n","            relative_position, torch.zeros_like(relative_position)\n","        )\n","    # now relative_position is in the range [0, inf)\n","\n","    # half of the buckets are for exact increments in positions\n","    max_exact = num_buckets // 2\n","    is_small = relative_position < max_exact\n","\n","    # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n","    relative_postion_if_large = max_exact + (\n","        torch.log(relative_position.float() / max_exact)\n","        / math.log(max_distance / max_exact)\n","        * (num_buckets - max_exact)\n","    ).to(torch.long)\n","    relative_postion_if_large = torch.min(\n","        relative_postion_if_large,\n","        torch.full_like(relative_postion_if_large, num_buckets - 1),\n","    )\n","\n","    relative_buckets += torch.where(\n","        is_small, relative_position, relative_postion_if_large\n","    )\n","    return relative_buckets\n","\n","\n","def get_relative_positions(\n","    seq_len, bidirectional=True, num_buckets=32, max_distance=128\n","):\n","    x = torch.arange(seq_len)[None, :]\n","    y = torch.arange(seq_len)[:, None]\n","    relative_positions = _get_relative_position_bucket(\n","        x - y, bidirectional, num_buckets, max_distance\n","    )\n","    return relative_positions"],"metadata":{"id":"1QbW7ihvBByl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TUPEMultiHeadAttention(nn.Module):\n","    def __init__(self, config: TUPEConfig, pos_embed: nn.Module) -> None:\n","        super().__init__()\n","        self.max_len = config.max_len\n","        self.num_heads = config.num_heads\n","        self.num_buckets = config.num_buckets\n","        self.max_distance = config.max_distance\n","        self.bidirectional = config.bidirectional_bias\n","        self.scale = math.sqrt(2 * config.d_head)\n","\n","        self.pos_embed = pos_embed\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","        # kqv in one pass\n","        self.pos_kq = nn.Linear(config.d_model, 2 * config.d_model, bias=False)\n","        self.tok_kqv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n","\n","        self.relative_bias = config.relative_bias\n","        if config.relative_bias:\n","            self.bias = nn.Embedding(config.max_len * 2, config.num_heads)\n","\n","    def forward(self, x: torch.tensor) -> torch.tensor:\n","        batch_size, seq_len, _ = x.shape\n","\n","        pos_embed = self.pos_embed(seq_len).repeat(batch_size, 1, 1)\n","        # pos_embed.shape == (batch_size, seq_len, d_model)\n","        pos_key, pos_query = self.pos_kq(pos_embed).chunk(2, dim=-1)\n","        pos_key = pos_key.view(batch_size, seq_len, self.num_heads, -1).permute(\n","            0, 2, 3, 1\n","        )\n","        # pos_key.shape == (batch_size, num_heads, d_head, seq_len)\n","        pos_query = pos_query.view(batch_size, seq_len, self.num_heads, -1).transpose(\n","            1, 2\n","        )\n","        # pos_query.shape == (batch_size, num_heads, seq_len, d_head)\n","        pos_attn = torch.matmul(pos_query, pos_key)\n","        # pos_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n","\n","        tok_key, tok_query, tok_value = self.tok_kqv(x).chunk(3, dim=-1)\n","        tok_key = tok_key.view(batch_size, seq_len, self.num_heads, -1).permute(\n","            0, 2, 3, 1\n","        )\n","        # tok_key.shape == (batch_size, num_heads, d_head, seq_len)\n","        tok_query = tok_query.view(batch_size, seq_len, self.num_heads, -1).transpose(\n","            1, 2\n","        )\n","        tok_value = tok_value.view(batch_size, seq_len, self.num_heads, -1).transpose(\n","            1, 2\n","        )\n","        # tok_qv.shape == (batch_size, num_heads, seq_len, d_head)\n","        tok_attn = torch.matmul(tok_query, tok_key)\n","        # tok_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n","\n","        attn = (tok_attn + pos_attn) / self.scale\n","        if self.relative_bias:\n","            relative_positions = get_relative_positions(\n","                seq_len, self.bidirectional, self.num_buckets, self.max_distance\n","            ).to(attn.device)\n","            # relative_positions.shape == (seq_len, seq_len)\n","            bias = self.bias(relative_positions + self.max_len)\n","            # bias.shape == (seq_len, seq_len, num_heads)\n","            bias = bias.permute(2, 0, 1).unsqueeze(0)\n","            # bias.shape == (1, num_heads, seq_len, seq_len)\n","            attn = attn + bias\n","\n","        attn = F.softmax(attn, dim=-1)\n","        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n","        out = torch.matmul(attn, tok_value)\n","        # out.shape == (batch_size, num_heads, seq_len, d_head)\n","        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n","        # out.shape == (batch_size, seq_len, d_model)\n","        out = self.dropout(out)\n","        return out"],"metadata":{"id":"9uW0SNXu_oda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","import math\n","from torchsummary import summary\n","\n","# Positional Encoding\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def pos_encoding(self, q_len, d_model, normalize=True):\n","        pe = torch.zeros(q_len, d_model)\n","        position = torch.arange(0, q_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        if normalize:\n","            pe = pe - pe.mean()\n","            pe = pe / (pe.std() * 10)\n","        return pe\n","\n","    def forward(self, x):\n","        x = x + self.pos_encoding(q_len=x.size(1), d_model=x.size(2))\n","        return self.dropout(x)\n","\n","# Learned Positional Encoding\n","class LearnedPositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(LearnedPositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1), :]\n","        return self.dropout(x)\n","\n","# Time Series Patch Embedding Layer\n","class TimeSeriesPatchEmbeddingLayer(nn.Module):\n","    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps, pos_encoding='fixed'):\n","        super().__init__()\n","        self.patch_size = patch_size\n","        self.embedding_dim = embedding_dim\n","        self.in_channels = in_channels\n","\n","        self.num_patches = -(-input_timesteps // patch_size)\n","        self.padding = (self.num_patches * patch_size) - input_timesteps\n","\n","        self.conv_layer = nn.Conv1d(\n","            in_channels=in_channels,\n","            out_channels=embedding_dim,\n","            kernel_size=patch_size,\n","            stride=patch_size,\n","        )\n","\n","        self.class_token_embeddings = nn.Parameter(\n","            torch.randn((1, 1, embedding_dim), requires_grad=True)\n","        )\n","\n","        # Instantiate the positional encoding\n","        pos_encoder_class = get_pos_encoder(pos_encoding)\n","        self.position_embeddings = pos_encoder_class(embedding_dim, dropout=0.1, max_len=input_timesteps)\n","\n","    def forward(self, x):\n","        if self.padding > 0:\n","            x = nn.functional.pad(x, (0, 0, 0, self.padding))\n","\n","        x = x.permute(0, 2, 1)\n","        conv_output = self.conv_layer(x)\n","        conv_output = conv_output.permute(0, 2, 1)\n","\n","        batch_size = x.shape[0]\n","        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n","        output = torch.cat((class_tokens, conv_output), dim=1)\n","\n","        output = self.position_embeddings(output)\n","\n","        return output\n","\n","# Get Positional Encoding\n","def get_pos_encoder(pos_encoding):\n","    if pos_encoding == 'fixed':\n","        return PositionalEncoding\n","    elif pos_encoding == 'learned':\n","        return LearnedPositionalEncoding\n","    else:\n","        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n","\n","class TimeSeriesTransformer(nn.Module):\n","    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, pos_encoding='fixed', num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2, attention_type='absolute'):\n","        super().__init__()\n","\n","        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps, pos_encoding)\n","        self.num_patches = -(-input_timesteps // patch_size)\n","\n","        if attention_type == 'relative_scl':\n","            self.attention_layer = Attention_Rel_Scl(embedding_dim, num_heads, self.num_patches + 1, dropout)\n","        elif attention_type == 'relative_vec':\n","            self.attention_layer = Attention_Rel_Vec(embedding_dim, num_heads, self.num_patches + 1, dropout)\n","        elif attention_type == 'tupe':\n","            self.attention_layer = TUPEMultiHeadAttention(tupe_config, pos_embed)\n","        else:\n","            self.attention_layer = Attention(embedding_dim, num_heads, dropout)\n","\n","        encoder_layers = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n","\n","        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n","        self.classifier = nn.Linear(dim_feedforward, num_classes)\n","\n","    def forward(self, x):\n","        x = self.patch_embedding(x)\n","        x = self.attention_layer(x)\n","        x = self.transformer_encoder(x)\n","        class_token_output = x[:, 0, :]\n","        x = self.ff_layer(class_token_output)\n","        output = self.classifier(x)\n","        return output\n","\n","\n","random_instances, random_labels = next(iter(train_loader))\n","random_instance = random_instances[0]\n","\n","BATCH_SIZE = random_instances.shape[0]\n","TIMESTEPS = random_instance.shape[0]\n","CHANNELS = random_instance.shape[1]\n","PATCH_SIZE = 8\n","\n","patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n","    in_channels=CHANNELS,\n","    patch_size=PATCH_SIZE,\n","    embedding_dim=CHANNELS * PATCH_SIZE,\n","    input_timesteps=TIMESTEPS,\n",")\n","\n","patch_embeddings = patch_embedding_layer(random_instances)\n","patch_embeddings.shape\n","\n","# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Instantiate the model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=8,\n","    embedding_dim=32,\n","    pos_encoding='fixed',\n","    num_transformer_layers=4,\n","    num_heads=4,\n","    dim_feedforward=128,\n","    dropout=0.2,\n","    num_classes=n_classes,\n",").to(device)\n","\n","# Define the loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Define the learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n","\n","\n","# Training Function\n","def train_model(model, train_loader, criterion, optimizer, num_epochs):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        train_losses = []\n","        train_correct = 0\n","        total = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_losses.append(loss.item())\n","            train_correct += (predictions.argmax(1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","        train_loss = np.mean(train_losses)\n","        train_acc = train_correct / total\n","\n","        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n","\n","# Evaluation Function\n","def evaluate_model(model, valid_loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total_val = 0\n","    with torch.no_grad():\n","        for inputs, labels in valid_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            total_loss += loss.item()\n","            correct += (predictions.argmax(1) == labels).sum().item()\n","            total_val += labels.size(0)\n","    val_loss = total_loss / len(valid_loader)\n","    val_acc = correct / total_val\n","    return val_loss, val_acc\n","\n","# Train the model\n","n_epochs = 10\n","best_validation_acc = 0.0\n","best_model_path = 'best_model_v2.pth'\n","\n","for epoch in range(n_epochs):\n","    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n","    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n","\n","    if val_acc > best_validation_acc:\n","        best_validation_acc = val_acc\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n","\n","    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n","    lr_scheduler.step()\n","\n","# Load the best model\n","model.load_state_dict(torch.load(best_model_path))\n","print('Loaded best model for testing or further use.')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oeNs_BKxfx0i","executionInfo":{"status":"ok","timestamp":1728347527695,"user_tz":300,"elapsed":23733,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"02e657bc-cb62-44ac-d671-04f600d1169e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 1.9255, Train Acc: 0.2426\n","Epoch 1: New best model saved with validation accuracy: 0.3325\n","Epoch 1, Val Loss: 1.5751, Val Acc: 0.3325\n","Epoch 1, Train Loss: 1.4607, Train Acc: 0.4237\n","Epoch 2: New best model saved with validation accuracy: 0.5217\n","Epoch 2, Val Loss: 1.1375, Val Acc: 0.5217\n","Epoch 1, Train Loss: 1.1830, Train Acc: 0.5037\n","Epoch 3: New best model saved with validation accuracy: 0.5807\n","Epoch 3, Val Loss: 0.9852, Val Acc: 0.5807\n","Epoch 1, Train Loss: 1.0641, Train Acc: 0.5322\n","Epoch 4, Val Loss: 0.9009, Val Acc: 0.5642\n","Epoch 1, Train Loss: 0.9565, Train Acc: 0.5643\n","Epoch 5: New best model saved with validation accuracy: 0.6363\n","Epoch 5, Val Loss: 0.8227, Val Acc: 0.6363\n","Epoch 1, Train Loss: 0.8865, Train Acc: 0.6149\n","Epoch 6: New best model saved with validation accuracy: 0.6710\n","Epoch 6, Val Loss: 0.7431, Val Acc: 0.6710\n","Epoch 1, Train Loss: 0.8600, Train Acc: 0.6388\n","Epoch 7, Val Loss: 0.7420, Val Acc: 0.6710\n","Epoch 1, Train Loss: 0.7943, Train Acc: 0.6811\n","Epoch 8: New best model saved with validation accuracy: 0.6892\n","Epoch 8, Val Loss: 0.7008, Val Acc: 0.6892\n","Epoch 1, Train Loss: 0.7695, Train Acc: 0.6746\n","Epoch 9: New best model saved with validation accuracy: 0.7283\n","Epoch 9, Val Loss: 0.6600, Val Acc: 0.7283\n","Epoch 1, Train Loss: 0.7355, Train Acc: 0.6875\n","Epoch 10, Val Loss: 0.6638, Val Acc: 0.7161\n","Loaded best model for testing or further use.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-b06036bd6b7c>:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(best_model_path))\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from einops import rearrange\n","import pandas as pd\n","\n","class Attention(nn.Module):\n","    def __init__(self, emb_size, num_heads, dropout):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.scale = emb_size ** -0.5\n","        self.key = nn.Linear(emb_size, emb_size, bias=False)\n","        self.value = nn.Linear(emb_size, emb_size, bias=False)\n","        self.query = nn.Linear(emb_size, emb_size, bias=False)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_out = nn.LayerNorm(emb_size)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.shape\n","        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","\n","        attn = torch.matmul(q, k) * self.scale\n","        attn = nn.functional.softmax(attn, dim=-1)\n","\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2)\n","        out = out.reshape(batch_size, seq_len, -1)\n","        out = self.to_out(out)\n","        return out\n","\n","\n","class Attention_Rel_Scl(nn.Module):\n","    def __init__(self, emb_size, num_heads, seq_len, dropout):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.num_heads = num_heads\n","        self.scale = emb_size ** -0.5\n","        self.key = nn.Linear(emb_size, emb_size, bias=False)\n","        self.value = nn.Linear(emb_size, emb_size, bias=False)\n","        self.query = nn.Linear(emb_size, emb_size, bias=False)\n","\n","        self.relative_bias_table = nn.Parameter(torch.zeros((2 * self.seq_len - 1), num_heads))\n","        coords = torch.meshgrid((torch.arange(1), torch.arange(self.seq_len)))\n","        coords = torch.flatten(torch.stack(coords), 1)\n","        relative_coords = coords[:, :, None] - coords[:, None, :]\n","        relative_coords[1] += self.seq_len - 1\n","        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n","        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n","        self.register_buffer(\"relative_index\", relative_index)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_out = nn.LayerNorm(emb_size)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.shape\n","        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","\n","        attn = torch.matmul(q, k) * self.scale\n","        attn = nn.functional.softmax(attn, dim=-1)\n","\n","        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, 8))\n","        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=1 * self.seq_len, w=1 * self.seq_len)\n","        attn = attn + relative_bias\n","\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2)\n","        out = out.reshape(batch_size, seq_len, -1)\n","        out = self.to_out(out)\n","        return out\n","\n","\n","class Attention_Rel_Vec(nn.Module):\n","    def __init__(self, emb_size, num_heads, seq_len, dropout):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.num_heads = num_heads\n","        self.scale = emb_size ** -0.5\n","        self.key = nn.Linear(emb_size, emb_size, bias=False)\n","        self.value = nn.Linear(emb_size, emb_size, bias=False)\n","        self.query = nn.Linear(emb_size, emb_size, bias=False)\n","\n","        self.Er = nn.Parameter(torch.randn(self.seq_len, int(emb_size / num_heads)))\n","\n","        self.register_buffer(\n","            \"mask\",\n","            torch.tril(torch.ones(self.seq_len, self.seq_len))\n","                .unsqueeze(0).unsqueeze(0)\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_out = nn.LayerNorm(emb_size)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.shape\n","        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","\n","        QEr = torch.matmul(q, self.Er.transpose(0, 1))\n","        Srel = self.skew(QEr)\n","\n","        attn = torch.matmul(q, k)\n","        attn = (attn + Srel) * self.scale\n","\n","        attn = nn.functional.softmax(attn, dim=-1)\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2)\n","        out = out.reshape(batch_size, seq_len, -1)\n","        out = self.to_out(out)\n","        return out\n","\n","    def skew(self, QEr):\n","        padded = nn.functional.pad(QEr, (1, 0))\n","        batch_size, num_heads, num_rows, num_cols = padded.shape\n","        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n","        Srel = reshaped[:, :, 1:, :]\n","        return Srel\n"],"metadata":{"id":"ESOXdCAqaM_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class tAPE(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n","        super(tAPE, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin((position * div_term) * (d_model / max_len))\n","        pe[:, 1::2] = torch.cos((position * div_term) * (d_model / max_len))\n","        pe = scale_factor * pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return self.dropout(x)\n","\n","class AbsolutePositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n","        super(AbsolutePositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = scale_factor * pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return self.dropout(x)\n","\n","class LearnablePositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=1024):\n","        super(LearnablePositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.pe = nn.Parameter(torch.empty(max_len, d_model))\n","        nn.init.uniform_(self.pe, -0.02, 0.02)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(1), :]\n","        return self.dropout(x)"],"metadata":{"id":"dplhsyvE-jR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_pos_encoder(pos_encoding):\n","    if pos_encoding == 'fixed':\n","        return PositionalEncoding\n","    elif pos_encoding == 'learned':\n","        return LearnedPositionalEncoding\n","    elif pos_encoding == 'tAPE':\n","        return tAPE\n","    elif pos_encoding == 'absolute':\n","        return AbsolutePositionalEncoding\n","    else:\n","        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n"],"metadata":{"id":"q9sb14F6-kc8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Instantiate the model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=8,\n","    embedding_dim=32,\n","    pos_encoding='tAPE',\n","    num_transformer_layers=4,\n","    num_heads=4,\n","    dim_feedforward=128,\n","    dropout=0.2,\n","    num_classes=n_classes,\n",").to(device)\n","\n","# Define the loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Define the learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n","\n","\n","# Training Function\n","def train_model(model, train_loader, criterion, optimizer, num_epochs):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        train_losses = []\n","        train_correct = 0\n","        total = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_losses.append(loss.item())\n","            train_correct += (predictions.argmax(1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","        train_loss = np.mean(train_losses)\n","        train_acc = train_correct / total\n","\n","        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n","\n","# Evaluation Function\n","def evaluate_model(model, valid_loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total_val = 0\n","    with torch.no_grad():\n","        for inputs, labels in valid_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            total_loss += loss.item()\n","            correct += (predictions.argmax(1) == labels).sum().item()\n","            total_val += labels.size(0)\n","    val_loss = total_loss / len(valid_loader)\n","    val_acc = correct / total_val\n","    return val_loss, val_acc\n","\n","# Train the model\n","n_epochs = 20\n","best_validation_acc = 0.0\n","best_model_path = 'best_model_v2.pth'\n","\n","for epoch in range(n_epochs):\n","    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n","    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n","\n","    if val_acc > best_validation_acc:\n","        best_validation_acc = val_acc\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n","\n","    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n","    lr_scheduler.step()\n","\n","# Load the best model\n","model.load_state_dict(torch.load(best_model_path))\n","print('Loaded best model for testing or further use.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"egoVdYVX-mq4","executionInfo":{"status":"ok","timestamp":1728347589042,"user_tz":300,"elapsed":25405,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"27f5fbd3-15f3-4537-ad18-9cf11fe6416e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 1.9269, Train Acc: 0.2325\n","Epoch 1: New best model saved with validation accuracy: 0.3524\n","Epoch 1, Val Loss: 1.5592, Val Acc: 0.3524\n","Epoch 1, Train Loss: 1.4799, Train Acc: 0.3915\n","Epoch 2: New best model saved with validation accuracy: 0.4653\n","Epoch 2, Val Loss: 1.1581, Val Acc: 0.4653\n","Epoch 1, Train Loss: 1.2186, Train Acc: 0.4586\n","Epoch 3: New best model saved with validation accuracy: 0.5295\n","Epoch 3, Val Loss: 1.0682, Val Acc: 0.5295\n","Epoch 1, Train Loss: 1.1287, Train Acc: 0.5046\n","Epoch 4: New best model saved with validation accuracy: 0.5755\n","Epoch 4, Val Loss: 0.9466, Val Acc: 0.5755\n","Epoch 1, Train Loss: 1.0560, Train Acc: 0.5395\n","Epoch 5: New best model saved with validation accuracy: 0.6458\n","Epoch 5, Val Loss: 0.9033, Val Acc: 0.6458\n","Epoch 1, Train Loss: 1.0232, Train Acc: 0.5395\n","Epoch 6, Val Loss: 0.9294, Val Acc: 0.5599\n","Epoch 1, Train Loss: 0.9887, Train Acc: 0.5680\n","Epoch 7, Val Loss: 0.8137, Val Acc: 0.6372\n","Epoch 1, Train Loss: 0.9161, Train Acc: 0.5873\n","Epoch 8: New best model saved with validation accuracy: 0.6675\n","Epoch 8, Val Loss: 0.7566, Val Acc: 0.6675\n","Epoch 1, Train Loss: 0.8509, Train Acc: 0.6250\n","Epoch 9: New best model saved with validation accuracy: 0.6780\n","Epoch 9, Val Loss: 0.7762, Val Acc: 0.6780\n","Epoch 1, Train Loss: 0.8470, Train Acc: 0.6057\n","Epoch 10: New best model saved with validation accuracy: 0.7092\n","Epoch 10, Val Loss: 0.7083, Val Acc: 0.7092\n","Epoch 1, Train Loss: 0.8121, Train Acc: 0.6388\n","Epoch 11, Val Loss: 0.6937, Val Acc: 0.7075\n","Epoch 1, Train Loss: 0.8033, Train Acc: 0.6434\n","Epoch 12: New best model saved with validation accuracy: 0.7283\n","Epoch 12, Val Loss: 0.6618, Val Acc: 0.7283\n","Epoch 1, Train Loss: 0.7607, Train Acc: 0.6682\n","Epoch 13: New best model saved with validation accuracy: 0.7951\n","Epoch 13, Val Loss: 0.6069, Val Acc: 0.7951\n","Epoch 1, Train Loss: 0.7212, Train Acc: 0.7059\n","Epoch 14, Val Loss: 0.6156, Val Acc: 0.7648\n","Epoch 1, Train Loss: 0.7027, Train Acc: 0.7123\n","Epoch 15, Val Loss: 0.5822, Val Acc: 0.7665\n","Epoch 1, Train Loss: 0.6584, Train Acc: 0.7233\n","Epoch 16, Val Loss: 0.5812, Val Acc: 0.7804\n","Epoch 1, Train Loss: 0.6334, Train Acc: 0.7482\n","Epoch 17: New best model saved with validation accuracy: 0.8073\n","Epoch 17, Val Loss: 0.5371, Val Acc: 0.8073\n","Epoch 1, Train Loss: 0.6490, Train Acc: 0.7362\n","Epoch 18, Val Loss: 0.5346, Val Acc: 0.7804\n","Epoch 1, Train Loss: 0.6309, Train Acc: 0.7399\n","Epoch 19, Val Loss: 0.5276, Val Acc: 0.7977\n","Epoch 1, Train Loss: 0.6354, Train Acc: 0.7482\n","Epoch 20, Val Loss: 0.5366, Val Acc: 0.7969\n","Loaded best model for testing or further use.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-25-f4ea5e1fc41b>:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(best_model_path))\n"]}]},{"cell_type":"code","source":["# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Instantiate the model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=8,\n","    embedding_dim=32,\n","    pos_encoding='fixed',\n","    num_transformer_layers=4,\n","    num_heads=4,\n","    dim_feedforward=128,\n","    dropout=0.2,\n","    num_classes=n_classes,\n","    attention_type= 'relative_vec',\n",").to(device)\n","\n","# Define the loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Define the learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n","\n","\n","# Training Function\n","def train_model(model, train_loader, criterion, optimizer, num_epochs):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        train_losses = []\n","        train_correct = 0\n","        total = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_losses.append(loss.item())\n","            train_correct += (predictions.argmax(1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","        train_loss = np.mean(train_losses)\n","        train_acc = train_correct / total\n","\n","        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n","\n","# Evaluation Function\n","def evaluate_model(model, valid_loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total_val = 0\n","    with torch.no_grad():\n","        for inputs, labels in valid_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            total_loss += loss.item()\n","            correct += (predictions.argmax(1) == labels).sum().item()\n","            total_val += labels.size(0)\n","    val_loss = total_loss / len(valid_loader)\n","    val_acc = correct / total_val\n","    return val_loss, val_acc\n","\n","# Train the model\n","n_epochs = 20\n","best_validation_acc = 0.0\n","best_model_path = 'best_model_v2.pth'\n","\n","for epoch in range(n_epochs):\n","    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n","    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n","\n","    if val_acc > best_validation_acc:\n","        best_validation_acc = val_acc\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n","\n","    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n","    lr_scheduler.step()\n","\n","# Load the best model\n","model.load_state_dict(torch.load(best_model_path))\n","print('Loaded best model for testing or further use.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xzFfH3xX8fE_","executionInfo":{"status":"ok","timestamp":1728347623426,"user_tz":300,"elapsed":19968,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"e44e26d0-0f3f-4cd7-e706-bd0af595893a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 1.9290, Train Acc: 0.2914\n","Epoch 1: New best model saved with validation accuracy: 0.4809\n","Epoch 1, Val Loss: 1.4344, Val Acc: 0.4809\n","Epoch 1, Train Loss: 1.3325, Train Acc: 0.4761\n","Epoch 2: New best model saved with validation accuracy: 0.5903\n","Epoch 2, Val Loss: 1.0358, Val Acc: 0.5903\n","Epoch 1, Train Loss: 1.0950, Train Acc: 0.5331\n","Epoch 3, Val Loss: 0.9206, Val Acc: 0.5738\n","Epoch 1, Train Loss: 0.9819, Train Acc: 0.5616\n","Epoch 4: New best model saved with validation accuracy: 0.6354\n","Epoch 4, Val Loss: 0.8263, Val Acc: 0.6354\n","Epoch 1, Train Loss: 0.9236, Train Acc: 0.5827\n","Epoch 5: New best model saved with validation accuracy: 0.6684\n","Epoch 5, Val Loss: 0.7913, Val Acc: 0.6684\n","Epoch 1, Train Loss: 0.9676, Train Acc: 0.5699\n","Epoch 6, Val Loss: 0.8856, Val Acc: 0.6293\n","Epoch 1, Train Loss: 0.8946, Train Acc: 0.6029\n","Epoch 7, Val Loss: 0.7779, Val Acc: 0.6510\n","Epoch 1, Train Loss: 0.7967, Train Acc: 0.6590\n","Epoch 8: New best model saved with validation accuracy: 0.7057\n","Epoch 8, Val Loss: 0.7459, Val Acc: 0.7057\n","Epoch 1, Train Loss: 0.7880, Train Acc: 0.6599\n","Epoch 9: New best model saved with validation accuracy: 0.7274\n","Epoch 9, Val Loss: 0.6565, Val Acc: 0.7274\n","Epoch 1, Train Loss: 0.7672, Train Acc: 0.6765\n","Epoch 10: New best model saved with validation accuracy: 0.7535\n","Epoch 10, Val Loss: 0.6356, Val Acc: 0.7535\n","Epoch 1, Train Loss: 0.7026, Train Acc: 0.7077\n","Epoch 11: New best model saved with validation accuracy: 0.7613\n","Epoch 11, Val Loss: 0.5963, Val Acc: 0.7613\n","Epoch 1, Train Loss: 0.6782, Train Acc: 0.7114\n","Epoch 12: New best model saved with validation accuracy: 0.7648\n","Epoch 12, Val Loss: 0.5770, Val Acc: 0.7648\n","Epoch 1, Train Loss: 0.6394, Train Acc: 0.7353\n","Epoch 13, Val Loss: 0.5721, Val Acc: 0.7578\n","Epoch 1, Train Loss: 0.6549, Train Acc: 0.7206\n","Epoch 14: New best model saved with validation accuracy: 0.8003\n","Epoch 14, Val Loss: 0.5349, Val Acc: 0.8003\n","Epoch 1, Train Loss: 0.5836, Train Acc: 0.7684\n","Epoch 15: New best model saved with validation accuracy: 0.8186\n","Epoch 15, Val Loss: 0.4945, Val Acc: 0.8186\n","Epoch 1, Train Loss: 0.5653, Train Acc: 0.7794\n","Epoch 16, Val Loss: 0.4824, Val Acc: 0.8177\n","Epoch 1, Train Loss: 0.5622, Train Acc: 0.7831\n","Epoch 17: New best model saved with validation accuracy: 0.8342\n","Epoch 17, Val Loss: 0.4566, Val Acc: 0.8342\n","Epoch 1, Train Loss: 0.5448, Train Acc: 0.7904\n","Epoch 18: New best model saved with validation accuracy: 0.8394\n","Epoch 18, Val Loss: 0.4539, Val Acc: 0.8394\n","Epoch 1, Train Loss: 0.5520, Train Acc: 0.7794\n","Epoch 19, Val Loss: 0.4474, Val Acc: 0.8186\n","Epoch 1, Train Loss: 0.5061, Train Acc: 0.8042\n","Epoch 20: New best model saved with validation accuracy: 0.8733\n","Epoch 20, Val Loss: 0.4131, Val Acc: 0.8733\n","Loaded best model for testing or further use.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-27-014626430eba>:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(best_model_path))\n"]}]},{"cell_type":"markdown","source":["### TUPE"],"metadata":{"id":"ntF6fok4KzMF"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","class TUPEConfig:\n","    def __init__(self):\n","        self.d_model = 128\n","        self.num_heads = 8\n","        self.dim_feedforward = 512\n","        self.dropout = 0.1\n","        self.max_len = 5000\n","        self.num_buckets = 32\n","        self.max_distance = 128\n","        self.relative_bias = True\n","        self.bidirectional_bias = True\n","\n","class TUPEMultiHeadAttention(nn.Module):\n","    def __init__(self, config: TUPEConfig, pos_embed: nn.Module) -> None:\n","        super().__init__()\n","        self.max_len = config.max_len\n","        self.num_heads = config.num_heads\n","        self.num_buckets = config.num_buckets\n","        self.max_distance = config.max_distance\n","        self.bidirectional = config.bidirectional_bias\n","        self.scale = math.sqrt(2 * config.d_model // config.num_heads)\n","\n","        self.pos_embed = pos_embed\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","        # kqv in one pass\n","        self.pos_kq = nn.Linear(config.d_model, 2 * config.d_model, bias=False)\n","        self.tok_kqv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n","\n","        self.relative_bias = config.relative_bias\n","        if config.relative_bias:\n","            self.bias = nn.Embedding(config.max_len * 2, config.num_heads)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        batch_size, seq_len, _ = x.shape\n","\n","        pos_embed = self.pos_embed(seq_len).repeat(batch_size, 1, 1)\n","        # pos_embed.shape == (batch_size, seq_len, d_model)\n","        pos_key, pos_query = self.pos_kq(pos_embed).chunk(2, dim=-1)\n","        pos_key = pos_key.view(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        # pos_key.shape == (batch_size, num_heads, d_head, seq_len)\n","        pos_query = pos_query.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        # pos_query.shape == (batch_size, num_heads, seq_len, d_head)\n","        pos_attn = torch.matmul(pos_query, pos_key)\n","        # pos_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n","\n","        tok_key, tok_query, tok_value = self.tok_kqv(x).chunk(3, dim=-1)\n","        tok_key = tok_key.view(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        # tok_key.shape == (batch_size, num_heads, d_head, seq_len)\n","        tok_query = tok_query.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        tok_value = tok_value.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        # tok_qv.shape == (batch_size, num_heads, seq_len, d_head)\n","        tok_attn = torch.matmul(tok_query, tok_key)\n","        # tok_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n","\n","        attn = (tok_attn + pos_attn) / self.scale\n","        if self.relative_bias:\n","            relative_positions = self.get_relative_positions(seq_len)\n","            # relative_positions.shape == (seq_len, seq_len)\n","            bias = self.bias(relative_positions + self.max_len)\n","            # bias.shape == (seq_len, seq_len, num_heads)\n","            bias = bias.permute(2, 0, 1).unsqueeze(0)\n","            # bias.shape == (1, num_heads, seq_len, seq_len)\n","            attn = attn + bias\n","\n","        attn = F.softmax(attn, dim=-1)\n","        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n","        out = torch.matmul(attn, tok_value)\n","        # out.shape == (batch_size, num_heads, seq_len, d_head)\n","        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n","        # out.shape == (batch_size, seq_len, d_model)\n","        out = self.dropout(out)\n","        return out\n","\n","    def get_relative_positions(self, seq_len):\n","        # Generate relative position encodings\n","        range_vec = torch.arange(seq_len)\n","        distance_mat = range_vec.unsqueeze(1) - range_vec.unsqueeze(0)\n","        distance_mat_clipped = torch.clamp(distance_mat, -self.max_distance, self.max_distance)\n","        final_mat = distance_mat_clipped + self.max_distance\n","        return final_mat\n"],"metadata":{"id":"LWu8jg32ORdz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FixedPositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(FixedPositionalEncoding, self).__init__()\n","        self.encoding = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        self.encoding[:, 0::2] = torch.sin(position * div_term)\n","        self.encoding[:, 1::2] = torch.cos(position * div_term)\n","        self.encoding = self.encoding.unsqueeze(0)  # (1, max_len, d_model)\n","\n","    def forward(self, seq_len):\n","        return self.encoding[:, :seq_len, :].detach()\n","\n","class TimeSeriesTransformer(nn.Module):\n","    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_layers, num_heads, dim_feedforward, dropout, num_classes, attention_type, pos_encoding):\n","        super(TimeSeriesTransformer, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.num_layers = num_layers\n","        self.patch_size = patch_size\n","        self.in_channels = in_channels\n","        self.num_classes = num_classes\n","        self.attention_type = attention_type\n","\n","        # Patch embedding\n","        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n","        self.pos_encoding = pos_encoding\n","\n","        # Transformer layers with TUPEMultiHeadAttention\n","        self.transformer_layers = nn.ModuleList([\n","            TUPEMultiHeadAttention(\n","                config=TUPEConfig(),\n","                pos_embed=FixedPositionalEncoding(embedding_dim, max_len=input_timesteps)\n","            )\n","            for _ in range(num_layers)\n","        ])\n","\n","        # Classification head\n","        self.classifier = nn.Linear(embedding_dim, num_classes)\n","\n","    def forward(self, x):\n","        #print(f\"Input to model: {x.shape}\")\n","\n","        # Patch embedding\n","        x = x.permute(0, 2, 1)  # Change shape from (batch_size, seq_len, in_channels) to (batch_size, in_channels, seq_len)\n","        #print(f\"Input to Conv1d: {x.shape}\")\n","\n","        x = self.patch_embedding(x)  # Apply Conv1d\n","        #print(f\"Patch embedding output: {x.shape}\")\n","\n","        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, embedding_dim)\n","        #print(f\"Patch embedding output permuted: {x.shape}\")\n","\n","        # Positional encoding\n","        seq_len = x.size(1)\n","        x = x + self.pos_encoding(seq_len).to(x.device)\n","        #print(f\"Positional encoding output: {x.shape}\")\n","\n","        # Transformer layers\n","        for layer in self.transformer_layers:\n","            x = layer(x)\n","            #print(f\"Transformer layer output: {x.shape}\")\n","\n","        # Classification head\n","        x = x.mean(dim=1)\n","        x = self.classifier(x)\n","        #print(f\"Classifier output: {x.shape}\")\n","        return x\n","\n","# Hyperparameters\n","TIMESTEPS = 24\n","CHANNELS = 1\n","n_classes = 10\n","\n","# Initialize positional encoding\n","pos_embed = FixedPositionalEncoding(d_model=128, max_len=TIMESTEPS)\n","\n","# Define the model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=8,\n","    embedding_dim=128,\n","    num_layers=4,\n","    num_heads=8,\n","    dim_feedforward=512,\n","    dropout=0.1,\n","    num_classes=n_classes,\n","    attention_type='tupe',\n","    pos_encoding=pos_embed,\n",")\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training and validation loop\n","num_epochs = 20\n","best_valid_acc = 0.0\n","\n","for epoch in range(num_epochs):\n","    # Training\n","    model.train()\n","    train_loss = 0.0\n","    train_preds = []\n","    train_labels = []\n","    for batch in train_loader:\n","        inputs, labels = batch\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * inputs.size(0)\n","        train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n","        train_labels.extend(labels.cpu().numpy())\n","\n","    train_loss /= len(train_loader.dataset)\n","    train_acc = accuracy_score(train_labels, train_preds)\n","\n","    # Validation\n","    model.eval()\n","    valid_loss = 0.0\n","    valid_preds = []\n","    valid_labels = []\n","    with torch.no_grad():\n","        for batch in valid_loader:\n","            inputs, labels = batch\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            valid_loss += loss.item() * inputs.size(0)\n","            valid_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n","            valid_labels.extend(labels.cpu().numpy())\n","\n","    valid_loss /= len(valid_loader.dataset)\n","    valid_acc = accuracy_score(valid_labels, valid_preds)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, '\n","          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n","          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')\n","\n","    # Save the best model\n","    if valid_acc > best_valid_acc:\n","        best_valid_acc = valid_acc\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TgP2Ge01OTt0","executionInfo":{"status":"ok","timestamp":1728347671708,"user_tz":300,"elapsed":20528,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"1c79888e-4210-4ffa-d9cc-859c661bed22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Train Loss: 1.8327, Train Acc: 0.2601, Valid Loss: 1.5799, Valid Acc: 0.3819\n","Epoch 2/20, Train Loss: 1.3760, Train Acc: 0.4504, Valid Loss: 1.1217, Valid Acc: 0.6033\n","Epoch 3/20, Train Loss: 0.9513, Train Acc: 0.6103, Valid Loss: 0.8274, Valid Acc: 0.7066\n","Epoch 4/20, Train Loss: 0.7981, Train Acc: 0.6765, Valid Loss: 0.7656, Valid Acc: 0.7578\n","Epoch 5/20, Train Loss: 0.6515, Train Acc: 0.7399, Valid Loss: 0.6160, Valid Acc: 0.8151\n","Epoch 6/20, Train Loss: 0.5620, Train Acc: 0.7877, Valid Loss: 0.5788, Valid Acc: 0.7934\n","Epoch 7/20, Train Loss: 0.4925, Train Acc: 0.8088, Valid Loss: 0.5045, Valid Acc: 0.7830\n","Epoch 8/20, Train Loss: 0.4555, Train Acc: 0.8263, Valid Loss: 0.5833, Valid Acc: 0.8212\n","Epoch 9/20, Train Loss: 0.4531, Train Acc: 0.8336, Valid Loss: 0.4634, Valid Acc: 0.8681\n","Epoch 10/20, Train Loss: 0.4103, Train Acc: 0.8465, Valid Loss: 0.4848, Valid Acc: 0.8941\n","Epoch 11/20, Train Loss: 0.4242, Train Acc: 0.8410, Valid Loss: 0.5150, Valid Acc: 0.8759\n","Epoch 12/20, Train Loss: 0.3761, Train Acc: 0.8557, Valid Loss: 0.4675, Valid Acc: 0.8637\n","Epoch 13/20, Train Loss: 0.3349, Train Acc: 0.8814, Valid Loss: 0.3422, Valid Acc: 0.8993\n","Epoch 14/20, Train Loss: 0.3096, Train Acc: 0.8980, Valid Loss: 0.3989, Valid Acc: 0.8950\n","Epoch 15/20, Train Loss: 0.3120, Train Acc: 0.8934, Valid Loss: 0.3390, Valid Acc: 0.8993\n","Epoch 16/20, Train Loss: 0.2635, Train Acc: 0.9017, Valid Loss: 0.3866, Valid Acc: 0.8837\n","Epoch 17/20, Train Loss: 0.2863, Train Acc: 0.8888, Valid Loss: 0.3943, Valid Acc: 0.8993\n","Epoch 18/20, Train Loss: 0.2427, Train Acc: 0.9127, Valid Loss: 0.4239, Valid Acc: 0.9080\n","Epoch 19/20, Train Loss: 0.2436, Train Acc: 0.9127, Valid Loss: 0.4458, Valid Acc: 0.8967\n","Epoch 20/20, Train Loss: 0.2474, Train Acc: 0.9053, Valid Loss: 0.4120, Valid Acc: 0.9097\n","Best Validation Accuracy: 0.9097\n"]}]},{"cell_type":"code","source":["class TimeSeriesTransformer(nn.Module):\n","    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_layers, num_heads, dim_feedforward, dropout, num_classes, pos_encoding_type, spe_params):\n","        super(TimeSeriesTransformer, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.num_layers = num_layers\n","        self.patch_size = patch_size\n","        self.in_channels = in_channels\n","        self.num_classes = num_classes\n","\n","        # Patch embedding\n","        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n","\n","        # Positional encoding\n","        if pos_encoding_type == 'sine':\n","            self.pos_encoding = SineSPE(num_heads=num_heads, in_features=embedding_dim // num_heads, **spe_params)\n","        elif pos_encoding_type == 'conv':\n","            self.pos_encoding = ConvSPE(ndim=1, num_heads=num_heads, in_features=embedding_dim // num_heads, **spe_params)\n","        else:\n","            raise ValueError(\"Invalid positional encoding type\")\n","\n","        # Transformer layers with TUPEMultiHeadAttention\n","        self.transformer_layers = nn.ModuleList([\n","            TUPEMultiHeadAttention(\n","                config=TUPEConfig(),\n","                pos_embed=self.pos_encoding\n","            )\n","            for _ in range(num_layers)\n","        ])\n","\n","        # Classification head\n","        self.classifier = nn.Linear(embedding_dim, num_classes)\n","\n","    def forward(self, x):\n","        # Patch embedding\n","        x = x.permute(0, 2, 1)  # Change shape from (batch_size, seq_len, in_channels) to (batch_size, in_channels, seq_len)\n","        x = self.patch_embedding(x)  # Apply Conv1d\n","        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, embedding_dim)\n","\n","        # Positional encoding\n","        seq_len = x.size(1)\n","        pos_enc = self.pos_encoding(seq_len).to(x.device)\n","        x = x + pos_enc\n","\n","        # Transformer layers\n","        for layer in self.transformer_layers:\n","            x = layer(x)\n","\n","        # Classification head\n","        x = x.mean(dim=1)\n","        x = self.classifier(x)\n","        return x\n"],"metadata":{"id":"pMTQ5_2nyjht"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SineSPE(nn.Module):\n","    def __init__(self, num_heads, in_features, max_len=512):\n","        super(SineSPE, self).__init__()\n","        self.num_heads = num_heads\n","        self.in_features = in_features\n","        self.max_len = max_len\n","        self.position = nn.Parameter(torch.zeros(1, max_len, in_features))\n","        self.register_buffer('sine', self._generate_sine_encoding())\n","\n","    def _generate_sine_encoding(self):\n","        position = torch.arange(self.max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, self.in_features, 2).float() * -(math.log(10000.0) / self.in_features))\n","        encoding = torch.zeros(self.max_len, self.in_features)\n","        encoding[:, 0::2] = torch.sin(position * div_term)\n","        encoding[:, 1::2] = torch.cos(position * div_term)\n","        return encoding\n","\n","    def forward(self, seq_len):\n","        return self.sine[:seq_len, :].unsqueeze(0).repeat(self.num_heads, 1, 1)\n","\n","class ConvSPE(nn.Module):\n","    def __init__(self, ndim, num_heads, in_features, kernel_size=7, padding=3):\n","        super(ConvSPE, self).__init__()\n","        self.conv = nn.Conv1d(in_features, in_features, kernel_size=kernel_size, padding=padding)\n","        self.num_heads = num_heads\n","\n","    def forward(self, seq_len):\n","        # Generate sinusoidal encoding\n","        x = torch.arange(seq_len).unsqueeze(0).float()\n","        x = x.permute(0, 2, 1)\n","        x = self.conv(x)\n","        x = x.squeeze(0).permute(1, 0).unsqueeze(0).repeat(self.num_heads, 1, 1)\n","        return x\n"],"metadata":{"id":"_qHLmzj7yl1i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","import math\n","\n","# Example of positional encoding classes\n","class SineSPE(nn.Module):\n","    def __init__(self, in_features, max_len=512):\n","        super(SineSPE, self).__init__()\n","        self.in_features = in_features\n","        self.max_len = max_len\n","        self.position = nn.Parameter(torch.zeros(1, max_len, in_features))\n","        self.register_buffer('sine', self._generate_sine_encoding())\n","\n","    def _generate_sine_encoding(self):\n","        position = torch.arange(self.max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, self.in_features, 2).float() * -(math.log(10000.0) / self.in_features))\n","        encoding = torch.zeros(self.max_len, self.in_features)\n","        encoding[:, 0::2] = torch.sin(position * div_term)\n","        encoding[:, 1::2] = torch.cos(position * div_term)\n","        return encoding\n","\n","    def forward(self, seq_len):\n","        return self.sine[:seq_len, :].unsqueeze(0)  # Shape: (1, seq_len, in_features)\n","\n","\n","class ConvSPE(nn.Module):\n","    def __init__(self, num_heads, in_features, kernel_size=3, num_realizations=1):\n","        super(ConvSPE, self).__init__()\n","        padding = kernel_size // 2  # This ensures that the output size matches the input size if stride=1\n","\n","        # Define a 1D convolutional layer\n","        self.conv = nn.Conv1d(in_features, in_features, kernel_size=kernel_size, padding=padding)\n","\n","        self.num_heads = num_heads\n","        self.in_features = in_features\n","        self.kernel_size = kernel_size\n","        self.num_realizations = num_realizations\n","\n","    def forward(self, x):\n","        # x should be of shape (batch_size, seq_len, in_features)\n","        x = x.permute(0, 2, 1)  # Change shape to (batch_size, in_features, seq_len)\n","        x = self.conv(x)  # Apply convolution\n","        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, in_features)\n","        return x\n","\n","\n","# Model Definition\n","class TimeSeriesTransformer(nn.Module):\n","    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2, pos_encoding_type='sine', spe_params={}):\n","        super(TimeSeriesTransformer, self).__init__()\n","\n","        # Embedding layer\n","        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n","\n","        # Positional encoding\n","        if pos_encoding_type == 'sineSPE':\n","            self.pos_encoding = SineSPE(embedding_dim, **spe_params)\n","        elif pos_encoding_type == 'convSPE':\n","            self.pos_encoding = ConvSPE(num_heads=num_heads, in_features=embedding_dim, **spe_params)\n","        else:\n","            raise ValueError(\"Invalid positional encoding type\")\n","\n","        # Calculate the number of patches\n","        self.num_patches = -(-input_timesteps // patch_size)  # Ceiling division\n","\n","        # Transformer Encoder\n","        encoder_layers = TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n","        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n","\n","        # Feedforward layer\n","        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n","        # Classifier Head\n","        self.classifier = nn.Linear(dim_feedforward, num_classes)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, in_channels, input_timesteps)\n","\n","        # Get patch embeddings\n","        x = x.permute(0, 2, 1)  # Change shape from (batch_size, in_channels, input_timesteps) to (batch_size, input_timesteps, in_channels)\n","        x = self.patch_embedding(x)  # Apply Conv1d\n","        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, num_patches, embedding_dim)\n","\n","        # Apply positional encoding\n","        seq_len = x.size(1)\n","        pos_enc = self.pos_encoding(seq_len).to(x.device)\n","        x = x + pos_enc\n","\n","        # Apply Transformer Encoder\n","        x = self.transformer_encoder(x)  # Output shape: (batch_size, num_patches, embedding_dim)\n","\n","        # Use the output corresponding to the class token for classification\n","        class_token_output = x[:, 0, :]  # Select the class token for each item in the batch\n","\n","        # Feedforward layer\n","        x = self.ff_layer(class_token_output)\n","\n","        # Classifier head\n","        output = self.classifier(x)  # Output shape: (batch_size, num_classes)\n","\n","        return output\n"],"metadata":{"id":"3dIJpcJ3ypDM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","TIMESTEPS = 24\n","CHANNELS = 1\n","patch_size = 7\n","embedding_dim = 128\n","num_transformer_layers = 6\n","num_heads = 8\n","dim_feedforward = 128\n","dropout = 0.1\n","num_classes = 10\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Instantiate model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=patch_size,\n","    embedding_dim=embedding_dim,\n","    num_transformer_layers=num_transformer_layers,\n","    num_heads=num_heads,\n","    dim_feedforward=dim_feedforward,\n","    dropout=dropout,\n","    num_classes=num_classes,\n","    pos_encoding_type='sineSPE',\n","    spe_params={'max_len': TIMESTEPS}\n",").to(device)\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","\n","# Training function\n","def train_epoch(model, data_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    for inputs, labels in data_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","\n","        # Compute loss\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","\n","        # Update parameters\n","        optimizer.step()\n","\n","        # Update metrics\n","        running_loss += loss.item() * inputs.size(0)\n","        _, predicted = torch.max(outputs, 1)\n","        all_preds.extend(predicted.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    train_loss = running_loss / len(data_loader.dataset)\n","    train_acc = accuracy_score(all_labels, all_preds)\n","\n","    return train_loss, train_acc\n","\n","# Evaluation function\n","def evaluate(model, data_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","\n","            # Compute loss\n","            loss = criterion(outputs, labels)\n","\n","            # Update metrics\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    valid_loss = running_loss / len(data_loader.dataset)\n","    valid_acc = accuracy_score(all_labels, all_preds)\n","\n","    return valid_loss, valid_acc\n","\n","# Training and Evaluation Loop\n","num_epochs = 20\n","best_valid_acc = 0.0\n","\n","for epoch in range(num_epochs):\n","    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n","    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion, device)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, '\n","          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n","          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n","\n","    # Save the best model\n","    if valid_accuracy > best_valid_acc:\n","        best_valid_acc = valid_accuracy\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlTvPqd6zvXw","executionInfo":{"status":"ok","timestamp":1728347765997,"user_tz":300,"elapsed":62792,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"3aa400ba-54a6-40a6-c6ef-035b46d8e958"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Train Loss: 2.0022, Train Acc: 0.1847, Valid Loss: 1.8602, Valid Acc: 0.2839\n","Epoch 2/20, Train Loss: 1.6899, Train Acc: 0.3952, Valid Loss: 1.5678, Valid Acc: 0.4705\n","Epoch 3/20, Train Loss: 1.4543, Train Acc: 0.5000, Valid Loss: 1.3297, Valid Acc: 0.5625\n","Epoch 4/20, Train Loss: 1.2327, Train Acc: 0.5818, Valid Loss: 1.0900, Valid Acc: 0.6493\n","Epoch 5/20, Train Loss: 1.0347, Train Acc: 0.6360, Valid Loss: 0.9045, Valid Acc: 0.7179\n","Epoch 6/20, Train Loss: 0.8834, Train Acc: 0.7004, Valid Loss: 0.8010, Valid Acc: 0.7613\n","Epoch 7/20, Train Loss: 0.7758, Train Acc: 0.7390, Valid Loss: 0.7021, Valid Acc: 0.7795\n","Epoch 8/20, Train Loss: 0.6909, Train Acc: 0.7693, Valid Loss: 0.6239, Valid Acc: 0.8099\n","Epoch 9/20, Train Loss: 0.6145, Train Acc: 0.8051, Valid Loss: 0.5577, Valid Acc: 0.8125\n","Epoch 10/20, Train Loss: 0.5784, Train Acc: 0.8006, Valid Loss: 0.5158, Valid Acc: 0.8203\n","Epoch 11/20, Train Loss: 0.5109, Train Acc: 0.8309, Valid Loss: 0.4609, Valid Acc: 0.8611\n","Epoch 12/20, Train Loss: 0.4765, Train Acc: 0.8364, Valid Loss: 0.4295, Valid Acc: 0.8533\n","Epoch 13/20, Train Loss: 0.4514, Train Acc: 0.8438, Valid Loss: 0.4086, Valid Acc: 0.8464\n","Epoch 14/20, Train Loss: 0.3990, Train Acc: 0.8695, Valid Loss: 0.3655, Valid Acc: 0.8941\n","Epoch 15/20, Train Loss: 0.3795, Train Acc: 0.8704, Valid Loss: 0.3426, Valid Acc: 0.9071\n","Epoch 16/20, Train Loss: 0.3554, Train Acc: 0.8943, Valid Loss: 0.3167, Valid Acc: 0.9010\n","Epoch 17/20, Train Loss: 0.3363, Train Acc: 0.8934, Valid Loss: 0.3097, Valid Acc: 0.8906\n","Epoch 18/20, Train Loss: 0.3040, Train Acc: 0.9007, Valid Loss: 0.2887, Valid Acc: 0.9045\n","Epoch 19/20, Train Loss: 0.2931, Train Acc: 0.9044, Valid Loss: 0.3661, Valid Acc: 0.8681\n","Epoch 20/20, Train Loss: 0.2928, Train Acc: 0.9072, Valid Loss: 0.2668, Valid Acc: 0.9280\n","Best Validation Accuracy: 0.9280\n"]}]},{"cell_type":"code","source":["# Hyperparameters\n","TIMESTEPS = 24\n","CHANNELS = 1\n","patch_size = 7\n","embedding_dim = 128\n","num_transformer_layers = 6\n","num_heads = 8\n","dim_feedforward = 128\n","dropout = 0.1\n","num_classes = 10\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Instantiate model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=patch_size,\n","    embedding_dim=embedding_dim,\n","    num_transformer_layers=num_transformer_layers,\n","    num_heads=num_heads,\n","    dim_feedforward=dim_feedforward,\n","    dropout=dropout,\n","    num_classes=num_classes,\n","    pos_encoding_type='sineSPE',\n","    spe_params={'max_len': TIMESTEPS}\n",").to(device)\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","\n","# Training function\n","def train_epoch(model, data_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    for inputs, labels in data_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","\n","        # Compute loss\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","\n","        # Update parameters\n","        optimizer.step()\n","\n","        # Update metrics\n","        running_loss += loss.item() * inputs.size(0)\n","        _, predicted = torch.max(outputs, 1)\n","        all_preds.extend(predicted.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    train_loss = running_loss / len(data_loader.dataset)\n","    train_acc = accuracy_score(all_labels, all_preds)\n","\n","    return train_loss, train_acc\n","\n","# Evaluation function\n","def evaluate(model, data_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","\n","            # Compute loss\n","            loss = criterion(outputs, labels)\n","\n","            # Update metrics\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    valid_loss = running_loss / len(data_loader.dataset)\n","    valid_acc = accuracy_score(all_labels, all_preds)\n","\n","    return valid_loss, valid_acc\n","\n","# Training and Evaluation Loop\n","num_epochs = 20  # Adjust as needed\n","best_valid_acc = 0.0\n","\n","for epoch in range(num_epochs):\n","    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n","    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion, device)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, '\n","          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n","          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n","\n","    # Save the best model\n","    if valid_accuracy > best_valid_acc:\n","        best_valid_acc = valid_accuracy\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34GXwW8DBnzG","executionInfo":{"status":"ok","timestamp":1728347814866,"user_tz":300,"elapsed":48873,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"e70df9ec-a98d-418d-e3a0-2cb677b51f76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Train Loss: 1.9433, Train Acc: 0.3107, Valid Loss: 1.7364, Valid Acc: 0.4531\n","Epoch 2/20, Train Loss: 1.5958, Train Acc: 0.4375, Valid Loss: 1.4550, Valid Acc: 0.5573\n","Epoch 3/20, Train Loss: 1.3454, Train Acc: 0.5726, Valid Loss: 1.1840, Valid Acc: 0.6311\n","Epoch 4/20, Train Loss: 1.1005, Train Acc: 0.6544, Valid Loss: 0.9502, Valid Acc: 0.7743\n","Epoch 5/20, Train Loss: 0.9147, Train Acc: 0.6994, Valid Loss: 0.8016, Valid Acc: 0.7578\n","Epoch 6/20, Train Loss: 0.7879, Train Acc: 0.7491, Valid Loss: 0.7036, Valid Acc: 0.7995\n","Epoch 7/20, Train Loss: 0.6997, Train Acc: 0.7748, Valid Loss: 0.6110, Valid Acc: 0.8403\n","Epoch 8/20, Train Loss: 0.6282, Train Acc: 0.7895, Valid Loss: 0.5490, Valid Acc: 0.8403\n","Epoch 9/20, Train Loss: 0.5542, Train Acc: 0.8162, Valid Loss: 0.5069, Valid Acc: 0.8672\n","Epoch 10/20, Train Loss: 0.5007, Train Acc: 0.8373, Valid Loss: 0.4464, Valid Acc: 0.8750\n","Epoch 11/20, Train Loss: 0.4677, Train Acc: 0.8493, Valid Loss: 0.4501, Valid Acc: 0.8411\n","Epoch 12/20, Train Loss: 0.4400, Train Acc: 0.8566, Valid Loss: 0.4041, Valid Acc: 0.8689\n","Epoch 13/20, Train Loss: 0.4287, Train Acc: 0.8621, Valid Loss: 0.3755, Valid Acc: 0.8932\n","Epoch 14/20, Train Loss: 0.3773, Train Acc: 0.8759, Valid Loss: 0.3436, Valid Acc: 0.8950\n","Epoch 15/20, Train Loss: 0.3520, Train Acc: 0.8952, Valid Loss: 0.3302, Valid Acc: 0.8898\n","Epoch 16/20, Train Loss: 0.3173, Train Acc: 0.9053, Valid Loss: 0.2922, Valid Acc: 0.9097\n","Epoch 17/20, Train Loss: 0.3188, Train Acc: 0.8952, Valid Loss: 0.3106, Valid Acc: 0.8889\n","Epoch 18/20, Train Loss: 0.3072, Train Acc: 0.8952, Valid Loss: 0.2750, Valid Acc: 0.9193\n","Epoch 19/20, Train Loss: 0.2769, Train Acc: 0.9145, Valid Loss: 0.2982, Valid Acc: 0.8976\n","Epoch 20/20, Train Loss: 0.2819, Train Acc: 0.9090, Valid Loss: 0.3076, Valid Acc: 0.8924\n","Best Validation Accuracy: 0.9193\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class TPS_SelfAttention_Author(nn.Module):\n","    def __init__(self, num_heads, model_dim, max_len, pow=2, LrEnb=0, LrMo=0, dropout=0.1):\n","        \"\"\"\n","        Initialize the TPS (Temporal Pseudo-Gaussian Self-Attention) based on the author's implementation.\n","        :param num_heads: Number of attention heads.\n","        :param model_dim: Dimensionality of the input and output.\n","        :param max_len: Maximum sequence length (used to compute temporal distances).\n","        :param pow: Controls the power applied to the Gaussian decay.\n","        :param LrEnb: Enables trainable adjustments for temporal distances.\n","        :param LrMo: Additional mode for learnable adjustments.\n","        :param dropout: Dropout rate for regularization.\n","        \"\"\"\n","        super(TPS_SelfAttention_Author, self).__init__()\n","        assert model_dim % num_heads == 0\n","        self.num_heads = num_heads\n","        self.model_dim = model_dim\n","        self.d_k = model_dim // num_heads\n","        self.pow = pow\n","        self.LrEnb = LrEnb\n","        self.LrMo = LrMo\n","        self.Max_Len = max_len\n","\n","        # Layers for multi-head attention and dropout\n","        self.attention = nn.MultiheadAttention(embed_dim=model_dim, num_heads=num_heads)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # Temporal distance using Gaussian weighting\n","        t = torch.arange(0, max_len, dtype=torch.float)\n","        t1 = t.repeat(max_len, 1)\n","        t2 = t1.permute([1, 0])\n","\n","        # Gaussian decay on temporal distances (dis1)\n","        if pow == 2:\n","            dis1 = torch.exp(-1 * torch.pow((t2 - t1), 2) / 2)\n","            self.dist = nn.Parameter(-1 * torch.pow((t2 - t1), 2) / 2, requires_grad=False)\n","        else:\n","            dis1 = torch.exp(-1 * torch.abs((t2 - t1)))\n","            self.dist = nn.Parameter(-1 * torch.abs((t2 - t1)), requires_grad=False)\n","\n","        if LrEnb:\n","            self.adj1 = nn.Parameter(dis1)  # Learnable temporal weighting\n","\n","        # Optional adjustment based on LrMo\n","        if LrMo == 1:\n","            self.FCgamma = nn.Linear(self.d_k, 1)\n","            torch.nn.init.xavier_uniform_(self.FCgamma.weight)\n","\n","    def forward(self, q, k, v, mask=None):\n","        \"\"\"\n","        Forward pass for TPS with temporal distance weighting.\n","        :param q: Query matrix.\n","        :param k: Key matrix.\n","        :param v: Value matrix.\n","        :param mask: Masking matrix for padding (optional).\n","        :return: Output of the attention mechanism with TPS.\n","        \"\"\"\n","        # Apply standard multi-head attention\n","        attn_output, attn_weights = self.attention(q, k, v, key_padding_mask=mask)\n","\n","        # Apply Gaussian-based temporal weighting\n","        if self.LrEnb:\n","            weighted_attn = attn_weights * self.adj1[:attn_weights.size(-2), :attn_weights.size(-1)]\n","        else:\n","            weighted_attn = attn_weights * self.dist[:attn_weights.size(-2), :attn_weights.size(-1)]\n","\n","        # Normalize attention scores\n","        weighted_attn = weighted_attn / weighted_attn.sum(dim=-1, keepdim=True)\n","\n","        # Apply dropout to the attention output\n","        output = torch.matmul(weighted_attn, v)\n","        output = self.dropout(output)\n","\n","        return output, weighted_attn\n"],"metadata":{"id":"50avDJ54bxBX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","TIMESTEPS = 24\n","CHANNELS = 1\n","patch_size = 7\n","embedding_dim = 128\n","num_transformer_layers = 6\n","num_heads = 8\n","dim_feedforward = 128\n","dropout = 0.1\n","num_classes = 10\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Instantiate model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=patch_size,\n","    embedding_dim=embedding_dim,\n","    num_transformer_layers=num_transformer_layers,\n","    num_heads=num_heads,\n","    dim_feedforward=dim_feedforward,\n","    dropout=dropout,\n","    num_classes=num_classes,\n","    pos_encoding_type='sineSPE',\n","    spe_params={'max_len': TIMESTEPS}\n",").to(device)\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","\n","# Training function\n","def train_epoch(model, data_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    for inputs, labels in data_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","\n","        # Compute loss\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","\n","        # Update parameters\n","        optimizer.step()\n","\n","        # Update metrics\n","        running_loss += loss.item() * inputs.size(0)\n","        _, predicted = torch.max(outputs, 1)\n","        all_preds.extend(predicted.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    train_loss = running_loss / len(data_loader.dataset)\n","    train_acc = accuracy_score(all_labels, all_preds)\n","\n","    return train_loss, train_acc\n","\n","# Evaluation function\n","def evaluate(model, data_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","\n","            # Compute loss\n","            loss = criterion(outputs, labels)\n","\n","            # Update metrics\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    valid_loss = running_loss / len(data_loader.dataset)\n","    valid_acc = accuracy_score(all_labels, all_preds)\n","\n","    return valid_loss, valid_acc\n","\n","# Training and Evaluation Loop\n","num_epochs = 20\n","best_valid_acc = 0.0\n","\n","for epoch in range(num_epochs):\n","    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n","    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion, device)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, '\n","          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n","          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n","\n","    # Save the best model\n","    if valid_accuracy > best_valid_acc:\n","        best_valid_acc = valid_accuracy\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Wu69Z0LcAeT","executionInfo":{"status":"ok","timestamp":1728347857727,"user_tz":300,"elapsed":42867,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"28c8507a-4047-460e-88ca-a1d524723645"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Train Loss: 1.9601, Train Acc: 0.3392, Valid Loss: 1.7414, Valid Acc: 0.4818\n","Epoch 2/20, Train Loss: 1.5631, Train Acc: 0.5037, Valid Loss: 1.3730, Valid Acc: 0.5764\n","Epoch 3/20, Train Loss: 1.2754, Train Acc: 0.5974, Valid Loss: 1.1000, Valid Acc: 0.6727\n","Epoch 4/20, Train Loss: 1.0718, Train Acc: 0.6094, Valid Loss: 0.9393, Valid Acc: 0.6641\n","Epoch 5/20, Train Loss: 0.9153, Train Acc: 0.6985, Valid Loss: 0.7915, Valid Acc: 0.7500\n","Epoch 6/20, Train Loss: 0.7842, Train Acc: 0.7445, Valid Loss: 0.6855, Valid Acc: 0.7700\n","Epoch 7/20, Train Loss: 0.6954, Train Acc: 0.7555, Valid Loss: 0.6152, Valid Acc: 0.7882\n","Epoch 8/20, Train Loss: 0.6185, Train Acc: 0.7812, Valid Loss: 0.5772, Valid Acc: 0.8394\n","Epoch 9/20, Train Loss: 0.5696, Train Acc: 0.7950, Valid Loss: 0.5191, Valid Acc: 0.8151\n","Epoch 10/20, Train Loss: 0.5270, Train Acc: 0.8088, Valid Loss: 0.4794, Valid Acc: 0.8498\n","Epoch 11/20, Train Loss: 0.4843, Train Acc: 0.8235, Valid Loss: 0.4587, Valid Acc: 0.8776\n","Epoch 12/20, Train Loss: 0.4500, Train Acc: 0.8281, Valid Loss: 0.4049, Valid Acc: 0.8802\n","Epoch 13/20, Train Loss: 0.4100, Train Acc: 0.8658, Valid Loss: 0.3713, Valid Acc: 0.8932\n","Epoch 14/20, Train Loss: 0.3768, Train Acc: 0.8860, Valid Loss: 0.3616, Valid Acc: 0.8811\n","Epoch 15/20, Train Loss: 0.3699, Train Acc: 0.8732, Valid Loss: 0.3358, Valid Acc: 0.8967\n","Epoch 16/20, Train Loss: 0.3459, Train Acc: 0.8934, Valid Loss: 0.3066, Valid Acc: 0.9132\n","Epoch 17/20, Train Loss: 0.3242, Train Acc: 0.8952, Valid Loss: 0.2927, Valid Acc: 0.9080\n","Epoch 18/20, Train Loss: 0.3139, Train Acc: 0.8851, Valid Loss: 0.2891, Valid Acc: 0.9132\n","Epoch 19/20, Train Loss: 0.2838, Train Acc: 0.9154, Valid Loss: 0.2700, Valid Acc: 0.9132\n","Epoch 20/20, Train Loss: 0.2960, Train Acc: 0.8989, Valid Loss: 0.2976, Valid Acc: 0.8880\n","Best Validation Accuracy: 0.9132\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import math\n","\n","class TemporalPositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=24):\n","        super(TemporalPositionalEncoding, self).__init__()\n","        self.d_model = d_model\n","        self.max_len = max_len\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        seq_len = x.size(1)\n","        return self.pe[:seq_len, :].unsqueeze(0).expand(x.size(0), -1, -1)\n"],"metadata":{"id":"qJUoBvFFpmdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TimeSeriesTransformer(nn.Module):\n","    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_layers, num_heads, dim_feedforward, dropout, num_classes):\n","        super(TimeSeriesTransformer, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.num_layers = num_layers\n","        self.patch_size = patch_size\n","        self.in_channels = in_channels\n","        self.num_classes = num_classes\n","\n","        # Patch embedding\n","        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n","\n","        # Temporal positional encoding (T-PE)\n","        self.pos_encoding = TemporalPositionalEncoding(d_model=embedding_dim, max_len=input_timesteps)\n","\n","        # Transformer encoder layers\n","        self.transformer_layers = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout),\n","            num_layers=num_layers\n","        )\n","\n","        # Classification head\n","        self.classifier = nn.Linear(embedding_dim, num_classes)\n","\n","    def forward(self, x):\n","        # Reshape input for Conv1D\n","        x = x.permute(0, 2, 1)  # Change shape to (batch_size, in_channels, seq_len)\n","\n","        # Apply patch embedding\n","        x = self.patch_embedding(x)  # (batch_size, embedding_dim, seq_len // patch_size)\n","        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len // patch_size, embedding_dim)\n","\n","        # Apply positional encoding\n","        x = x + self.pos_encoding(x)\n","\n","        # Apply transformer layers\n","        x = self.transformer_layers(x)\n","\n","        # Pool the sequence output and apply classifier\n","        x = x.mean(dim=1)\n","        return self.classifier(x)\n"],"metadata":{"id":"AWFtUGHIprCQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the temporal positional encoding\n","temporal_pos_embed = TemporalPositionalEncoding(d_model=128, max_len=24)  # Sequence length is 24 (from your data)\n","\n","# Initialize the model\n","model = TimeSeriesTransformer(\n","    input_timesteps=24,  # Sequence length of your data\n","    in_channels=1,  # Since it's univariate\n","    patch_size=8,  # This can be adjusted based on your experiments\n","    embedding_dim=128,\n","    num_layers=4,\n","    num_heads=8,\n","    dim_feedforward=512,\n","    dropout=0.1,\n","    num_classes=10  # According to your data\n",")\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wZUU3ZqrptrX","executionInfo":{"status":"ok","timestamp":1728347876934,"user_tz":300,"elapsed":21,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"30fd22f2-414a-4560-ee47-32ac7b115967"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}]},{"cell_type":"code","source":["# Training and validation loop\n","num_epochs = 20\n","best_valid_acc = 0.0\n","\n","for epoch in range(num_epochs):\n","    # Training\n","    model.train()\n","    train_loss = 0.0\n","    train_preds = []\n","    train_labels = []\n","    for batch in train_loader:\n","        inputs, labels = batch\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * inputs.size(0)\n","        train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n","        train_labels.extend(labels.cpu().numpy())\n","\n","    train_loss /= len(train_loader.dataset)\n","    train_acc = accuracy_score(train_labels, train_preds)\n","\n","    # Validation\n","    model.eval()\n","    valid_loss = 0.0\n","    valid_preds = []\n","    valid_labels = []\n","    with torch.no_grad():\n","        for batch in valid_loader:\n","            inputs, labels = batch\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            valid_loss += loss.item() * inputs.size(0)\n","            valid_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n","            valid_labels.extend(labels.cpu().numpy())\n","\n","    valid_loss /= len(valid_loader.dataset)\n","    valid_acc = accuracy_score(valid_labels, valid_preds)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, '\n","          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n","          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')\n","\n","    # Save the best model\n","    if valid_acc > best_valid_acc:\n","        best_valid_acc = valid_acc\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3MnRD8wqpwzi","executionInfo":{"status":"ok","timestamp":1728347899863,"user_tz":300,"elapsed":22947,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"357e185d-d9bb-421b-b8b3-b6d67e9ed667"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Train Loss: 1.7723, Train Acc: 0.2978, Valid Loss: 1.2832, Valid Acc: 0.4826\n","Epoch 2/20, Train Loss: 1.1556, Train Acc: 0.4871, Valid Loss: 1.1637, Valid Acc: 0.5113\n","Epoch 3/20, Train Loss: 0.9319, Train Acc: 0.6342, Valid Loss: 0.7642, Valid Acc: 0.7413\n","Epoch 4/20, Train Loss: 0.6392, Train Acc: 0.7721, Valid Loss: 0.5226, Valid Acc: 0.8464\n","Epoch 5/20, Train Loss: 0.4788, Train Acc: 0.8281, Valid Loss: 0.4207, Valid Acc: 0.8715\n","Epoch 6/20, Train Loss: 0.3844, Train Acc: 0.8722, Valid Loss: 0.3861, Valid Acc: 0.8811\n","Epoch 7/20, Train Loss: 0.3580, Train Acc: 0.8879, Valid Loss: 0.3323, Valid Acc: 0.8906\n","Epoch 8/20, Train Loss: 0.3088, Train Acc: 0.9035, Valid Loss: 0.3758, Valid Acc: 0.8663\n","Epoch 9/20, Train Loss: 0.3145, Train Acc: 0.8915, Valid Loss: 0.2616, Valid Acc: 0.9080\n","Epoch 10/20, Train Loss: 0.2350, Train Acc: 0.9237, Valid Loss: 0.2928, Valid Acc: 0.9062\n","Epoch 11/20, Train Loss: 0.2647, Train Acc: 0.9118, Valid Loss: 0.3333, Valid Acc: 0.8863\n","Epoch 12/20, Train Loss: 0.2478, Train Acc: 0.9118, Valid Loss: 0.2290, Valid Acc: 0.9201\n","Epoch 13/20, Train Loss: 0.2339, Train Acc: 0.9228, Valid Loss: 0.2139, Valid Acc: 0.9288\n","Epoch 14/20, Train Loss: 0.1957, Train Acc: 0.9347, Valid Loss: 0.1989, Valid Acc: 0.9210\n","Epoch 15/20, Train Loss: 0.1988, Train Acc: 0.9311, Valid Loss: 0.2387, Valid Acc: 0.9071\n","Epoch 16/20, Train Loss: 0.2110, Train Acc: 0.9219, Valid Loss: 0.2355, Valid Acc: 0.9262\n","Epoch 17/20, Train Loss: 0.1927, Train Acc: 0.9347, Valid Loss: 0.2356, Valid Acc: 0.9158\n","Epoch 18/20, Train Loss: 0.2001, Train Acc: 0.9283, Valid Loss: 0.1747, Valid Acc: 0.9384\n","Epoch 19/20, Train Loss: 0.1902, Train Acc: 0.9292, Valid Loss: 0.1880, Valid Acc: 0.9410\n","Epoch 20/20, Train Loss: 0.1720, Train Acc: 0.9421, Valid Loss: 0.2155, Valid Acc: 0.9149\n","Best Validation Accuracy: 0.9410\n"]}]}],"metadata":{"kernelspec":{"display_name":"ml","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[{"file_id":"1TCOBur-tr4nscVumhSiQ_7EU3yQu1GGO","timestamp":1728493242644},{"file_id":"1GB1HthaHXUnit7vy6q8HemyuZFK4TqTp","timestamp":1728492058004},{"file_id":"1RmDTXQulrl4yguNnKX8AqtJYW9ZBQv_d","timestamp":1726787849008},{"file_id":"1-iOVoL9YigbkHoheB830Z6i2F_tHy4YA","timestamp":1726778031650}]}},"nbformat":4,"nbformat_minor":0}