{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Directory where datasets will be downloaded and extracted\n",
        "DATA_DIR = 'datasets'\n",
        "\n",
        "# Ensure the dataset directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def download_dataset(dataset_name, url):\n",
        "    \"\"\"\n",
        "    Downloads and extracts a zip file containing the dataset.\n",
        "    \"\"\"\n",
        "    zip_path = os.path.join(DATA_DIR, f\"{dataset_name}.zip\")\n",
        "    extract_path = os.path.join(DATA_DIR, dataset_name)\n",
        "\n",
        "    # Download the dataset\n",
        "    print(f\"Downloading {dataset_name} from {url}...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "    # Extract the zip file\n",
        "    print(f\"Extracting {dataset_name}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    # Remove the zip file after extraction\n",
        "    os.remove(zip_path)\n",
        "    print(f\"Dataset {dataset_name} extracted to {extract_path}.\")\n",
        "    return extract_path\n",
        "\n",
        "def load_arff_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads ARFF file and converts it to a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    print(f\"Loading ARFF file: {file_path}\")\n",
        "    data, meta = arff.loadarff(file_path)\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "def preprocess_data(train_df, test_df, batch_size=64):\n",
        "    \"\"\"\n",
        "    Preprocesses the data:\n",
        "    - Splits the features and labels.\n",
        "    - Normalizes the features.\n",
        "    - Converts them into PyTorch tensors.\n",
        "    - Creates DataLoaders for training, validation, and testing.\n",
        "    \"\"\"\n",
        "    # Separate features and labels\n",
        "    train_features = train_df.drop(columns=['target'])\n",
        "    test_features = test_df.drop(columns=['target'])\n",
        "\n",
        "    # Adjust labels to start from 0 instead of 1\n",
        "    train_labels = train_df['target'].apply(lambda x: int(x) - 1).values\n",
        "    test_labels = test_df['target'].apply(lambda x: int(x) - 1).values\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    train_features_normalized = scaler.fit_transform(train_features)\n",
        "    test_features_normalized = scaler.transform(test_features)\n",
        "\n",
        "    # Reshape the features into 3D arrays (samples, time_steps, dimensions)\n",
        "    X_train = train_features_normalized.reshape(-1, 24, 1)\n",
        "    X_test = test_features_normalized.reshape(-1, 24, 1)\n",
        "\n",
        "    # Split test data into validation and test sets\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_test, test_labels, test_size=0.50, random_state=42)\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(train_labels, dtype=torch.int64)\n",
        "\n",
        "    X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "    # Output dataset shapes\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    # Return both the DataLoaders and the raw tensors\n",
        "    return train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test\n",
        "\n",
        "# Example usage for downloading, extracting, and preprocessing the ElectricDevices dataset\n",
        "if __name__ == \"__main__\":\n",
        "    # URL for the dataset (replace with the actual dataset you want to download)\n",
        "    dataset_name = 'MelbournePedestrian'\n",
        "    dataset_url = 'https://timeseriesclassification.com/aeon-toolkit/MelbournePedestrian.zip'\n",
        "\n",
        "    # Download and extract the dataset\n",
        "    extract_path = download_dataset(dataset_name, dataset_url)\n",
        "\n",
        "    # Load ARFF data\n",
        "    train_file = os.path.join(extract_path, f'{dataset_name}_nmv_TRAIN.arff')\n",
        "    test_file = os.path.join(extract_path, f'{dataset_name}_nmv_TEST.arff')\n",
        "\n",
        "    # Load data into Pandas DataFrames\n",
        "    train_df = load_arff_data(train_file)\n",
        "    test_df = load_arff_data(test_file)\n",
        "\n",
        "    # Preprocess the data\n",
        "    train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test = preprocess_data(train_df, test_df)\n",
        "\n",
        "    n_classes = len(torch.unique(y_train))\n",
        "\n",
        "    # Output the number of classes\n",
        "    print(f\"Number of classes: {n_classes}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K--i86wv_lY8",
        "outputId": "2d108065-a16d-437a-e236-6bb722a01483"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading MelbournePedestrian from https://timeseriesclassification.com/aeon-toolkit/MelbournePedestrian.zip...\n",
            "Extracting MelbournePedestrian...\n",
            "Dataset MelbournePedestrian extracted to datasets/MelbournePedestrian.\n",
            "Loading ARFF file: datasets/MelbournePedestrian/MelbournePedestrian_nmv_TRAIN.arff\n",
            "Loading ARFF file: datasets/MelbournePedestrian/MelbournePedestrian_nmv_TEST.arff\n",
            "X_train shape: torch.Size([1138, 24, 1]), y_train shape: torch.Size([1138])\n",
            "X_valid shape: torch.Size([1159, 24, 1]), y_valid shape: torch.Size([1159])\n",
            "X_test shape: torch.Size([1160, 24, 1]), y_test shape: torch.Size([1160])\n",
            "Number of classes: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDT8m0OYWpQj",
        "outputId": "8cca0e09-96f2-4a24-f601-d5284a943106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAP9J-zwWMbt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchinfo import summary\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alrtTQdHWMbt"
      },
      "source": [
        "## 3. Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1tFm3BmWMbu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class TimeSeriesPatchEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Calculate the number of patches, adjusting for padding if necessary\n",
        "        # Ceiling division to account for padding\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "        self.padding = (\n",
        "            self.num_patches * patch_size\n",
        "        ) - input_timesteps  # Calculate padding length\n",
        "\n",
        "        self.conv_layer = nn.Conv1d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embedding_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "        )\n",
        "\n",
        "        self.class_token_embeddings = nn.Parameter(\n",
        "            torch.randn((1, 1, embedding_dim), requires_grad=True)\n",
        "        )\n",
        "        self.position_embeddings = PositionalEncoding(embedding_dim, dropout=0.1, max_len=input_timesteps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pad the input sequence if necessary\n",
        "        if self.padding > 0:\n",
        "            x = nn.functional.pad(x, (0, 0, 0, self.padding))  # Pad the second to last dimension, which is input_timesteps\n",
        "\n",
        "        # We use a Conv1d layer to generate the patch embeddings\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, timesteps)\n",
        "        conv_output = self.conv_layer(x)\n",
        "        conv_output = conv_output.permute(0, 2, 1)  # (batch, timesteps, features)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n",
        "        output = torch.cat((class_tokens, conv_output), dim=1)\n",
        "\n",
        "        output = self.position_embeddings(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def pos_encoding(self, q_len, d_model, normalize=True):\n",
        "        pe = torch.zeros(q_len, d_model)\n",
        "        position = torch.arange(0, q_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        if normalize:\n",
        "            pe = pe - pe.mean()\n",
        "            pe = pe / (pe.std() * 10)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_encoding(q_len = x.size(1), d_model = x.size(2))\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(LearnedPositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "def get_pos_encoder(pos_encoding):\n",
        "    if pos_encoding == 'fixed':\n",
        "        return PositionalEncoding\n",
        "    elif pos_encoding == 'learned':\n",
        "        return LearnedPositionalEncoding\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8MKozUTWMbu",
        "outputId": "f92ac15b-ad0b-4d8c-f5d1-77caa3cae072"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================================================================================================\n",
              "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
              "=================================================================================================================================================\n",
              "TimeSeriesPatchEmbeddingLayer (TimeSeriesPatchEmbeddingLayer)     [64, 96, 1]          [64, 13, 8]          8                    True\n",
              "├─Conv1d (conv_layer)                                             [64, 1, 96]          [64, 8, 12]          72                   True\n",
              "├─PositionalEncoding (position_embeddings)                        [64, 13, 8]          [64, 13, 8]          --                   --\n",
              "│    └─Dropout (dropout)                                          [64, 13, 8]          [64, 13, 8]          --                   --\n",
              "=================================================================================================================================================\n",
              "Total params: 80\n",
              "Trainable params: 80\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.06\n",
              "=================================================================================================================================================\n",
              "Input size (MB): 0.02\n",
              "Forward/backward pass size (MB): 0.05\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.07\n",
              "================================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "random_instances, random_labels = next(iter(train_loader))\n",
        "random_instance = random_instances[0]\n",
        "\n",
        "BATCH_SIZE = random_instances.shape[0]\n",
        "TIMESTEPS = random_instance.shape[0]\n",
        "CHANNELS = random_instance.shape[1]\n",
        "PATCH_SIZE = 8\n",
        "\n",
        "patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    embedding_dim=CHANNELS * PATCH_SIZE,\n",
        "    input_timesteps=TIMESTEPS,\n",
        ")\n",
        "\n",
        "patch_embeddings = patch_embedding_layer(random_instances)\n",
        "patch_embeddings.shape\n",
        "\n",
        "summary(\n",
        "    model=patch_embedding_layer,\n",
        "    # (batch_size, input_channels, input_timesteps)\n",
        "    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s30BKFOOWMbu"
      },
      "outputs": [],
      "source": [
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps)\n",
        "\n",
        "        # Calculate the number of patches\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        # Setting batch_first=True to accommodate inputs with batch dimension first\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n",
        "\n",
        "        # Feedforward layer\n",
        "        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n",
        "        # Classifier Head\n",
        "        self.classifier = nn.Linear(dim_feedforward, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, in_channels, input_timesteps)\n",
        "\n",
        "        # Get patch embeddings\n",
        "        x = self.patch_embedding(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n",
        "\n",
        "        # Apply Transformer Encoder with batch first\n",
        "        x = self.transformer_encoder(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n",
        "\n",
        "        # Use the output corresponding to the class token for classification\n",
        "        class_token_output = x[:, 0, :]  # Select the class token for each item in the batch\n",
        "\n",
        "        # Feedforward layer\n",
        "        x = self.ff_layer(class_token_output)\n",
        "\n",
        "        # Classifier head\n",
        "        output = self.classifier(x)  # Output shape: (batch_size, num_classes)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3tGq9kmWMbu",
        "outputId": "e1288d51-5b2a-4389-c155-bb9284331005"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=======================================================================================================================================\n",
              "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
              "=======================================================================================================================================\n",
              "TimeSeriesTransformer (TimeSeriesTransformer)           [64, 96, 1]          [64, 7]              --                   True\n",
              "├─TimeSeriesPatchEmbeddingLayer (patch_embedding)       [64, 96, 1]          [64, 13, 32]         32                   True\n",
              "│    └─Conv1d (conv_layer)                              [64, 1, 96]          [64, 32, 12]         288                  True\n",
              "│    └─PositionalEncoding (position_embeddings)         [64, 13, 32]         [64, 13, 32]         --                   --\n",
              "│    │    └─Dropout (dropout)                           [64, 13, 32]         [64, 13, 32]         --                   --\n",
              "├─TransformerEncoder (transformer_encoder)              [64, 13, 32]         [64, 13, 32]         --                   True\n",
              "│    └─ModuleList (layers)                              --                   --                   --                   True\n",
              "│    │    └─TransformerEncoderLayer (0)                 [64, 13, 32]         [64, 13, 32]         12,704               True\n",
              "│    │    └─TransformerEncoderLayer (1)                 [64, 13, 32]         [64, 13, 32]         12,704               True\n",
              "│    │    └─TransformerEncoderLayer (2)                 [64, 13, 32]         [64, 13, 32]         12,704               True\n",
              "│    │    └─TransformerEncoderLayer (3)                 [64, 13, 32]         [64, 13, 32]         12,704               True\n",
              "├─Linear (ff_layer)                                     [64, 32]             [64, 128]            4,224                True\n",
              "├─Linear (classifier)                                   [64, 128]            [64, 7]              903                  True\n",
              "=======================================================================================================================================\n",
              "Total params: 56,263\n",
              "Trainable params: 56,263\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.55\n",
              "=======================================================================================================================================\n",
              "Input size (MB): 0.02\n",
              "Forward/backward pass size (MB): 0.27\n",
              "Params size (MB): 0.02\n",
              "Estimated Total Size (MB): 0.31\n",
              "======================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler to reduce the learning rate by the specified step size and factor (gamma) every step_size epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "summary(\n",
        "    model=model,\n",
        "    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPO8SLCpWMbu"
      },
      "source": [
        "## 4. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smirk3qsWMbu",
        "outputId": "75ce4d0a-6797-4891-b0d5-fb9321cdd12c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: New best model saved with validation accuracy: 0.5208\n",
            "Epoch 1, Train Loss: 0.7712, Train Acc: 0.7025, Val Loss: 1.3890, Val Acc: 0.5208\n",
            "Epoch 2: New best model saved with validation accuracy: 0.5622\n",
            "Epoch 2, Train Loss: 0.7436, Train Acc: 0.7048, Val Loss: 1.3258, Val Acc: 0.5622\n",
            "Epoch 3: New best model saved with validation accuracy: 0.5656\n",
            "Epoch 3, Train Loss: 0.7164, Train Acc: 0.7241, Val Loss: 1.3900, Val Acc: 0.5656\n",
            "Epoch 4: New best model saved with validation accuracy: 0.6034\n",
            "Epoch 4, Train Loss: 0.7139, Train Acc: 0.7196, Val Loss: 1.1777, Val Acc: 0.6034\n",
            "Epoch 5: New best model saved with validation accuracy: 0.6180\n",
            "Epoch 5, Train Loss: 0.7023, Train Acc: 0.7268, Val Loss: 1.2032, Val Acc: 0.6180\n",
            "Epoch 6: New best model saved with validation accuracy: 0.6250\n",
            "Epoch 6, Train Loss: 0.6943, Train Acc: 0.7290, Val Loss: 1.1806, Val Acc: 0.6250\n",
            "Epoch 7: New best model saved with validation accuracy: 0.6359\n",
            "Epoch 7, Train Loss: 0.6839, Train Acc: 0.7372, Val Loss: 1.1867, Val Acc: 0.6359\n",
            "Epoch 8: New best model saved with validation accuracy: 0.6398\n",
            "Epoch 8, Train Loss: 0.6680, Train Acc: 0.7416, Val Loss: 1.1467, Val Acc: 0.6398\n",
            "Epoch 9, Train Loss: 0.6701, Train Acc: 0.7436, Val Loss: 1.3993, Val Acc: 0.5807\n",
            "Epoch 10: New best model saved with validation accuracy: 0.6438\n",
            "Epoch 10, Train Loss: 0.6516, Train Acc: 0.7492, Val Loss: 1.2408, Val Acc: 0.6438\n",
            "Epoch 11, Train Loss: 0.6471, Train Acc: 0.7527, Val Loss: 1.1824, Val Acc: 0.5974\n",
            "Epoch 12: New best model saved with validation accuracy: 0.6656\n",
            "Epoch 12, Train Loss: 0.6392, Train Acc: 0.7528, Val Loss: 1.0670, Val Acc: 0.6656\n",
            "Epoch 13, Train Loss: 0.6400, Train Acc: 0.7581, Val Loss: 1.2168, Val Acc: 0.6273\n",
            "Epoch 14, Train Loss: 0.6189, Train Acc: 0.7655, Val Loss: 1.0985, Val Acc: 0.6617\n",
            "Epoch 15, Train Loss: 0.6189, Train Acc: 0.7664, Val Loss: 1.1611, Val Acc: 0.6479\n",
            "Epoch 16, Train Loss: 0.6076, Train Acc: 0.7650, Val Loss: 1.1461, Val Acc: 0.6617\n",
            "Epoch 17, Train Loss: 0.5967, Train Acc: 0.7724, Val Loss: 1.2805, Val Acc: 0.6292\n",
            "Epoch 18, Train Loss: 0.6037, Train Acc: 0.7715, Val Loss: 1.1903, Val Acc: 0.6312\n",
            "Epoch 19, Train Loss: 0.5904, Train Acc: 0.7808, Val Loss: 1.1191, Val Acc: 0.6609\n",
            "Epoch 20, Train Loss: 0.5828, Train Acc: 0.7802, Val Loss: 1.2347, Val Acc: 0.6331\n",
            "Loaded best model for testing or further use.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-9c1bc2205eaa>:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        }
      ],
      "source": [
        "# Model, loss function, and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Number of epochs\n",
        "n_epochs = 20\n",
        "\n",
        "# Initialize variables for tracking the best model\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    train_losses = []\n",
        "    train_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Training loop\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "        predictions = model(inputs)  # Forward pass\n",
        "        loss = criterion(predictions, labels)  # Calculate loss\n",
        "\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Count the number of correct predictions\n",
        "        train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = np.mean(train_losses)\n",
        "    train_acc = train_correct / total\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        validation_losses = []\n",
        "        validation_correct = 0\n",
        "        total_val = 0\n",
        "\n",
        "        for inputs, labels in valid_loader:\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            validation_losses.append(loss.item())\n",
        "\n",
        "            validation_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "        validation_loss = np.mean(validation_losses)\n",
        "        validation_acc = validation_correct / total_val\n",
        "\n",
        "    # Check if this is the best model so far\n",
        "    if validation_acc > best_validation_acc:\n",
        "        best_validation_acc = validation_acc\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {validation_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {validation_loss:.4f}, Val Acc: {validation_acc:.4f}')\n",
        "\n",
        "# Loading the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na4sOhN7WMbv"
      },
      "source": [
        "## 5. Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyotm-hrWMbv",
        "outputId": "cebeaf3c-3e33-42c3-d8a2-6ebe5c5af2f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.29      0.36       328\n",
            "           1       0.95      0.75      0.84       958\n",
            "           2       0.70      0.76      0.73       382\n",
            "           3       0.50      0.56      0.53       580\n",
            "           4       0.63      0.86      0.72       935\n",
            "           5       0.49      0.53      0.51       380\n",
            "           6       0.53      0.24      0.33       293\n",
            "\n",
            "    accuracy                           0.65      3856\n",
            "   macro avg       0.61      0.57      0.57      3856\n",
            "weighted avg       0.66      0.65      0.64      3856\n",
            "\n",
            "Confusion matrix:\n",
            "[[ 94   4   9  59  75  87   0]\n",
            " [  0 722   0  38 196   0   2]\n",
            " [  3   0 291   5   0  36  47]\n",
            " [ 54   0  89 325  23  86   3]\n",
            " [  3  34   8  73 806   2   9]\n",
            " [ 36   0  20 116   4 203   1]\n",
            " [  3   4   0  28 185   3  70]]\n"
          ]
        }
      ],
      "source": [
        "# Prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    Y_pred_prob = model(X_test)\n",
        "\n",
        "Y_pred = Y_pred_prob.argmax(1)\n",
        "\n",
        "print(classification_report(y_test, Y_pred))\n",
        "confusion = confusion_matrix(y_test, Y_pred)\n",
        "print(f\"Confusion matrix:\\n{confusion}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "LhQeXQWnWMbv",
        "outputId": "f322d311-16eb-4021-8aff-270ec153b414"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Visualize the confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Prediction\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encodings"
      ],
      "metadata": {
        "id": "PHiMwsYvVAzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rwyM_sG-KTN",
        "outputId": "e3b7f2e4-b4bb-46b2-d64f-54579d2a06d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "import pandas as pd\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Scl(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.relative_bias_table = nn.Parameter(torch.zeros((2 * self.seq_len - 1), num_heads))\n",
        "        coords = torch.meshgrid((torch.arange(1), torch.arange(self.seq_len)))\n",
        "        coords = torch.flatten(torch.stack(coords), 1)\n",
        "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
        "        relative_coords[1] += self.seq_len - 1\n",
        "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
        "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
        "        self.register_buffer(\"relative_index\", relative_index)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, 8))\n",
        "        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=1 * self.seq_len, w=1 * self.seq_len)\n",
        "        attn = attn + relative_bias\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Vec(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.Er = nn.Parameter(torch.randn(self.seq_len, int(emb_size / num_heads)))\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.tril(torch.ones(self.seq_len, self.seq_len))\n",
        "                .unsqueeze(0).unsqueeze(0)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        QEr = torch.matmul(q, self.Er.transpose(0, 1))\n",
        "        Srel = self.skew(QEr)\n",
        "\n",
        "        attn = torch.matmul(q, k)\n",
        "        attn = (attn + Srel) * self.scale\n",
        "\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "    def skew(self, QEr):\n",
        "        padded = nn.functional.pad(QEr, (1, 0))\n",
        "        batch_size, num_heads, num_rows, num_cols = padded.shape\n",
        "        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
        "        Srel = reshaped[:, :, 1:, :]\n",
        "        return Srel\n"
      ],
      "metadata": {
        "id": "yD7K5F3D-Hji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TUPEConfig:\n",
        "    num_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    d_model: int = 128\n",
        "    d_head: int = 0\n",
        "    max_len: int = 256\n",
        "    dropout: float = 0.1\n",
        "    expansion_factor: int = 1\n",
        "    relative_bias: bool = True\n",
        "    bidirectional_bias: bool = True\n",
        "    num_buckets: int = 32\n",
        "    max_distance: int = 128\n",
        "\n",
        "    def __post_init__(self):\n",
        "        d_head, remainder = divmod(self.d_model, self.num_heads)\n",
        "        assert remainder == 0, \"`d_model` should be divisible by `num_heads`\"\n",
        "        self.d_head = d_head"
      ],
      "metadata": {
        "id": "B2e6n_Yj_38p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_relative_position_bucket(\n",
        "    relative_position, bidirectional, num_buckets, max_distance\n",
        "):\n",
        "    \"\"\"\n",
        "    from https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/modeling_t5.py\n",
        "    \"\"\"\n",
        "    relative_buckets = 0\n",
        "    if bidirectional:\n",
        "        num_buckets //= 2\n",
        "        relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n",
        "        relative_position = torch.abs(relative_position)\n",
        "    else:\n",
        "        relative_position = -torch.min(\n",
        "            relative_position, torch.zeros_like(relative_position)\n",
        "        )\n",
        "    # now relative_position is in the range [0, inf)\n",
        "\n",
        "    # half of the buckets are for exact increments in positions\n",
        "    max_exact = num_buckets // 2\n",
        "    is_small = relative_position < max_exact\n",
        "\n",
        "    # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n",
        "    relative_postion_if_large = max_exact + (\n",
        "        torch.log(relative_position.float() / max_exact)\n",
        "        / math.log(max_distance / max_exact)\n",
        "        * (num_buckets - max_exact)\n",
        "    ).to(torch.long)\n",
        "    relative_postion_if_large = torch.min(\n",
        "        relative_postion_if_large,\n",
        "        torch.full_like(relative_postion_if_large, num_buckets - 1),\n",
        "    )\n",
        "\n",
        "    relative_buckets += torch.where(\n",
        "        is_small, relative_position, relative_postion_if_large\n",
        "    )\n",
        "    return relative_buckets\n",
        "\n",
        "\n",
        "def get_relative_positions(\n",
        "    seq_len, bidirectional=True, num_buckets=32, max_distance=128\n",
        "):\n",
        "    x = torch.arange(seq_len)[None, :]\n",
        "    y = torch.arange(seq_len)[:, None]\n",
        "    relative_positions = _get_relative_position_bucket(\n",
        "        x - y, bidirectional, num_buckets, max_distance\n",
        "    )\n",
        "    return relative_positions"
      ],
      "metadata": {
        "id": "1QbW7ihvBByl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TUPEMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config: TUPEConfig, pos_embed: nn.Module) -> None:\n",
        "        super().__init__()\n",
        "        self.max_len = config.max_len\n",
        "        self.num_heads = config.num_heads\n",
        "        self.num_buckets = config.num_buckets\n",
        "        self.max_distance = config.max_distance\n",
        "        self.bidirectional = config.bidirectional_bias\n",
        "        self.scale = math.sqrt(2 * config.d_head)\n",
        "\n",
        "        self.pos_embed = pos_embed\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # kqv in one pass\n",
        "        self.pos_kq = nn.Linear(config.d_model, 2 * config.d_model, bias=False)\n",
        "        self.tok_kqv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n",
        "\n",
        "        self.relative_bias = config.relative_bias\n",
        "        if config.relative_bias:\n",
        "            self.bias = nn.Embedding(config.max_len * 2, config.num_heads)\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        pos_embed = self.pos_embed(seq_len).repeat(batch_size, 1, 1)\n",
        "        # pos_embed.shape == (batch_size, seq_len, d_model)\n",
        "        pos_key, pos_query = self.pos_kq(pos_embed).chunk(2, dim=-1)\n",
        "        pos_key = pos_key.view(batch_size, seq_len, self.num_heads, -1).permute(\n",
        "            0, 2, 3, 1\n",
        "        )\n",
        "        # pos_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        pos_query = pos_query.view(batch_size, seq_len, self.num_heads, -1).transpose(\n",
        "            1, 2\n",
        "        )\n",
        "        # pos_query.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        pos_attn = torch.matmul(pos_query, pos_key)\n",
        "        # pos_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        tok_key, tok_query, tok_value = self.tok_kqv(x).chunk(3, dim=-1)\n",
        "        tok_key = tok_key.view(batch_size, seq_len, self.num_heads, -1).permute(\n",
        "            0, 2, 3, 1\n",
        "        )\n",
        "        # tok_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        tok_query = tok_query.view(batch_size, seq_len, self.num_heads, -1).transpose(\n",
        "            1, 2\n",
        "        )\n",
        "        tok_value = tok_value.view(batch_size, seq_len, self.num_heads, -1).transpose(\n",
        "            1, 2\n",
        "        )\n",
        "        # tok_qv.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        tok_attn = torch.matmul(tok_query, tok_key)\n",
        "        # tok_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        attn = (tok_attn + pos_attn) / self.scale\n",
        "        if self.relative_bias:\n",
        "            relative_positions = get_relative_positions(\n",
        "                seq_len, self.bidirectional, self.num_buckets, self.max_distance\n",
        "            ).to(attn.device)\n",
        "            # relative_positions.shape == (seq_len, seq_len)\n",
        "            bias = self.bias(relative_positions + self.max_len)\n",
        "            # bias.shape == (seq_len, seq_len, num_heads)\n",
        "            bias = bias.permute(2, 0, 1).unsqueeze(0)\n",
        "            # bias.shape == (1, num_heads, seq_len, seq_len)\n",
        "            attn = attn + bias\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "        out = torch.matmul(attn, tok_value)\n",
        "        # out.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
        "        # out.shape == (batch_size, seq_len, d_model)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "9uW0SNXu_oda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import math\n",
        "from torchsummary import summary\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def pos_encoding(self, q_len, d_model, normalize=True):\n",
        "        pe = torch.zeros(q_len, d_model)\n",
        "        position = torch.arange(0, q_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        if normalize:\n",
        "            pe = pe - pe.mean()\n",
        "            pe = pe / (pe.std() * 10)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_encoding(q_len=x.size(1), d_model=x.size(2))\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Learned Positional Encoding\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(LearnedPositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Time Series Patch Embedding Layer\n",
        "class TimeSeriesPatchEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps, pos_encoding='fixed'):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "        self.padding = (self.num_patches * patch_size) - input_timesteps\n",
        "\n",
        "        self.conv_layer = nn.Conv1d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embedding_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "        )\n",
        "\n",
        "        self.class_token_embeddings = nn.Parameter(\n",
        "            torch.randn((1, 1, embedding_dim), requires_grad=True)\n",
        "        )\n",
        "\n",
        "        # Instantiate the positional encoding\n",
        "        pos_encoder_class = get_pos_encoder(pos_encoding)\n",
        "        self.position_embeddings = pos_encoder_class(embedding_dim, dropout=0.1, max_len=input_timesteps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.padding > 0:\n",
        "            x = nn.functional.pad(x, (0, 0, 0, self.padding))\n",
        "\n",
        "        x = x.permute(0, 2, 1)\n",
        "        conv_output = self.conv_layer(x)\n",
        "        conv_output = conv_output.permute(0, 2, 1)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n",
        "        output = torch.cat((class_tokens, conv_output), dim=1)\n",
        "\n",
        "        output = self.position_embeddings(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Get Positional Encoding\n",
        "def get_pos_encoder(pos_encoding):\n",
        "    if pos_encoding == 'fixed':\n",
        "        return PositionalEncoding\n",
        "    elif pos_encoding == 'learned':\n",
        "        return LearnedPositionalEncoding\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, pos_encoding='fixed', num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2, attention_type='absolute'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps, pos_encoding)\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "\n",
        "        if attention_type == 'relative_scl':\n",
        "            self.attention_layer = Attention_Rel_Scl(embedding_dim, num_heads, self.num_patches + 1, dropout)\n",
        "        elif attention_type == 'relative_vec':\n",
        "            self.attention_layer = Attention_Rel_Vec(embedding_dim, num_heads, self.num_patches + 1, dropout)\n",
        "        elif attention_type == 'tupe':\n",
        "            self.attention_layer = TUPEMultiHeadAttention(tupe_config, pos_embed)\n",
        "        else:\n",
        "            self.attention_layer = Attention(embedding_dim, num_heads, dropout)\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n",
        "\n",
        "        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n",
        "        self.classifier = nn.Linear(dim_feedforward, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        x = self.attention_layer(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        class_token_output = x[:, 0, :]\n",
        "        x = self.ff_layer(class_token_output)\n",
        "        output = self.classifier(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "random_instances, random_labels = next(iter(train_loader))\n",
        "random_instance = random_instances[0]\n",
        "\n",
        "BATCH_SIZE = random_instances.shape[0]\n",
        "TIMESTEPS = random_instance.shape[0]\n",
        "CHANNELS = random_instance.shape[1]\n",
        "PATCH_SIZE = 8\n",
        "\n",
        "patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    embedding_dim=CHANNELS * PATCH_SIZE,\n",
        "    input_timesteps=TIMESTEPS,\n",
        ")\n",
        "\n",
        "patch_embeddings = patch_embedding_layer(random_instances)\n",
        "patch_embeddings.shape\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    pos_encoding='fixed',\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "    val_loss = total_loss / len(valid_loader)\n",
        "    val_acc = correct / total_val\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n",
        "    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n",
        "\n",
        "    if val_acc > best_validation_acc:\n",
        "        best_validation_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeNs_BKxfx0i",
        "outputId": "b18f5fdb-ae92-4bef-a7c0-7ea26eb068c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1.4488, Train Acc: 0.4622\n",
            "Epoch 1: New best model saved with validation accuracy: 0.4982\n",
            "Epoch 1, Val Loss: 1.3612, Val Acc: 0.4982\n",
            "Epoch 1, Train Loss: 1.0634, Train Acc: 0.5867\n",
            "Epoch 2: New best model saved with validation accuracy: 0.5016\n",
            "Epoch 2, Val Loss: 1.3072, Val Acc: 0.5016\n",
            "Epoch 1, Train Loss: 0.9760, Train Acc: 0.6214\n",
            "Epoch 3: New best model saved with validation accuracy: 0.5505\n",
            "Epoch 3, Val Loss: 1.2670, Val Acc: 0.5505\n",
            "Epoch 1, Train Loss: 0.9123, Train Acc: 0.6487\n",
            "Epoch 4, Val Loss: 1.3334, Val Acc: 0.5211\n",
            "Epoch 1, Train Loss: 0.8694, Train Acc: 0.6684\n",
            "Epoch 5: New best model saved with validation accuracy: 0.5638\n",
            "Epoch 5, Val Loss: 1.2637, Val Acc: 0.5638\n",
            "Epoch 1, Train Loss: 0.8437, Train Acc: 0.6790\n",
            "Epoch 6, Val Loss: 1.2772, Val Acc: 0.5440\n",
            "Epoch 1, Train Loss: 0.8207, Train Acc: 0.6842\n",
            "Epoch 7, Val Loss: 1.2701, Val Acc: 0.5612\n",
            "Epoch 1, Train Loss: 0.7992, Train Acc: 0.6976\n",
            "Epoch 8, Val Loss: 1.4201, Val Acc: 0.5227\n",
            "Epoch 1, Train Loss: 0.7769, Train Acc: 0.7067\n",
            "Epoch 9, Val Loss: 1.3189, Val Acc: 0.5594\n",
            "Epoch 1, Train Loss: 0.7635, Train Acc: 0.7075\n",
            "Epoch 10, Val Loss: 1.3262, Val Acc: 0.5589\n",
            "Loaded best model for testing or further use.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-b06036bd6b7c>:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "import pandas as pd\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Scl(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.relative_bias_table = nn.Parameter(torch.zeros((2 * self.seq_len - 1), num_heads))\n",
        "        coords = torch.meshgrid((torch.arange(1), torch.arange(self.seq_len)))\n",
        "        coords = torch.flatten(torch.stack(coords), 1)\n",
        "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
        "        relative_coords[1] += self.seq_len - 1\n",
        "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
        "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
        "        self.register_buffer(\"relative_index\", relative_index)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, 8))\n",
        "        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=1 * self.seq_len, w=1 * self.seq_len)\n",
        "        attn = attn + relative_bias\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Vec(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.Er = nn.Parameter(torch.randn(self.seq_len, int(emb_size / num_heads)))\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.tril(torch.ones(self.seq_len, self.seq_len))\n",
        "                .unsqueeze(0).unsqueeze(0)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        QEr = torch.matmul(q, self.Er.transpose(0, 1))\n",
        "        Srel = self.skew(QEr)\n",
        "\n",
        "        attn = torch.matmul(q, k)\n",
        "        attn = (attn + Srel) * self.scale\n",
        "\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "    def skew(self, QEr):\n",
        "        padded = nn.functional.pad(QEr, (1, 0))\n",
        "        batch_size, num_heads, num_rows, num_cols = padded.shape\n",
        "        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
        "        Srel = reshaped[:, :, 1:, :]\n",
        "        return Srel\n"
      ],
      "metadata": {
        "id": "ESOXdCAqaM_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class tAPE(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n",
        "        super(tAPE, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin((position * div_term) * (d_model / max_len))\n",
        "        pe[:, 1::2] = torch.cos((position * div_term) * (d_model / max_len))\n",
        "        pe = scale_factor * pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class AbsolutePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n",
        "        super(AbsolutePositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = scale_factor * pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024):\n",
        "        super(LearnablePositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pe = nn.Parameter(torch.empty(max_len, d_model))\n",
        "        nn.init.uniform_(self.pe, -0.02, 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(1), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "dplhsyvE-jR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_encoder(pos_encoding):\n",
        "    if pos_encoding == 'fixed':\n",
        "        return PositionalEncoding\n",
        "    elif pos_encoding == 'learned':\n",
        "        return LearnedPositionalEncoding\n",
        "    elif pos_encoding == 'tAPE':\n",
        "        return tAPE\n",
        "    elif pos_encoding == 'absolute':\n",
        "        return AbsolutePositionalEncoding\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n"
      ],
      "metadata": {
        "id": "q9sb14F6-kc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    pos_encoding='tAPE',\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "    val_loss = total_loss / len(valid_loader)\n",
        "    val_acc = correct / total_val\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n",
        "    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n",
        "\n",
        "    if val_acc > best_validation_acc:\n",
        "        best_validation_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egoVdYVX-mq4",
        "outputId": "597d4462-6998-4031-e714-91eb38f4825c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1.4832, Train Acc: 0.4435\n",
            "Epoch 1: New best model saved with validation accuracy: 0.5065\n",
            "Epoch 1, Val Loss: 1.2993, Val Acc: 0.5065\n",
            "Epoch 1, Train Loss: 1.0710, Train Acc: 0.5795\n",
            "Epoch 2, Val Loss: 1.2988, Val Acc: 0.4977\n",
            "Epoch 1, Train Loss: 0.9773, Train Acc: 0.6066\n",
            "Epoch 3: New best model saved with validation accuracy: 0.5206\n",
            "Epoch 3, Val Loss: 1.2364, Val Acc: 0.5206\n",
            "Epoch 1, Train Loss: 0.9150, Train Acc: 0.6348\n",
            "Epoch 4: New best model saved with validation accuracy: 0.5594\n",
            "Epoch 4, Val Loss: 1.2674, Val Acc: 0.5594\n",
            "Epoch 1, Train Loss: 0.8785, Train Acc: 0.6486\n",
            "Epoch 5, Val Loss: 1.2017, Val Acc: 0.5505\n",
            "Epoch 1, Train Loss: 0.8478, Train Acc: 0.6611\n",
            "Epoch 6: New best model saved with validation accuracy: 0.6003\n",
            "Epoch 6, Val Loss: 1.1721, Val Acc: 0.6003\n",
            "Epoch 1, Train Loss: 0.8237, Train Acc: 0.6757\n",
            "Epoch 7, Val Loss: 1.1648, Val Acc: 0.5974\n",
            "Epoch 1, Train Loss: 0.7999, Train Acc: 0.6897\n",
            "Epoch 8: New best model saved with validation accuracy: 0.6148\n",
            "Epoch 8, Val Loss: 1.1484, Val Acc: 0.6148\n",
            "Epoch 1, Train Loss: 0.7795, Train Acc: 0.6936\n",
            "Epoch 9: New best model saved with validation accuracy: 0.6391\n",
            "Epoch 9, Val Loss: 1.0820, Val Acc: 0.6391\n",
            "Epoch 1, Train Loss: 0.7692, Train Acc: 0.6982\n",
            "Epoch 10, Val Loss: 1.1401, Val Acc: 0.6247\n",
            "Loaded best model for testing or further use.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-ea2d96ec00ab>:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    pos_encoding='fixed',\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        "    attention_type= 'relative_vec',\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "    val_loss = total_loss / len(valid_loader)\n",
        "    val_acc = correct / total_val\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n",
        "    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n",
        "\n",
        "    if val_acc > best_validation_acc:\n",
        "        best_validation_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzFfH3xX8fE_",
        "outputId": "a189606e-5ee8-495f-d203-9f70d1f2c0c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1.4258, Train Acc: 0.4705\n",
            "Epoch 1: New best model saved with validation accuracy: 0.5125\n",
            "Epoch 1, Val Loss: 1.3063, Val Acc: 0.5125\n",
            "Epoch 1, Train Loss: 1.0614, Train Acc: 0.5897\n",
            "Epoch 2: New best model saved with validation accuracy: 0.5253\n",
            "Epoch 2, Val Loss: 1.2935, Val Acc: 0.5253\n",
            "Epoch 1, Train Loss: 0.9636, Train Acc: 0.6257\n",
            "Epoch 3: New best model saved with validation accuracy: 0.5570\n",
            "Epoch 3, Val Loss: 1.2183, Val Acc: 0.5570\n",
            "Epoch 1, Train Loss: 0.8864, Train Acc: 0.6577\n",
            "Epoch 4, Val Loss: 1.2726, Val Acc: 0.5518\n",
            "Epoch 1, Train Loss: 0.8296, Train Acc: 0.6822\n",
            "Epoch 5: New best model saved with validation accuracy: 0.5578\n",
            "Epoch 5, Val Loss: 1.2302, Val Acc: 0.5578\n",
            "Epoch 1, Train Loss: 0.7708, Train Acc: 0.7101\n",
            "Epoch 6: New best model saved with validation accuracy: 0.5758\n",
            "Epoch 6, Val Loss: 1.2282, Val Acc: 0.5758\n",
            "Epoch 1, Train Loss: 0.7556, Train Acc: 0.7170\n",
            "Epoch 7: New best model saved with validation accuracy: 0.5904\n",
            "Epoch 7, Val Loss: 1.2075, Val Acc: 0.5904\n",
            "Epoch 1, Train Loss: 0.7259, Train Acc: 0.7267\n",
            "Epoch 8: New best model saved with validation accuracy: 0.6211\n",
            "Epoch 8, Val Loss: 1.1509, Val Acc: 0.6211\n",
            "Epoch 1, Train Loss: 0.6977, Train Acc: 0.7375\n",
            "Epoch 9, Val Loss: 1.2357, Val Acc: 0.5896\n",
            "Epoch 1, Train Loss: 0.6870, Train Acc: 0.7411\n",
            "Epoch 10: New best model saved with validation accuracy: 0.6234\n",
            "Epoch 10, Val Loss: 1.1507, Val Acc: 0.6234\n",
            "Loaded best model for testing or further use.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-a64704e9076d>:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TUPE"
      ],
      "metadata": {
        "id": "ntF6fok4KzMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class TUPEConfig:\n",
        "    def __init__(self):\n",
        "        self.d_model = 128\n",
        "        self.num_heads = 8\n",
        "        self.dim_feedforward = 512\n",
        "        self.dropout = 0.1\n",
        "        self.max_len = 5000\n",
        "        self.num_buckets = 32\n",
        "        self.max_distance = 128\n",
        "        self.relative_bias = True\n",
        "        self.bidirectional_bias = True\n",
        "\n",
        "class TUPEMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config: TUPEConfig, pos_embed: nn.Module) -> None:\n",
        "        super().__init__()\n",
        "        self.max_len = config.max_len\n",
        "        self.num_heads = config.num_heads\n",
        "        self.num_buckets = config.num_buckets\n",
        "        self.max_distance = config.max_distance\n",
        "        self.bidirectional = config.bidirectional_bias\n",
        "        self.scale = math.sqrt(2 * config.d_model // config.num_heads)\n",
        "\n",
        "        self.pos_embed = pos_embed\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # kqv in one pass\n",
        "        self.pos_kq = nn.Linear(config.d_model, 2 * config.d_model, bias=False)\n",
        "        self.tok_kqv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n",
        "\n",
        "        self.relative_bias = config.relative_bias\n",
        "        if config.relative_bias:\n",
        "            self.bias = nn.Embedding(config.max_len * 2, config.num_heads)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        pos_embed = self.pos_embed(seq_len).repeat(batch_size, 1, 1)\n",
        "        # pos_embed.shape == (batch_size, seq_len, d_model)\n",
        "        pos_key, pos_query = self.pos_kq(pos_embed).chunk(2, dim=-1)\n",
        "        pos_key = pos_key.view(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        # pos_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        pos_query = pos_query.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        # pos_query.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        pos_attn = torch.matmul(pos_query, pos_key)\n",
        "        # pos_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        tok_key, tok_query, tok_value = self.tok_kqv(x).chunk(3, dim=-1)\n",
        "        tok_key = tok_key.view(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        # tok_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        tok_query = tok_query.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        tok_value = tok_value.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        # tok_qv.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        tok_attn = torch.matmul(tok_query, tok_key)\n",
        "        # tok_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        attn = (tok_attn + pos_attn) / self.scale\n",
        "        if self.relative_bias:\n",
        "            relative_positions = self.get_relative_positions(seq_len)\n",
        "            # relative_positions.shape == (seq_len, seq_len)\n",
        "            bias = self.bias(relative_positions + self.max_len)\n",
        "            # bias.shape == (seq_len, seq_len, num_heads)\n",
        "            bias = bias.permute(2, 0, 1).unsqueeze(0)\n",
        "            # bias.shape == (1, num_heads, seq_len, seq_len)\n",
        "            attn = attn + bias\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "        out = torch.matmul(attn, tok_value)\n",
        "        # out.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
        "        # out.shape == (batch_size, seq_len, d_model)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "    def get_relative_positions(self, seq_len):\n",
        "        # Generate relative position encodings\n",
        "        range_vec = torch.arange(seq_len)\n",
        "        distance_mat = range_vec.unsqueeze(1) - range_vec.unsqueeze(0)\n",
        "        distance_mat_clipped = torch.clamp(distance_mat, -self.max_distance, self.max_distance)\n",
        "        final_mat = distance_mat_clipped + self.max_distance\n",
        "        return final_mat\n"
      ],
      "metadata": {
        "id": "LWu8jg32ORdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(FixedPositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)  # (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        return self.encoding[:, :seq_len, :].detach()\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_layers, num_heads, dim_feedforward, dropout, num_classes, attention_type, pos_encoding):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.attention_type = attention_type\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.pos_encoding = pos_encoding\n",
        "\n",
        "        # Transformer layers with TUPEMultiHeadAttention\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TUPEMultiHeadAttention(\n",
        "                config=TUPEConfig(),\n",
        "                pos_embed=FixedPositionalEncoding(embedding_dim, max_len=input_timesteps)\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(f\"Input to model: {x.shape}\")\n",
        "\n",
        "        # Patch embedding\n",
        "        x = x.permute(0, 2, 1)  # Change shape from (batch_size, seq_len, in_channels) to (batch_size, in_channels, seq_len)\n",
        "        #print(f\"Input to Conv1d: {x.shape}\")\n",
        "\n",
        "        x = self.patch_embedding(x)  # Apply Conv1d\n",
        "        #print(f\"Patch embedding output: {x.shape}\")\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, embedding_dim)\n",
        "        #print(f\"Patch embedding output permuted: {x.shape}\")\n",
        "\n",
        "        # Positional encoding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pos_encoding(seq_len).to(x.device)\n",
        "        #print(f\"Positional encoding output: {x.shape}\")\n",
        "\n",
        "        # Transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "            #print(f\"Transformer layer output: {x.shape}\")\n",
        "\n",
        "        # Classification head\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.classifier(x)\n",
        "        #print(f\"Classifier output: {x.shape}\")\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "TIMESTEPS = 151\n",
        "CHANNELS = 1\n",
        "n_classes = 7\n",
        "\n",
        "# Initialize positional encoding\n",
        "pos_embed = FixedPositionalEncoding(d_model=128, max_len=TIMESTEPS)\n",
        "\n",
        "# Define the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=128,\n",
        "    num_layers=4,\n",
        "    num_heads=8,\n",
        "    dim_feedforward=512,\n",
        "    dropout=0.1,\n",
        "    num_classes=n_classes,\n",
        "    attention_type='tupe',\n",
        "    pos_encoding=pos_embed,\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training and validation loop\n",
        "num_epochs = 10\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_preds = []\n",
        "    train_labels = []\n",
        "    for batch in train_loader:\n",
        "        inputs, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    valid_preds = []\n",
        "    valid_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            inputs, labels = batch\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item() * inputs.size(0)\n",
        "            valid_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "            valid_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    valid_loss /= len(valid_loader.dataset)\n",
        "    valid_acc = accuracy_score(valid_labels, valid_preds)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
        "          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgP2Ge01OTt0",
        "outputId": "71725da4-7e28-49e8-c6d3-bb9a1f24b72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 1.2511, Train Acc: 0.5344, Valid Loss: 1.3861, Valid Acc: 0.5216\n",
            "Epoch 2/10, Train Loss: 0.8640, Train Acc: 0.6794, Valid Loss: 1.2467, Valid Acc: 0.6083\n",
            "Epoch 3/10, Train Loss: 0.7777, Train Acc: 0.7149, Valid Loss: 1.4562, Valid Acc: 0.5781\n",
            "Epoch 4/10, Train Loss: 0.7258, Train Acc: 0.7362, Valid Loss: 1.3309, Valid Acc: 0.5990\n",
            "Epoch 5/10, Train Loss: 0.6972, Train Acc: 0.7449, Valid Loss: 1.4591, Valid Acc: 0.5974\n",
            "Epoch 6/10, Train Loss: 0.6722, Train Acc: 0.7534, Valid Loss: 1.3508, Valid Acc: 0.5911\n",
            "Epoch 7/10, Train Loss: 0.6304, Train Acc: 0.7705, Valid Loss: 1.4509, Valid Acc: 0.6068\n",
            "Epoch 8/10, Train Loss: 0.6290, Train Acc: 0.7655, Valid Loss: 1.4574, Valid Acc: 0.5992\n",
            "Epoch 9/10, Train Loss: 0.6098, Train Acc: 0.7732, Valid Loss: 1.4261, Valid Acc: 0.5711\n",
            "Epoch 10/10, Train Loss: 0.5943, Train Acc: 0.7816, Valid Loss: 1.3728, Valid Acc: 0.6221\n",
            "Best Validation Accuracy: 0.6221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_layers, num_heads, dim_feedforward, dropout, num_classes, pos_encoding_type, spe_params):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Positional encoding\n",
        "        if pos_encoding_type == 'sine':\n",
        "            self.pos_encoding = SineSPE(num_heads=num_heads, in_features=embedding_dim // num_heads, **spe_params)\n",
        "        elif pos_encoding_type == 'conv':\n",
        "            self.pos_encoding = ConvSPE(ndim=1, num_heads=num_heads, in_features=embedding_dim // num_heads, **spe_params)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid positional encoding type\")\n",
        "\n",
        "        # Transformer layers with TUPEMultiHeadAttention\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TUPEMultiHeadAttention(\n",
        "                config=TUPEConfig(),\n",
        "                pos_embed=self.pos_encoding\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch embedding\n",
        "        x = x.permute(0, 2, 1)  # Change shape from (batch_size, seq_len, in_channels) to (batch_size, in_channels, seq_len)\n",
        "        x = self.patch_embedding(x)  # Apply Conv1d\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Positional encoding\n",
        "        seq_len = x.size(1)\n",
        "        pos_enc = self.pos_encoding(seq_len).to(x.device)\n",
        "        x = x + pos_enc\n",
        "\n",
        "        # Transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "pMTQ5_2nyjht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SineSPE(nn.Module):\n",
        "    def __init__(self, num_heads, in_features, max_len=512):\n",
        "        super(SineSPE, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.in_features = in_features\n",
        "        self.max_len = max_len\n",
        "        self.position = nn.Parameter(torch.zeros(1, max_len, in_features))\n",
        "        self.register_buffer('sine', self._generate_sine_encoding())\n",
        "\n",
        "    def _generate_sine_encoding(self):\n",
        "        position = torch.arange(self.max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, self.in_features, 2).float() * -(math.log(10000.0) / self.in_features))\n",
        "        encoding = torch.zeros(self.max_len, self.in_features)\n",
        "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        return encoding\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        return self.sine[:seq_len, :].unsqueeze(0).repeat(self.num_heads, 1, 1)\n",
        "\n",
        "class ConvSPE(nn.Module):\n",
        "    def __init__(self, ndim, num_heads, in_features, kernel_size=7, padding=3):\n",
        "        super(ConvSPE, self).__init__()\n",
        "        self.conv = nn.Conv1d(in_features, in_features, kernel_size=kernel_size, padding=padding)\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        # Generate sinusoidal encoding\n",
        "        x = torch.arange(seq_len).unsqueeze(0).float()\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.conv(x)\n",
        "        x = x.squeeze(0).permute(1, 0).unsqueeze(0).repeat(self.num_heads, 1, 1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_qHLmzj7yl1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import math\n",
        "\n",
        "# Example of positional encoding classes\n",
        "class SineSPE(nn.Module):\n",
        "    def __init__(self, in_features, max_len=512):\n",
        "        super(SineSPE, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.max_len = max_len\n",
        "        self.position = nn.Parameter(torch.zeros(1, max_len, in_features))\n",
        "        self.register_buffer('sine', self._generate_sine_encoding())\n",
        "\n",
        "    def _generate_sine_encoding(self):\n",
        "        position = torch.arange(self.max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, self.in_features, 2).float() * -(math.log(10000.0) / self.in_features))\n",
        "        encoding = torch.zeros(self.max_len, self.in_features)\n",
        "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        return encoding\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        return self.sine[:seq_len, :].unsqueeze(0)  # Shape: (1, seq_len, in_features)\n",
        "\n",
        "\n",
        "class ConvSPE(nn.Module):\n",
        "    def __init__(self, num_heads, in_features, kernel_size=3, num_realizations=1):\n",
        "        super(ConvSPE, self).__init__()\n",
        "        padding = kernel_size // 2  # This ensures that the output size matches the input size if stride=1\n",
        "\n",
        "        # Define a 1D convolutional layer\n",
        "        self.conv = nn.Conv1d(in_features, in_features, kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.in_features = in_features\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_realizations = num_realizations\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x should be of shape (batch_size, seq_len, in_features)\n",
        "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, in_features, seq_len)\n",
        "        x = self.conv(x)  # Apply convolution\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, in_features)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model Definition\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2, pos_encoding_type='sine', spe_params={}):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Positional encoding\n",
        "        if pos_encoding_type == 'sineSPE':\n",
        "            self.pos_encoding = SineSPE(embedding_dim, **spe_params)\n",
        "        elif pos_encoding_type == 'convSPE':\n",
        "            self.pos_encoding = ConvSPE(num_heads=num_heads, in_features=embedding_dim, **spe_params)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid positional encoding type\")\n",
        "\n",
        "        # Calculate the number of patches\n",
        "        self.num_patches = -(-input_timesteps // patch_size)  # Ceiling division\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n",
        "\n",
        "        # Feedforward layer\n",
        "        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n",
        "        # Classifier Head\n",
        "        self.classifier = nn.Linear(dim_feedforward, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, in_channels, input_timesteps)\n",
        "\n",
        "        # Get patch embeddings\n",
        "        x = x.permute(0, 2, 1)  # Change shape from (batch_size, in_channels, input_timesteps) to (batch_size, input_timesteps, in_channels)\n",
        "        x = self.patch_embedding(x)  # Apply Conv1d\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, num_patches, embedding_dim)\n",
        "\n",
        "        # Apply positional encoding\n",
        "        seq_len = x.size(1)\n",
        "        pos_enc = self.pos_encoding(seq_len).to(x.device)\n",
        "        x = x + pos_enc\n",
        "\n",
        "        # Apply Transformer Encoder\n",
        "        x = self.transformer_encoder(x)  # Output shape: (batch_size, num_patches, embedding_dim)\n",
        "\n",
        "        # Use the output corresponding to the class token for classification\n",
        "        class_token_output = x[:, 0, :]  # Select the class token for each item in the batch\n",
        "\n",
        "        # Feedforward layer\n",
        "        x = self.ff_layer(class_token_output)\n",
        "\n",
        "        # Classifier head\n",
        "        output = self.classifier(x)  # Output shape: (batch_size, num_classes)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "3dIJpcJ3ypDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "TIMESTEPS = 151\n",
        "CHANNELS = 1\n",
        "patch_size = 7\n",
        "embedding_dim = 128\n",
        "num_transformer_layers = 6\n",
        "num_heads = 8\n",
        "dim_feedforward = 128\n",
        "dropout = 0.1\n",
        "num_classes = 7\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=patch_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_transformer_layers=num_transformer_layers,\n",
        "    num_heads=num_heads,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    dropout=dropout,\n",
        "    num_classes=num_classes,\n",
        "    pos_encoding_type='sineSPE',\n",
        "    spe_params={'max_len': TIMESTEPS}\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for inputs, labels in data_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss = running_loss / len(data_loader.dataset)\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    valid_loss = running_loss / len(data_loader.dataset)\n",
        "    valid_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return valid_loss, valid_acc\n",
        "\n",
        "# Training and Evaluation Loop\n",
        "num_epochs = 20  # Adjust as needed\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
        "          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if valid_accuracy > best_valid_acc:\n",
        "        best_valid_acc = valid_accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlTvPqd6zvXw",
        "outputId": "099d9d30-a1ac-44ad-ca1b-4dc8eeeccbbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.3798, Train Acc: 0.4907, Valid Loss: 1.3426, Valid Acc: 0.5346\n",
            "Epoch 2/20, Train Loss: 0.9217, Train Acc: 0.6579, Valid Loss: 1.1544, Valid Acc: 0.6359\n",
            "Epoch 3/20, Train Loss: 0.7903, Train Acc: 0.7025, Valid Loss: 1.2592, Valid Acc: 0.5891\n",
            "Epoch 4/20, Train Loss: 0.7107, Train Acc: 0.7315, Valid Loss: 1.2178, Valid Acc: 0.6016\n",
            "Epoch 5/20, Train Loss: 0.6692, Train Acc: 0.7476, Valid Loss: 1.1848, Valid Acc: 0.6279\n",
            "Epoch 6/20, Train Loss: 0.6245, Train Acc: 0.7627, Valid Loss: 1.1411, Valid Acc: 0.6529\n",
            "Epoch 7/20, Train Loss: 0.6029, Train Acc: 0.7667, Valid Loss: 1.2142, Valid Acc: 0.6211\n",
            "Epoch 8/20, Train Loss: 0.5811, Train Acc: 0.7777, Valid Loss: 1.1861, Valid Acc: 0.6565\n",
            "Epoch 9/20, Train Loss: 0.5573, Train Acc: 0.7907, Valid Loss: 1.1270, Valid Acc: 0.6789\n",
            "Epoch 10/20, Train Loss: 0.5390, Train Acc: 0.7947, Valid Loss: 1.1699, Valid Acc: 0.6625\n",
            "Epoch 11/20, Train Loss: 0.5213, Train Acc: 0.8050, Valid Loss: 1.2289, Valid Acc: 0.6448\n",
            "Epoch 12/20, Train Loss: 0.5052, Train Acc: 0.8088, Valid Loss: 1.1549, Valid Acc: 0.6818\n",
            "Epoch 13/20, Train Loss: 0.4929, Train Acc: 0.8112, Valid Loss: 1.1912, Valid Acc: 0.6721\n",
            "Epoch 14/20, Train Loss: 0.4750, Train Acc: 0.8207, Valid Loss: 1.1575, Valid Acc: 0.6750\n",
            "Epoch 15/20, Train Loss: 0.4660, Train Acc: 0.8266, Valid Loss: 1.1789, Valid Acc: 0.6893\n",
            "Epoch 16/20, Train Loss: 0.4471, Train Acc: 0.8291, Valid Loss: 1.1560, Valid Acc: 0.6930\n",
            "Epoch 17/20, Train Loss: 0.4411, Train Acc: 0.8323, Valid Loss: 1.2197, Valid Acc: 0.6786\n",
            "Epoch 18/20, Train Loss: 0.4241, Train Acc: 0.8422, Valid Loss: 1.2158, Valid Acc: 0.6755\n",
            "Epoch 19/20, Train Loss: 0.4186, Train Acc: 0.8450, Valid Loss: 1.3273, Valid Acc: 0.6594\n",
            "Epoch 20/20, Train Loss: 0.4084, Train Acc: 0.8485, Valid Loss: 1.2159, Valid Acc: 0.6818\n",
            "Best Validation Accuracy: 0.6930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "TIMESTEPS = 151\n",
        "CHANNELS = 1\n",
        "patch_size = 7\n",
        "embedding_dim = 128\n",
        "num_transformer_layers = 6\n",
        "num_heads = 8\n",
        "dim_feedforward = 128\n",
        "dropout = 0.1\n",
        "num_classes = 7\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=patch_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_transformer_layers=num_transformer_layers,\n",
        "    num_heads=num_heads,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    dropout=dropout,\n",
        "    num_classes=num_classes,\n",
        "    pos_encoding_type='sineSPE',\n",
        "    spe_params={'max_len': TIMESTEPS}\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for inputs, labels in data_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss = running_loss / len(data_loader.dataset)\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    valid_loss = running_loss / len(data_loader.dataset)\n",
        "    valid_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return valid_loss, valid_acc\n",
        "\n",
        "# Training and Evaluation Loop\n",
        "num_epochs = 10  # Adjust as needed\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
        "          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if valid_accuracy > best_valid_acc:\n",
        "        best_valid_acc = valid_accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34GXwW8DBnzG",
        "outputId": "9ffe61ae-7386-4ce0-9621-b990fa4f11c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 1.2981, Train Acc: 0.5203, Valid Loss: 1.2036, Valid Acc: 0.5896\n",
            "Epoch 2/10, Train Loss: 0.9311, Train Acc: 0.6453, Valid Loss: 1.1766, Valid Acc: 0.6042\n",
            "Epoch 3/10, Train Loss: 0.7983, Train Acc: 0.6981, Valid Loss: 1.0938, Valid Acc: 0.6464\n",
            "Epoch 4/10, Train Loss: 0.7320, Train Acc: 0.7191, Valid Loss: 1.0639, Valid Acc: 0.6464\n",
            "Epoch 5/10, Train Loss: 0.6822, Train Acc: 0.7443, Valid Loss: 1.1370, Valid Acc: 0.6203\n",
            "Epoch 6/10, Train Loss: 0.6452, Train Acc: 0.7563, Valid Loss: 1.1129, Valid Acc: 0.6435\n",
            "Epoch 7/10, Train Loss: 0.6135, Train Acc: 0.7720, Valid Loss: 1.1698, Valid Acc: 0.6141\n",
            "Epoch 8/10, Train Loss: 0.5861, Train Acc: 0.7829, Valid Loss: 1.1251, Valid Acc: 0.6414\n",
            "Epoch 9/10, Train Loss: 0.5778, Train Acc: 0.7844, Valid Loss: 1.1534, Valid Acc: 0.6461\n",
            "Epoch 10/10, Train Loss: 0.5596, Train Acc: 0.7941, Valid Loss: 1.1551, Valid Acc: 0.6539\n",
            "Best Validation Accuracy: 0.6539\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}