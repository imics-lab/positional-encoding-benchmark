{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"4L4RZaqkWMbr","executionInfo":{"status":"ok","timestamp":1728494795898,"user_tz":300,"elapsed":134,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}}},"outputs":[],"source":["import os\n","import shutil  # https://docs.python.org/3/library/shutil.html\n","from shutil import unpack_archive  # to unzip\n","# from shutil import make_archive # to create zip for storage\n","import requests  # for downloading zip file\n","from scipy import io  # for loadmat, matlab conversion\n","import numpy as np\n","import urllib.request # to get files from web w/o !wget\n","\n","def namestr(obj, namespace):\n","    return [name for name in namespace if namespace[name] is obj]\n","\n","def get_shapes(np_arr_list):\n","    \"\"\"Returns text, each line is shape and dtype for numpy array in list\n","       example: print(get_shapes([X_train, X_test, y_train, y_test]))\"\"\"\n","    shapes = \"\"\n","    for i in np_arr_list:\n","        my_name = namestr(i,globals())\n","        shapes += (my_name[0] + \" shape is \" + str(i.shape) \\\n","            + \" data type is \" + str(i.dtype) + \"\\n\")\n","    return shapes"]},{"cell_type":"code","source":["def get_py_file(fname, url):\n","    \"\"\"checks for local file, if none downloads from URL.\n","    :return: nothing\"\"\"\n","    if (os.path.exists(fname)):\n","        print (\"Local\",fname, \"found, skipping download\")\n","    else:\n","        print(\"Downloading\",fname, \"from IMICS git repo\")\n","        urllib.request.urlretrieve(url, filename=fname)\n","\n","get_py_file(fname = 'load_data_utils.py', url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_utils.py')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1WkdQ8gCUS-M","executionInfo":{"status":"ok","timestamp":1728494798891,"user_tz":300,"elapsed":224,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"c57cb51f-28a9-4b2e-a70c-62bb29e64df9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading load_data_utils.py from IMICS git repo\n"]}]},{"cell_type":"code","source":["!gdown \"1P5PIYeYvbfL4kQj-P2sm-JquUIddAxec&confirm=t\" # ADL_Leotta_2021.zip\n","get_py_file(fname = 'leotta_2021_load_dataset.py',\n","            url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/ADL/Leotta_2021/leotta_2021_load_dataset.py')\n","    #full_filename = my_path+os.path.join('/ADL/Leotta_2021/'+'leotta_2021_load_dataset.py')\n","    #shutil.copy(full_filename,'leotta_2021_load_dataset.py')\n","import leotta_2021_load_dataset as leotta\n","# Load the dataset without one-hot encoding\n","x_train, y_train, x_valid, y_valid, x_test, y_test = leotta.leotta_2021_load_dataset(incl_val_group=True, one_hot_encode=False)\n","\n","# Perform one-hot encoding manually\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# Assuming 'label_map_leotta' is accessible for getting the number of labels\n","label_map = leotta.label_map_leotta\n","num_labels = len(label_map['label'])\n","all_categories = [[str(i) for i in range(num_labels)]]\n","\n","enc = OneHotEncoder(categories=all_categories, sparse_output=False)\n","\n","# Reshape y values to 2D array for encoding and then reshape back\n","y_train_oh = enc.fit_transform(y_train.reshape(-1, 1))\n","y_valid_oh = enc.transform(y_valid.reshape(-1, 1))\n","y_test_oh = enc.transform(y_test.reshape(-1, 1))\n","\n","t_names = list(leotta_2021_load_dataset.label_map_leotta.get('label').keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lVqLwGJrUbyy","executionInfo":{"status":"ok","timestamp":1728495198373,"user_tz":300,"elapsed":35001,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"afd54344-8240-48a8-f263-67a2a591ea9e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1P5PIYeYvbfL4kQj-P2sm-JquUIddAxec&confirm=t\n","To: /content/ADL_Leotta_2021.zip\n","100% 258M/258M [00:01<00:00, 183MB/s]\n","Local leotta_2021_load_dataset.py found, skipping download\n","Using existing Leotta archive in ./dataset\n"]},{"output_type":"stream","name":"stderr","text":["/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n"]}]},{"cell_type":"code","source":["import leotta_2021_load_dataset as leotta\n","\n","# Load the dataset without one-hot encoding\n","x_train, y_train, x_valid, y_valid, x_test, y_test = leotta.leotta_2021_load_dataset(incl_val_group=True, one_hot_encode=False)\n","\n","# Ensure the shapes are as expected\n","print(f\"X_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n","print(f\"X_valid shape: {x_valid.shape}, y_valid shape: {y_valid.shape}\")\n","print(f\"X_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n","\n","# Calculate and print the number of classes\n","n_classes = len(np.unique(y_train))\n","print(f\"Number of classes: {n_classes}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CU7EB3PZXTdQ","executionInfo":{"status":"ok","timestamp":1728495601928,"user_tz":300,"elapsed":33522,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"8e2d95ab-70fc-448f-d3b6-361e9f817c16"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Using existing Leotta archive in ./dataset\n"]},{"output_type":"stream","name":"stderr","text":["/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n","/content/load_data_transforms.py:134: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n","  df[label_name] = df[label_name].replace(label_mapping_dict[label_name])\n"]},{"output_type":"stream","name":"stdout","text":["X_train shape: (3733, 300, 3), y_train shape: (3733, 1)\n","X_valid shape: (1755, 300, 3), y_valid shape: (1755, 1)\n","X_test shape: (1987, 300, 3), y_test shape: (1987, 1)\n","Number of classes: 18\n"]}]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4oglWkA3WMbs","executionInfo":{"status":"ok","timestamp":1728495440347,"user_tz":300,"elapsed":137,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"e7be5e6f-66f8-48b4-e8a6-beb70b53f202"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train shape: (3733, 300, 3), y_train shape: (3733, 18)\n","X_valid shape: (1755, 300, 3), y_valid shape: (1755, 18)\n","X_test shape: (1987, 300, 3), y_test shape: (1987, 18)\n","Number of classes: 18\n"]}],"source":["# Number of classes\n","n_classes = len(np.unique(y_train))\n","print(f\"X_train shape: {x_train.shape}, y_train shape: {y_train_oh.shape}\")\n","print(f\"X_valid shape: {x_valid.shape}, y_valid shape: {y_valid_oh.shape}\")\n","print(f\"X_test shape: {x_test.shape}, y_test shape: {y_test_oh.shape}\")\n","print(f\"Number of classes: {n_classes}\")"]},{"cell_type":"markdown","metadata":{"id":"KzMPbGeUWMbt"},"source":["## 2. Data Preprocessing"]},{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NDT8m0OYWpQj","executionInfo":{"status":"ok","timestamp":1728495336076,"user_tz":300,"elapsed":4443,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"19cf2573-4569-43d3-83ac-264afaacaf36"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}]},{"cell_type":"code","execution_count":13,"metadata":{"id":"QAP9J-zwWMbt","executionInfo":{"status":"ok","timestamp":1728495355012,"user_tz":300,"elapsed":7765,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import classification_report, confusion_matrix\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchinfo import summary"]},{"cell_type":"markdown","metadata":{"id":"Sv80iRYUWMbt"},"source":["### Load the train, validation, and test sets into PyTorch DataLoaders"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"0UV-Y5vfWMbt","executionInfo":{"status":"ok","timestamp":1728495660920,"user_tz":300,"elapsed":150,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}}},"outputs":[],"source":["X_train = torch.tensor(x_train, dtype=torch.float32)\n","Y_train = torch.tensor(y_train, dtype=torch.int64)\n","\n","X_valid = torch.tensor(x_valid, dtype=torch.float32)\n","Y_valid = torch.tensor(y_valid, dtype=torch.int64)\n","\n","X_test = torch.tensor(x_test, dtype=torch.float32)\n","Y_test = torch.tensor(y_test, dtype=torch.int64)\n","\n","# DataLoaders\n","batch_size = 64\n","train_dataset = TensorDataset(X_train, Y_train)\n","train_loader = DataLoader(\n","    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","valid_dataset = TensorDataset(X_valid, Y_valid)\n","valid_loader = DataLoader(\n","    valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n","\n","test_dataset = TensorDataset(X_test, Y_test)\n","test_loader = DataLoader(\n","    test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"alrtTQdHWMbt"},"source":["## 3. Model Building"]},{"cell_type":"markdown","metadata":{"id":"y9VxJySSWMbt"},"source":["The `TimeSeriesPatchEmbeddingLayer` class is a PyTorch module used to create an embedding layer for time series data. It takes as input the number of channels, patch size, embedding dimension, and input timesteps. The class calculates the number of patches and padding needed, and initializes a 1D convolutional layer, class token embeddings, and position embeddings. The forward method pads the input if necessary, applies the convolutional layer, permutes the output, and concatenates the class tokens. Finally, it adds the position embeddings to the output."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"j1tFm3BmWMbu","executionInfo":{"status":"ok","timestamp":1728495682188,"user_tz":300,"elapsed":172,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}}},"outputs":[],"source":["import math\n","\n","class TimeSeriesPatchEmbeddingLayer(nn.Module):\n","    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps):\n","        super().__init__()\n","        self.patch_size = patch_size\n","        self.embedding_dim = embedding_dim\n","        self.in_channels = in_channels\n","\n","        # Calculate the number of patches, adjusting for padding if necessary\n","        # Ceiling division to account for padding\n","        self.num_patches = -(-input_timesteps // patch_size)\n","        self.padding = (\n","            self.num_patches * patch_size\n","        ) - input_timesteps  # Calculate padding length\n","\n","        self.conv_layer = nn.Conv1d(\n","            in_channels=in_channels,\n","            out_channels=embedding_dim,\n","            kernel_size=patch_size,\n","            stride=patch_size,\n","        )\n","\n","        self.class_token_embeddings = nn.Parameter(\n","            torch.randn((1, 1, embedding_dim), requires_grad=True)\n","        )\n","        self.position_embeddings = PositionalEncoding(embedding_dim, dropout=0.1, max_len=input_timesteps)\n","\n","    def forward(self, x):\n","        # Pad the input sequence if necessary\n","        if self.padding > 0:\n","            x = nn.functional.pad(x, (0, 0, 0, self.padding))  # Pad the second to last dimension, which is input_timesteps\n","\n","        # We use a Conv1d layer to generate the patch embeddings\n","        x = x.permute(0, 2, 1)  # (batch, features, timesteps)\n","        conv_output = self.conv_layer(x)\n","        conv_output = conv_output.permute(0, 2, 1)  # (batch, timesteps, features)\n","\n","        batch_size = x.shape[0]\n","        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n","        output = torch.cat((class_tokens, conv_output), dim=1)\n","\n","        output = self.position_embeddings(output)\n","\n","        return output\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def pos_encoding(self, q_len, d_model, normalize=True):\n","        pe = torch.zeros(q_len, d_model)\n","        position = torch.arange(0, q_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        if normalize:\n","            pe = pe - pe.mean()\n","            pe = pe / (pe.std() * 10)\n","        return pe\n","\n","    def forward(self, x):\n","        x = x + self.pos_encoding(q_len = x.size(1), d_model = x.size(2))\n","        return self.dropout(x)\n","\n","class LearnedPositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(LearnedPositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1), :]\n","        return self.dropout(x)\n","\n","\n","def get_pos_encoder(pos_encoding):\n","    if pos_encoding == 'fixed':\n","        return PositionalEncoding\n","    elif pos_encoding == 'learned':\n","        return LearnedPositionalEncoding\n","    else:\n","        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U0THG7-nWMbu"},"source":["Let's use the `summary` function to display the architecture of the embedding layer."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d8MKozUTWMbu","executionInfo":{"status":"ok","timestamp":1728495684882,"user_tz":300,"elapsed":150,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"da9d5381-afda-4753-896c-ebbe9fa1e4f0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["=================================================================================================================================================\n","Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n","=================================================================================================================================================\n","TimeSeriesPatchEmbeddingLayer (TimeSeriesPatchEmbeddingLayer)     [64, 300, 3]         [64, 39, 24]         24                   True\n","├─Conv1d (conv_layer)                                             [64, 3, 304]         [64, 24, 38]         600                  True\n","├─PositionalEncoding (position_embeddings)                        [64, 39, 24]         [64, 39, 24]         --                   --\n","│    └─Dropout (dropout)                                          [64, 39, 24]         [64, 39, 24]         --                   --\n","=================================================================================================================================================\n","Total params: 624\n","Trainable params: 624\n","Non-trainable params: 0\n","Total mult-adds (M): 1.46\n","=================================================================================================================================================\n","Input size (MB): 0.23\n","Forward/backward pass size (MB): 0.47\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 0.70\n","================================================================================================================================================="]},"metadata":{},"execution_count":25}],"source":["random_instances, random_labels = next(iter(train_loader))\n","random_instance = random_instances[0]\n","\n","BATCH_SIZE = random_instances.shape[0]\n","TIMESTEPS = random_instance.shape[0]\n","CHANNELS = random_instance.shape[1]\n","PATCH_SIZE = 8\n","\n","patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n","    in_channels=CHANNELS,\n","    patch_size=PATCH_SIZE,\n","    embedding_dim=CHANNELS * PATCH_SIZE,\n","    input_timesteps=TIMESTEPS,\n",")\n","\n","patch_embeddings = patch_embedding_layer(random_instances)\n","patch_embeddings.shape\n","\n","summary(\n","    model=patch_embedding_layer,\n","    # (batch_size, input_channels, input_timesteps)\n","    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n","    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","    col_width=20,\n","    row_settings=[\"var_names\"],\n",")"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"s30BKFOOWMbu","executionInfo":{"status":"ok","timestamp":1728495687907,"user_tz":300,"elapsed":132,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}}},"outputs":[],"source":["from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","class TimeSeriesTransformer(nn.Module):\n","    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2):\n","        super().__init__()\n","\n","        # Embedding layer\n","        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps)\n","\n","        # Calculate the number of patches\n","        self.num_patches = -(-input_timesteps // patch_size)\n","\n","        # Transformer Encoder\n","        # Setting batch_first=True to accommodate inputs with batch dimension first\n","        encoder_layers = TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n","        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n","\n","        # Feedforward layer\n","        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n","        # Classifier Head\n","        self.classifier = nn.Linear(dim_feedforward, num_classes)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, in_channels, input_timesteps)\n","\n","        # Get patch embeddings\n","        x = self.patch_embedding(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n","\n","        # Apply Transformer Encoder with batch first\n","        x = self.transformer_encoder(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n","\n","        # Use the output corresponding to the class token for classification\n","        class_token_output = x[:, 0, :]  # Select the class token for each item in the batch\n","\n","        # Feedforward layer\n","        x = self.ff_layer(class_token_output)\n","\n","        # Classifier head\n","        output = self.classifier(x)  # Output shape: (batch_size, num_classes)\n","\n","        return output\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3tGq9kmWMbu","executionInfo":{"status":"ok","timestamp":1728495694653,"user_tz":300,"elapsed":142,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"b4ec9f59-efc4-44eb-c134-d0a2c0c642b0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["=======================================================================================================================================\n","Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n","=======================================================================================================================================\n","TimeSeriesTransformer (TimeSeriesTransformer)           [64, 300, 3]         [64, 18]             --                   True\n","├─TimeSeriesPatchEmbeddingLayer (patch_embedding)       [64, 300, 3]         [64, 39, 32]         32                   True\n","│    └─Conv1d (conv_layer)                              [64, 3, 304]         [64, 32, 38]         800                  True\n","│    └─PositionalEncoding (position_embeddings)         [64, 39, 32]         [64, 39, 32]         --                   --\n","│    │    └─Dropout (dropout)                           [64, 39, 32]         [64, 39, 32]         --                   --\n","├─TransformerEncoder (transformer_encoder)              [64, 39, 32]         [64, 39, 32]         --                   True\n","│    └─ModuleList (layers)                              --                   --                   --                   True\n","│    │    └─TransformerEncoderLayer (0)                 [64, 39, 32]         [64, 39, 32]         12,704               True\n","│    │    └─TransformerEncoderLayer (1)                 [64, 39, 32]         [64, 39, 32]         12,704               True\n","│    │    └─TransformerEncoderLayer (2)                 [64, 39, 32]         [64, 39, 32]         12,704               True\n","│    │    └─TransformerEncoderLayer (3)                 [64, 39, 32]         [64, 39, 32]         12,704               True\n","├─Linear (ff_layer)                                     [64, 32]             [64, 128]            4,224                True\n","├─Linear (classifier)                                   [64, 128]            [64, 18]             2,322                True\n","=======================================================================================================================================\n","Total params: 58,194\n","Trainable params: 58,194\n","Non-trainable params: 0\n","Total mult-adds (M): 2.36\n","=======================================================================================================================================\n","Input size (MB): 0.23\n","Forward/backward pass size (MB): 0.70\n","Params size (MB): 0.03\n","Estimated Total Size (MB): 0.96\n","======================================================================================================================================="]},"metadata":{},"execution_count":27}],"source":["# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Instantiate the model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=8,\n","    embedding_dim=32,\n","    num_transformer_layers=4,\n","    num_heads=4,\n","    dim_feedforward=128,\n","    dropout=0.2,\n","    num_classes=n_classes,\n",").to(device)\n","\n","# Define the loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Define the learning rate scheduler to reduce the learning rate by the specified step size and factor (gamma) every step_size epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n","\n","summary(\n","    model=model,\n","    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n","    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","    col_width=20,\n","    row_settings=[\"var_names\"],\n",")"]},{"cell_type":"markdown","metadata":{"id":"tPO8SLCpWMbu"},"source":["## 4. Model Training"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"Smirk3qsWMbu","executionInfo":{"status":"error","timestamp":1728495876558,"user_tz":300,"elapsed":24969,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"a1d6d875-c44c-4eb6-e656-9f7a9b348a1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: New best model saved with validation accuracy: 1.0000\n","Epoch 1, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n","Epoch 2, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n","Epoch 3, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-9c427d5f5b19>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Model, loss function, and optimizer\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","# Number of epochs\n","n_epochs = 100\n","\n","# Initialize variables for tracking the best model\n","best_validation_acc = 0.0\n","best_model_path = 'best_model_v2.pth'\n","\n","for epoch in range(n_epochs):\n","    model.train()  # Set model to training mode\n","    train_losses = []\n","    train_correct = 0\n","    total = 0\n","\n","    # Training loop\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()  # Zero the parameter gradients\n","\n","        predictions = model(inputs)  # Forward pass\n","        labels = labels.argmax(dim=1)\n","        loss = criterion(predictions, labels)  # Calculate loss\n","\n","        loss.backward()  # Backward pass\n","        optimizer.step()  # Optimize\n","\n","        train_losses.append(loss.item())\n","\n","        # Count the number of correct predictions\n","        train_correct += (predictions.argmax(1) == labels).sum().item()\n","        total += labels.size(0)\n","\n","    train_loss = np.mean(train_losses)\n","    train_acc = train_correct / total\n","\n","    # Validation loop\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        validation_losses = []\n","        validation_correct = 0\n","        total_val = 0\n","\n","        for inputs, labels in valid_loader:\n","            predictions = model(inputs)\n","            labels = labels.argmax(dim=1)\n","            loss = criterion(predictions, labels)\n","            validation_losses.append(loss.item())\n","\n","            validation_correct += (predictions.argmax(1) == labels).sum().item()\n","            total_val += labels.size(0)\n","\n","        validation_loss = np.mean(validation_losses)\n","        validation_acc = validation_correct / total_val\n","\n","    # Check if this is the best model so far\n","    if validation_acc > best_validation_acc:\n","        best_validation_acc = validation_acc\n","        # Save the model\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {validation_acc:.4f}')\n","\n","    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {validation_loss:.4f}, Val Acc: {validation_acc:.4f}')\n","\n","# Loading the best model\n","model.load_state_dict(torch.load(best_model_path))\n","print('Loaded best model for testing or further use.')"]},{"cell_type":"markdown","metadata":{"id":"Na4sOhN7WMbv"},"source":["## 5. Model Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iyotm-hrWMbv","executionInfo":{"status":"ok","timestamp":1720191162175,"user_tz":300,"elapsed":578,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"c861e1df-b164-49e3-bee9-2c29df3e998b"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.51      0.62      0.56        34\n","           1       0.48      0.57      0.52        47\n","           2       0.72      0.94      0.81       344\n","           3       0.99      0.86      0.92       413\n","           4       0.79      0.59      0.68       184\n","           5       1.00      0.95      0.97       146\n","           6       0.76      0.74      0.75       256\n","           7       0.72      0.69      0.71        68\n","           8       0.54      0.44      0.48        32\n","\n","    accuracy                           0.80      1524\n","   macro avg       0.72      0.71      0.71      1524\n","weighted avg       0.82      0.80      0.80      1524\n","\n","Confusion matrix:\n","[[ 21   8   0   0   0   0   0   1   4]\n"," [ 10  27   0   0   0   0   0  10   0]\n"," [  0   0 322   1  21   0   0   0   0]\n"," [  0   0   0 356   1   0  56   0   0]\n"," [  0   7  63   0 109   0   5   0   0]\n"," [  0   0   1   3   4 138   0   0   0]\n"," [  0   0  64   0   3   0 189   0   0]\n"," [  4   9   0   0   0   0   0  47   8]\n"," [  6   5   0   0   0   0   0   7  14]]\n"]}],"source":["# Prediction\n","model.eval()\n","with torch.no_grad():\n","    Y_pred_prob = model(X_test)\n","\n","Y_pred = Y_pred_prob.argmax(1)\n","\n","print(classification_report(Y_test, Y_pred))\n","confusion = confusion_matrix(Y_test, Y_pred)\n","print(f\"Confusion matrix:\\n{confusion}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"LhQeXQWnWMbv","executionInfo":{"status":"ok","timestamp":1720191170371,"user_tz":300,"elapsed":1507,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"230da16f-a4ee-41d6-ca1d-4fc797e026c2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# Visualize the confusion matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n","plt.xlabel(\"Prediction\")\n","plt.ylabel(\"Target\")\n","plt.title(\"Confusion Matrix\")\n","plt.show()"]},{"cell_type":"markdown","source":["## Positional Encodings"],"metadata":{"id":"PHiMwsYvVAzp"}},{"cell_type":"code","source":["!pip install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4rwyM_sG-KTN","executionInfo":{"status":"ok","timestamp":1726503825531,"user_tz":300,"elapsed":3086,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"b7592b3d-e528-491d-cbb8-e288d019a438"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from einops import rearrange\n","import pandas as pd\n","\n","class Attention(nn.Module):\n","    def __init__(self, emb_size, num_heads, dropout):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.scale = emb_size ** -0.5\n","        self.key = nn.Linear(emb_size, emb_size, bias=False)\n","        self.value = nn.Linear(emb_size, emb_size, bias=False)\n","        self.query = nn.Linear(emb_size, emb_size, bias=False)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_out = nn.LayerNorm(emb_size)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.shape\n","        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","\n","        attn = torch.matmul(q, k) * self.scale\n","        attn = nn.functional.softmax(attn, dim=-1)\n","\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2)\n","        out = out.reshape(batch_size, seq_len, -1)\n","        out = self.to_out(out)\n","        return out\n","\n","\n","class Attention_Rel_Scl(nn.Module):\n","    def __init__(self, emb_size, num_heads, seq_len, dropout):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.num_heads = num_heads\n","        self.scale = emb_size ** -0.5\n","        self.key = nn.Linear(emb_size, emb_size, bias=False)\n","        self.value = nn.Linear(emb_size, emb_size, bias=False)\n","        self.query = nn.Linear(emb_size, emb_size, bias=False)\n","\n","        self.relative_bias_table = nn.Parameter(torch.zeros((2 * self.seq_len - 1), num_heads))\n","        coords = torch.meshgrid((torch.arange(1), torch.arange(self.seq_len)))\n","        coords = torch.flatten(torch.stack(coords), 1)\n","        relative_coords = coords[:, :, None] - coords[:, None, :]\n","        relative_coords[1] += self.seq_len - 1\n","        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n","        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n","        self.register_buffer(\"relative_index\", relative_index)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_out = nn.LayerNorm(emb_size)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.shape\n","        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","\n","        attn = torch.matmul(q, k) * self.scale\n","        attn = nn.functional.softmax(attn, dim=-1)\n","\n","        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, 8))\n","        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=1 * self.seq_len, w=1 * self.seq_len)\n","        attn = attn + relative_bias\n","\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2)\n","        out = out.reshape(batch_size, seq_len, -1)\n","        out = self.to_out(out)\n","        return out\n","\n","\n","class Attention_Rel_Vec(nn.Module):\n","    def __init__(self, emb_size, num_heads, seq_len, dropout):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.num_heads = num_heads\n","        self.scale = emb_size ** -0.5\n","        self.key = nn.Linear(emb_size, emb_size, bias=False)\n","        self.value = nn.Linear(emb_size, emb_size, bias=False)\n","        self.query = nn.Linear(emb_size, emb_size, bias=False)\n","\n","        self.Er = nn.Parameter(torch.randn(self.seq_len, int(emb_size / num_heads)))\n","\n","        self.register_buffer(\n","            \"mask\",\n","            torch.tril(torch.ones(self.seq_len, self.seq_len))\n","                .unsqueeze(0).unsqueeze(0)\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_out = nn.LayerNorm(emb_size)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.shape\n","        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","\n","        QEr = torch.matmul(q, self.Er.transpose(0, 1))\n","        Srel = self.skew(QEr)\n","\n","        attn = torch.matmul(q, k)\n","        attn = (attn + Srel) * self.scale\n","\n","        attn = nn.functional.softmax(attn, dim=-1)\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2)\n","        out = out.reshape(batch_size, seq_len, -1)\n","        out = self.to_out(out)\n","        return out\n","\n","    def skew(self, QEr):\n","        padded = nn.functional.pad(QEr, (1, 0))\n","        batch_size, num_heads, num_rows, num_cols = padded.shape\n","        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n","        Srel = reshaped[:, :, 1:, :]\n","        return Srel\n"],"metadata":{"id":"yD7K5F3D-Hji"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TUPEConfig:\n","    num_layers: int = 6\n","    num_heads: int = 8\n","    d_model: int = 128\n","    d_head: int = 0\n","    max_len: int = 256\n","    dropout: float = 0.1\n","    expansion_factor: int = 1\n","    relative_bias: bool = True\n","    bidirectional_bias: bool = True\n","    num_buckets: int = 32\n","    max_distance: int = 128\n","\n","    def __post_init__(self):\n","        d_head, remainder = divmod(self.d_model, self.num_heads)\n","        assert remainder == 0, \"`d_model` should be divisible by `num_heads`\"\n","        self.d_head = d_head"],"metadata":{"id":"B2e6n_Yj_38p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _get_relative_position_bucket(\n","    relative_position, bidirectional, num_buckets, max_distance\n","):\n","    \"\"\"\n","    from https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/modeling_t5.py\n","    \"\"\"\n","    relative_buckets = 0\n","    if bidirectional:\n","        num_buckets //= 2\n","        relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n","        relative_position = torch.abs(relative_position)\n","    else:\n","        relative_position = -torch.min(\n","            relative_position, torch.zeros_like(relative_position)\n","        )\n","    # now relative_position is in the range [0, inf)\n","\n","    # half of the buckets are for exact increments in positions\n","    max_exact = num_buckets // 2\n","    is_small = relative_position < max_exact\n","\n","    # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n","    relative_postion_if_large = max_exact + (\n","        torch.log(relative_position.float() / max_exact)\n","        / math.log(max_distance / max_exact)\n","        * (num_buckets - max_exact)\n","    ).to(torch.long)\n","    relative_postion_if_large = torch.min(\n","        relative_postion_if_large,\n","        torch.full_like(relative_postion_if_large, num_buckets - 1),\n","    )\n","\n","    relative_buckets += torch.where(\n","        is_small, relative_position, relative_postion_if_large\n","    )\n","    return relative_buckets\n","\n","\n","def get_relative_positions(\n","    seq_len, bidirectional=True, num_buckets=32, max_distance=128\n","):\n","    x = torch.arange(seq_len)[None, :]\n","    y = torch.arange(seq_len)[:, None]\n","    relative_positions = _get_relative_position_bucket(\n","        x - y, bidirectional, num_buckets, max_distance\n","    )\n","    return relative_positions"],"metadata":{"id":"1QbW7ihvBByl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TUPEMultiHeadAttention(nn.Module):\n","    def __init__(self, config: TUPEConfig, pos_embed: nn.Module) -> None:\n","        super().__init__()\n","        self.max_len = config.max_len\n","        self.num_heads = config.num_heads\n","        self.num_buckets = config.num_buckets\n","        self.max_distance = config.max_distance\n","        self.bidirectional = config.bidirectional_bias\n","        self.scale = math.sqrt(2 * config.d_head)\n","\n","        self.pos_embed = pos_embed\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","        # kqv in one pass\n","        self.pos_kq = nn.Linear(config.d_model, 2 * config.d_model, bias=False)\n","        self.tok_kqv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n","\n","        self.relative_bias = config.relative_bias\n","        if config.relative_bias:\n","            self.bias = nn.Embedding(config.max_len * 2, config.num_heads)\n","\n","    def forward(self, x: torch.tensor) -> torch.tensor:\n","        batch_size, seq_len, _ = x.shape\n","\n","        pos_embed = self.pos_embed(seq_len).repeat(batch_size, 1, 1)\n","        # pos_embed.shape == (batch_size, seq_len, d_model)\n","        pos_key, pos_query = self.pos_kq(pos_embed).chunk(2, dim=-1)\n","        pos_key = pos_key.view(batch_size, seq_len, self.num_heads, -1).permute(\n","            0, 2, 3, 1\n","        )\n","        # pos_key.shape == (batch_size, num_heads, d_head, seq_len)\n","        pos_query = pos_query.view(batch_size, seq_len, self.num_heads, -1).transpose(\n","            1, 2\n","        )\n","        # pos_query.shape == (batch_size, num_heads, seq_len, d_head)\n","        pos_attn = torch.matmul(pos_query, pos_key)\n","        # pos_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n","\n","        tok_key, tok_query, tok_value = self.tok_kqv(x).chunk(3, dim=-1)\n","        tok_key = tok_key.view(batch_size, seq_len, self.num_heads, -1).permute(\n","            0, 2, 3, 1\n","        )\n","        # tok_key.shape == (batch_size, num_heads, d_head, seq_len)\n","        tok_query = tok_query.view(batch_size, seq_len, self.num_heads, -1).transpose(\n","            1, 2\n","        )\n","        tok_value = tok_value.view(batch_size, seq_len, self.num_heads, -1).transpose(\n","            1, 2\n","        )\n","        # tok_qv.shape == (batch_size, num_heads, seq_len, d_head)\n","        tok_attn = torch.matmul(tok_query, tok_key)\n","        # tok_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n","\n","        attn = (tok_attn + pos_attn) / self.scale\n","        if self.relative_bias:\n","            relative_positions = get_relative_positions(\n","                seq_len, self.bidirectional, self.num_buckets, self.max_distance\n","            ).to(attn.device)\n","            # relative_positions.shape == (seq_len, seq_len)\n","            bias = self.bias(relative_positions + self.max_len)\n","            # bias.shape == (seq_len, seq_len, num_heads)\n","            bias = bias.permute(2, 0, 1).unsqueeze(0)\n","            # bias.shape == (1, num_heads, seq_len, seq_len)\n","            attn = attn + bias\n","\n","        attn = F.softmax(attn, dim=-1)\n","        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n","        out = torch.matmul(attn, tok_value)\n","        # out.shape == (batch_size, num_heads, seq_len, d_head)\n","        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n","        # out.shape == (batch_size, seq_len, d_model)\n","        out = self.dropout(out)\n","        return out"],"metadata":{"id":"9uW0SNXu_oda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","import math\n","from torchsummary import summary\n","\n","# Positional Encoding\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def pos_encoding(self, q_len, d_model, normalize=True):\n","        pe = torch.zeros(q_len, d_model)\n","        position = torch.arange(0, q_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        if normalize:\n","            pe = pe - pe.mean()\n","            pe = pe / (pe.std() * 10)\n","        return pe\n","\n","    def forward(self, x):\n","        x = x + self.pos_encoding(q_len=x.size(1), d_model=x.size(2))\n","        return self.dropout(x)\n","\n","# Learned Positional Encoding\n","class LearnedPositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(LearnedPositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1), :]\n","        return self.dropout(x)\n","\n","# Time Series Patch Embedding Layer\n","class TimeSeriesPatchEmbeddingLayer(nn.Module):\n","    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps, pos_encoding='fixed'):\n","        super().__init__()\n","        self.patch_size = patch_size\n","        self.embedding_dim = embedding_dim\n","        self.in_channels = in_channels\n","\n","        self.num_patches = -(-input_timesteps // patch_size)\n","        self.padding = (self.num_patches * patch_size) - input_timesteps\n","\n","        self.conv_layer = nn.Conv1d(\n","            in_channels=in_channels,\n","            out_channels=embedding_dim,\n","            kernel_size=patch_size,\n","            stride=patch_size,\n","        )\n","\n","        self.class_token_embeddings = nn.Parameter(\n","            torch.randn((1, 1, embedding_dim), requires_grad=True)\n","        )\n","\n","        # Instantiate the positional encoding\n","        pos_encoder_class = get_pos_encoder(pos_encoding)\n","        self.position_embeddings = pos_encoder_class(embedding_dim, dropout=0.1, max_len=input_timesteps)\n","\n","    def forward(self, x):\n","        if self.padding > 0:\n","            x = nn.functional.pad(x, (0, 0, 0, self.padding))\n","\n","        x = x.permute(0, 2, 1)\n","        conv_output = self.conv_layer(x)\n","        conv_output = conv_output.permute(0, 2, 1)\n","\n","        batch_size = x.shape[0]\n","        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n","        output = torch.cat((class_tokens, conv_output), dim=1)\n","\n","        output = self.position_embeddings(output)\n","\n","        return output\n","\n","# Get Positional Encoding\n","def get_pos_encoder(pos_encoding):\n","    if pos_encoding == 'fixed':\n","        return PositionalEncoding\n","    elif pos_encoding == 'learned':\n","        return LearnedPositionalEncoding\n","    else:\n","        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n","\n","class TimeSeriesTransformer(nn.Module):\n","    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, pos_encoding='fixed', num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2, attention_type='absolute'):\n","        super().__init__()\n","\n","        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps, pos_encoding)\n","        self.num_patches = -(-input_timesteps // patch_size)\n","\n","        if attention_type == 'relative_scl':\n","            self.attention_layer = Attention_Rel_Scl(embedding_dim, num_heads, self.num_patches + 1, dropout)\n","        elif attention_type == 'relative_vec':\n","            self.attention_layer = Attention_Rel_Vec(embedding_dim, num_heads, self.num_patches + 1, dropout)\n","        elif attention_type == 'tupe':\n","            self.attention_layer = TUPEMultiHeadAttention(tupe_config, pos_embed)\n","        else:\n","            self.attention_layer = Attention(embedding_dim, num_heads, dropout)\n","\n","        encoder_layers = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n","\n","        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n","        self.classifier = nn.Linear(dim_feedforward, num_classes)\n","\n","    def forward(self, x):\n","        x = self.patch_embedding(x)\n","        x = self.attention_layer(x)\n","        x = self.transformer_encoder(x)\n","        class_token_output = x[:, 0, :]\n","        x = self.ff_layer(class_token_output)\n","        output = self.classifier(x)\n","        return output\n","\n","\n","random_instances, random_labels = next(iter(train_loader))\n","random_instance = random_instances[0]\n","\n","BATCH_SIZE = random_instances.shape[0]\n","TIMESTEPS = random_instance.shape[0]\n","CHANNELS = random_instance.shape[1]\n","PATCH_SIZE = 8\n","\n","patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n","    in_channels=CHANNELS,\n","    patch_size=PATCH_SIZE,\n","    embedding_dim=CHANNELS * PATCH_SIZE,\n","    input_timesteps=TIMESTEPS,\n",")\n","\n","patch_embeddings = patch_embedding_layer(random_instances)\n","patch_embeddings.shape\n","\n","# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Instantiate the model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=8,\n","    embedding_dim=32,\n","    pos_encoding='fixed',\n","    num_transformer_layers=4,\n","    num_heads=4,\n","    dim_feedforward=128,\n","    dropout=0.2,\n","    num_classes=n_classes,\n",").to(device)\n","\n","# Define the loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Define the learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n","\n","\n","# Training Function\n","def train_model(model, train_loader, criterion, optimizer, num_epochs):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        train_losses = []\n","        train_correct = 0\n","        total = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_losses.append(loss.item())\n","            train_correct += (predictions.argmax(1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","        train_loss = np.mean(train_losses)\n","        train_acc = train_correct / total\n","\n","        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n","\n","# Evaluation Function\n","def evaluate_model(model, valid_loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total_val = 0\n","    with torch.no_grad():\n","        for inputs, labels in valid_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            total_loss += loss.item()\n","            correct += (predictions.argmax(1) == labels).sum().item()\n","            total_val += labels.size(0)\n","    val_loss = total_loss / len(valid_loader)\n","    val_acc = correct / total_val\n","    return val_loss, val_acc\n","\n","# Train the model\n","n_epochs = 10\n","best_validation_acc = 0.0\n","best_model_path = 'best_model_v2.pth'\n","\n","for epoch in range(n_epochs):\n","    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n","    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n","\n","    if val_acc > best_validation_acc:\n","        best_validation_acc = val_acc\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n","\n","    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n","    lr_scheduler.step()\n","\n","# Load the best model\n","model.load_state_dict(torch.load(best_model_path))\n","print('Loaded best model for testing or further use.')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oeNs_BKxfx0i","executionInfo":{"status":"ok","timestamp":1726503918746,"user_tz":300,"elapsed":75734,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"26f5e99b-d292-4f46-b4c1-2d6ef6e2f100"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 1.5498, Train Acc: 0.4294\n","Epoch 1: New best model saved with validation accuracy: 0.4531\n","Epoch 1, Val Loss: 1.3207, Val Acc: 0.4531\n","Epoch 1, Train Loss: 1.0144, Train Acc: 0.5942\n","Epoch 2: New best model saved with validation accuracy: 0.5263\n","Epoch 2, Val Loss: 1.1010, Val Acc: 0.5263\n","Epoch 1, Train Loss: 0.8352, Train Acc: 0.6598\n","Epoch 3: New best model saved with validation accuracy: 0.5909\n","Epoch 3, Val Loss: 1.0188, Val Acc: 0.5909\n","Epoch 1, Train Loss: 0.7127, Train Acc: 0.7130\n","Epoch 4: New best model saved with validation accuracy: 0.5987\n","Epoch 4, Val Loss: 1.1239, Val Acc: 0.5987\n","Epoch 1, Train Loss: 0.6133, Train Acc: 0.7496\n","Epoch 5: New best model saved with validation accuracy: 0.6669\n","Epoch 5, Val Loss: 0.8621, Val Acc: 0.6669\n","Epoch 1, Train Loss: 0.5648, Train Acc: 0.7790\n","Epoch 6, Val Loss: 1.1122, Val Acc: 0.6435\n","Epoch 1, Train Loss: 0.5206, Train Acc: 0.8006\n","Epoch 7: New best model saved with validation accuracy: 0.7109\n","Epoch 7, Val Loss: 0.8690, Val Acc: 0.7109\n","Epoch 1, Train Loss: 0.4974, Train Acc: 0.8127\n","Epoch 8: New best model saved with validation accuracy: 0.7195\n","Epoch 8, Val Loss: 0.9098, Val Acc: 0.7195\n","Epoch 1, Train Loss: 0.4650, Train Acc: 0.8220\n","Epoch 9: New best model saved with validation accuracy: 0.7315\n","Epoch 9, Val Loss: 0.8280, Val Acc: 0.7315\n","Epoch 1, Train Loss: 0.4277, Train Acc: 0.8341\n","Epoch 10: New best model saved with validation accuracy: 0.7663\n","Epoch 10, Val Loss: 0.7694, Val Acc: 0.7663\n","Loaded best model for testing or further use.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-15-b06036bd6b7c>:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(best_model_path))\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from einops import rearrange\n","import pandas as pd\n","\n","class Attention(nn.Module):\n","    def __init__(self, emb_size, num_heads, dropout):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.scale = emb_size ** -0.5\n","        self.key = nn.Linear(emb_size, emb_size, bias=False)\n","        self.value = nn.Linear(emb_size, emb_size, bias=False)\n","        self.query = nn.Linear(emb_size, emb_size, bias=False)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_out = nn.LayerNorm(emb_size)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.shape\n","        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","\n","        attn = torch.matmul(q, k) * self.scale\n","        attn = nn.functional.softmax(attn, dim=-1)\n","\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2)\n","        out = out.reshape(batch_size, seq_len, -1)\n","        out = self.to_out(out)\n","        return out\n","\n","\n","class Attention_Rel_Scl(nn.Module):\n","    def __init__(self, emb_size, num_heads, seq_len, dropout):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.num_heads = num_heads\n","        self.scale = emb_size ** -0.5\n","        self.key = nn.Linear(emb_size, emb_size, bias=False)\n","        self.value = nn.Linear(emb_size, emb_size, bias=False)\n","        self.query = nn.Linear(emb_size, emb_size, bias=False)\n","\n","        self.relative_bias_table = nn.Parameter(torch.zeros((2 * self.seq_len - 1), num_heads))\n","        coords = torch.meshgrid((torch.arange(1), torch.arange(self.seq_len)))\n","        coords = torch.flatten(torch.stack(coords), 1)\n","        relative_coords = coords[:, :, None] - coords[:, None, :]\n","        relative_coords[1] += self.seq_len - 1\n","        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n","        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n","        self.register_buffer(\"relative_index\", relative_index)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_out = nn.LayerNorm(emb_size)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.shape\n","        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","\n","        attn = torch.matmul(q, k) * self.scale\n","        attn = nn.functional.softmax(attn, dim=-1)\n","\n","        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, 8))\n","        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=1 * self.seq_len, w=1 * self.seq_len)\n","        attn = attn + relative_bias\n","\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2)\n","        out = out.reshape(batch_size, seq_len, -1)\n","        out = self.to_out(out)\n","        return out\n","\n","\n","class Attention_Rel_Vec(nn.Module):\n","    def __init__(self, emb_size, num_heads, seq_len, dropout):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.num_heads = num_heads\n","        self.scale = emb_size ** -0.5\n","        self.key = nn.Linear(emb_size, emb_size, bias=False)\n","        self.value = nn.Linear(emb_size, emb_size, bias=False)\n","        self.query = nn.Linear(emb_size, emb_size, bias=False)\n","\n","        self.Er = nn.Parameter(torch.randn(self.seq_len, int(emb_size / num_heads)))\n","\n","        self.register_buffer(\n","            \"mask\",\n","            torch.tril(torch.ones(self.seq_len, self.seq_len))\n","                .unsqueeze(0).unsqueeze(0)\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_out = nn.LayerNorm(emb_size)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, _ = x.shape\n","        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","\n","        QEr = torch.matmul(q, self.Er.transpose(0, 1))\n","        Srel = self.skew(QEr)\n","\n","        attn = torch.matmul(q, k)\n","        attn = (attn + Srel) * self.scale\n","\n","        attn = nn.functional.softmax(attn, dim=-1)\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2)\n","        out = out.reshape(batch_size, seq_len, -1)\n","        out = self.to_out(out)\n","        return out\n","\n","    def skew(self, QEr):\n","        padded = nn.functional.pad(QEr, (1, 0))\n","        batch_size, num_heads, num_rows, num_cols = padded.shape\n","        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n","        Srel = reshaped[:, :, 1:, :]\n","        return Srel\n"],"metadata":{"id":"ESOXdCAqaM_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class tAPE(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n","        super(tAPE, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin((position * div_term) * (d_model / max_len))\n","        pe[:, 1::2] = torch.cos((position * div_term) * (d_model / max_len))\n","        pe = scale_factor * pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return self.dropout(x)\n","\n","class AbsolutePositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n","        super(AbsolutePositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = scale_factor * pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return self.dropout(x)\n","\n","class LearnablePositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=1024):\n","        super(LearnablePositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.pe = nn.Parameter(torch.empty(max_len, d_model))\n","        nn.init.uniform_(self.pe, -0.02, 0.02)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(1), :]\n","        return self.dropout(x)"],"metadata":{"id":"dplhsyvE-jR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_pos_encoder(pos_encoding):\n","    if pos_encoding == 'fixed':\n","        return PositionalEncoding\n","    elif pos_encoding == 'learned':\n","        return LearnedPositionalEncoding\n","    elif pos_encoding == 'tAPE':\n","        return tAPE\n","    elif pos_encoding == 'absolute':\n","        return AbsolutePositionalEncoding\n","    else:\n","        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n"],"metadata":{"id":"q9sb14F6-kc8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Instantiate the model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=8,\n","    embedding_dim=32,\n","    pos_encoding='tAPE',\n","    num_transformer_layers=4,\n","    num_heads=4,\n","    dim_feedforward=128,\n","    dropout=0.2,\n","    num_classes=n_classes,\n",").to(device)\n","\n","# Define the loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Define the learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n","\n","\n","# Training Function\n","def train_model(model, train_loader, criterion, optimizer, num_epochs):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        train_losses = []\n","        train_correct = 0\n","        total = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_losses.append(loss.item())\n","            train_correct += (predictions.argmax(1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","        train_loss = np.mean(train_losses)\n","        train_acc = train_correct / total\n","\n","        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n","\n","# Evaluation Function\n","def evaluate_model(model, valid_loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total_val = 0\n","    with torch.no_grad():\n","        for inputs, labels in valid_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            total_loss += loss.item()\n","            correct += (predictions.argmax(1) == labels).sum().item()\n","            total_val += labels.size(0)\n","    val_loss = total_loss / len(valid_loader)\n","    val_acc = correct / total_val\n","    return val_loss, val_acc\n","\n","# Train the model\n","n_epochs = 10\n","best_validation_acc = 0.0\n","best_model_path = 'best_model_v2.pth'\n","\n","for epoch in range(n_epochs):\n","    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n","    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n","\n","    if val_acc > best_validation_acc:\n","        best_validation_acc = val_acc\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n","\n","    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n","    lr_scheduler.step()\n","\n","# Load the best model\n","model.load_state_dict(torch.load(best_model_path))\n","print('Loaded best model for testing or further use.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"egoVdYVX-mq4","executionInfo":{"status":"ok","timestamp":1726503960419,"user_tz":300,"elapsed":41677,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"fdc67e24-364e-4692-fb3c-7f8f060cd92c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 1.6305, Train Acc: 0.3981\n","Epoch 1: New best model saved with validation accuracy: 0.4730\n","Epoch 1, Val Loss: 1.3626, Val Acc: 0.4730\n","Epoch 1, Train Loss: 1.0419, Train Acc: 0.5878\n","Epoch 2: New best model saved with validation accuracy: 0.4964\n","Epoch 2, Val Loss: 1.3372, Val Acc: 0.4964\n","Epoch 1, Train Loss: 0.8725, Train Acc: 0.6576\n","Epoch 3: New best model saved with validation accuracy: 0.5859\n","Epoch 3, Val Loss: 1.2605, Val Acc: 0.5859\n","Epoch 1, Train Loss: 0.7775, Train Acc: 0.6910\n","Epoch 4: New best model saved with validation accuracy: 0.6314\n","Epoch 4, Val Loss: 1.0425, Val Acc: 0.6314\n","Epoch 1, Train Loss: 0.6684, Train Acc: 0.7410\n","Epoch 5: New best model saved with validation accuracy: 0.6875\n","Epoch 5, Val Loss: 0.9529, Val Acc: 0.6875\n","Epoch 1, Train Loss: 0.5744, Train Acc: 0.7760\n","Epoch 6: New best model saved with validation accuracy: 0.7045\n","Epoch 6, Val Loss: 0.9175, Val Acc: 0.7045\n","Epoch 1, Train Loss: 0.5416, Train Acc: 0.7887\n","Epoch 7, Val Loss: 0.8517, Val Acc: 0.6868\n","Epoch 1, Train Loss: 0.5008, Train Acc: 0.8074\n","Epoch 8: New best model saved with validation accuracy: 0.7138\n","Epoch 8, Val Loss: 0.9638, Val Acc: 0.7138\n","Epoch 1, Train Loss: 0.4800, Train Acc: 0.8178\n","Epoch 9: New best model saved with validation accuracy: 0.7386\n","Epoch 9, Val Loss: 0.9735, Val Acc: 0.7386\n","Epoch 1, Train Loss: 0.4446, Train Acc: 0.8338\n","Epoch 10, Val Loss: 0.9103, Val Acc: 0.7308\n","Loaded best model for testing or further use.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-19-ea2d96ec00ab>:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(best_model_path))\n"]}]},{"cell_type":"code","source":["# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Instantiate the model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=8,\n","    embedding_dim=32,\n","    pos_encoding='fixed',\n","    num_transformer_layers=4,\n","    num_heads=4,\n","    dim_feedforward=128,\n","    dropout=0.2,\n","    num_classes=n_classes,\n","    attention_type= 'relative_vec',\n",").to(device)\n","\n","# Define the loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Define the learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n","\n","\n","# Training Function\n","def train_model(model, train_loader, criterion, optimizer, num_epochs):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        train_losses = []\n","        train_correct = 0\n","        total = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_losses.append(loss.item())\n","            train_correct += (predictions.argmax(1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","        train_loss = np.mean(train_losses)\n","        train_acc = train_correct / total\n","\n","        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n","\n","# Evaluation Function\n","def evaluate_model(model, valid_loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total_val = 0\n","    with torch.no_grad():\n","        for inputs, labels in valid_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            predictions = model(inputs)\n","            loss = criterion(predictions, labels)\n","            total_loss += loss.item()\n","            correct += (predictions.argmax(1) == labels).sum().item()\n","            total_val += labels.size(0)\n","    val_loss = total_loss / len(valid_loader)\n","    val_acc = correct / total_val\n","    return val_loss, val_acc\n","\n","# Train the model\n","n_epochs = 10\n","best_validation_acc = 0.0\n","best_model_path = 'best_model_v2.pth'\n","\n","for epoch in range(n_epochs):\n","    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n","    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n","\n","    if val_acc > best_validation_acc:\n","        best_validation_acc = val_acc\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n","\n","    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n","    lr_scheduler.step()\n","\n","# Load the best model\n","model.load_state_dict(torch.load(best_model_path))\n","print('Loaded best model for testing or further use.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xzFfH3xX8fE_","executionInfo":{"status":"ok","timestamp":1721074969427,"user_tz":300,"elapsed":38328,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"4bc48347-82fd-4894-df88-c720b60a187c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 1.7242, Train Acc: 0.3534\n","Epoch 1: New best model saved with validation accuracy: 0.4304\n","Epoch 1, Val Loss: 1.4007, Val Acc: 0.4304\n","Epoch 1, Train Loss: 1.1194, Train Acc: 0.5788\n","Epoch 2: New best model saved with validation accuracy: 0.5362\n","Epoch 2, Val Loss: 1.1834, Val Acc: 0.5362\n","Epoch 1, Train Loss: 0.8447, Train Acc: 0.6736\n","Epoch 3: New best model saved with validation accuracy: 0.7010\n","Epoch 3, Val Loss: 0.7892, Val Acc: 0.7010\n","Epoch 1, Train Loss: 0.7093, Train Acc: 0.7256\n","Epoch 4, Val Loss: 0.9276, Val Acc: 0.6719\n","Epoch 1, Train Loss: 0.6211, Train Acc: 0.7592\n","Epoch 5, Val Loss: 1.0127, Val Acc: 0.6825\n","Epoch 1, Train Loss: 0.5698, Train Acc: 0.7815\n","Epoch 6: New best model saved with validation accuracy: 0.7322\n","Epoch 6, Val Loss: 0.7392, Val Acc: 0.7322\n","Epoch 1, Train Loss: 0.5123, Train Acc: 0.8048\n","Epoch 7, Val Loss: 0.8615, Val Acc: 0.7067\n","Epoch 1, Train Loss: 0.4707, Train Acc: 0.8085\n","Epoch 8, Val Loss: 0.7972, Val Acc: 0.7131\n","Epoch 1, Train Loss: 0.4657, Train Acc: 0.8151\n","Epoch 9: New best model saved with validation accuracy: 0.7656\n","Epoch 9, Val Loss: 0.6635, Val Acc: 0.7656\n","Epoch 1, Train Loss: 0.4117, Train Acc: 0.8358\n","Epoch 10, Val Loss: 0.8849, Val Acc: 0.7393\n","Loaded best model for testing or further use.\n"]}]},{"cell_type":"markdown","source":["### TUPE"],"metadata":{"id":"ntF6fok4KzMF"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","class TUPEConfig:\n","    def __init__(self):\n","        self.d_model = 128\n","        self.num_heads = 8\n","        self.dim_feedforward = 512\n","        self.dropout = 0.1\n","        self.max_len = 5000\n","        self.num_buckets = 32\n","        self.max_distance = 128\n","        self.relative_bias = True\n","        self.bidirectional_bias = True\n","\n","class TUPEMultiHeadAttention(nn.Module):\n","    def __init__(self, config: TUPEConfig, pos_embed: nn.Module) -> None:\n","        super().__init__()\n","        self.max_len = config.max_len\n","        self.num_heads = config.num_heads\n","        self.num_buckets = config.num_buckets\n","        self.max_distance = config.max_distance\n","        self.bidirectional = config.bidirectional_bias\n","        self.scale = math.sqrt(2 * config.d_model // config.num_heads)\n","\n","        self.pos_embed = pos_embed\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","        # kqv in one pass\n","        self.pos_kq = nn.Linear(config.d_model, 2 * config.d_model, bias=False)\n","        self.tok_kqv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n","\n","        self.relative_bias = config.relative_bias\n","        if config.relative_bias:\n","            self.bias = nn.Embedding(config.max_len * 2, config.num_heads)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        batch_size, seq_len, _ = x.shape\n","\n","        pos_embed = self.pos_embed(seq_len).repeat(batch_size, 1, 1)\n","        # pos_embed.shape == (batch_size, seq_len, d_model)\n","        pos_key, pos_query = self.pos_kq(pos_embed).chunk(2, dim=-1)\n","        pos_key = pos_key.view(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        # pos_key.shape == (batch_size, num_heads, d_head, seq_len)\n","        pos_query = pos_query.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        # pos_query.shape == (batch_size, num_heads, seq_len, d_head)\n","        pos_attn = torch.matmul(pos_query, pos_key)\n","        # pos_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n","\n","        tok_key, tok_query, tok_value = self.tok_kqv(x).chunk(3, dim=-1)\n","        tok_key = tok_key.view(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n","        # tok_key.shape == (batch_size, num_heads, d_head, seq_len)\n","        tok_query = tok_query.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        tok_value = tok_value.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n","        # tok_qv.shape == (batch_size, num_heads, seq_len, d_head)\n","        tok_attn = torch.matmul(tok_query, tok_key)\n","        # tok_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n","\n","        attn = (tok_attn + pos_attn) / self.scale\n","        if self.relative_bias:\n","            relative_positions = self.get_relative_positions(seq_len)\n","            # relative_positions.shape == (seq_len, seq_len)\n","            bias = self.bias(relative_positions + self.max_len)\n","            # bias.shape == (seq_len, seq_len, num_heads)\n","            bias = bias.permute(2, 0, 1).unsqueeze(0)\n","            # bias.shape == (1, num_heads, seq_len, seq_len)\n","            attn = attn + bias\n","\n","        attn = F.softmax(attn, dim=-1)\n","        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n","        out = torch.matmul(attn, tok_value)\n","        # out.shape == (batch_size, num_heads, seq_len, d_head)\n","        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n","        # out.shape == (batch_size, seq_len, d_model)\n","        out = self.dropout(out)\n","        return out\n","\n","    def get_relative_positions(self, seq_len):\n","        # Generate relative position encodings\n","        range_vec = torch.arange(seq_len)\n","        distance_mat = range_vec.unsqueeze(1) - range_vec.unsqueeze(0)\n","        distance_mat_clipped = torch.clamp(distance_mat, -self.max_distance, self.max_distance)\n","        final_mat = distance_mat_clipped + self.max_distance\n","        return final_mat\n"],"metadata":{"id":"LWu8jg32ORdz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FixedPositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(FixedPositionalEncoding, self).__init__()\n","        self.encoding = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        self.encoding[:, 0::2] = torch.sin(position * div_term)\n","        self.encoding[:, 1::2] = torch.cos(position * div_term)\n","        self.encoding = self.encoding.unsqueeze(0)  # (1, max_len, d_model)\n","\n","    def forward(self, seq_len):\n","        return self.encoding[:, :seq_len, :].detach()\n","\n","class TimeSeriesTransformer(nn.Module):\n","    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_layers, num_heads, dim_feedforward, dropout, num_classes, attention_type, pos_encoding):\n","        super(TimeSeriesTransformer, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.num_layers = num_layers\n","        self.patch_size = patch_size\n","        self.in_channels = in_channels\n","        self.num_classes = num_classes\n","        self.attention_type = attention_type\n","\n","        # Patch embedding\n","        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n","        self.pos_encoding = pos_encoding\n","\n","        # Transformer layers with TUPEMultiHeadAttention\n","        self.transformer_layers = nn.ModuleList([\n","            TUPEMultiHeadAttention(\n","                config=TUPEConfig(),\n","                pos_embed=FixedPositionalEncoding(embedding_dim, max_len=input_timesteps)\n","            )\n","            for _ in range(num_layers)\n","        ])\n","\n","        # Classification head\n","        self.classifier = nn.Linear(embedding_dim, num_classes)\n","\n","    def forward(self, x):\n","        #print(f\"Input to model: {x.shape}\")\n","\n","        # Patch embedding\n","        x = x.permute(0, 2, 1)  # Change shape from (batch_size, seq_len, in_channels) to (batch_size, in_channels, seq_len)\n","        #print(f\"Input to Conv1d: {x.shape}\")\n","\n","        x = self.patch_embedding(x)  # Apply Conv1d\n","        #print(f\"Patch embedding output: {x.shape}\")\n","\n","        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, embedding_dim)\n","        #print(f\"Patch embedding output permuted: {x.shape}\")\n","\n","        # Positional encoding\n","        seq_len = x.size(1)\n","        x = x + self.pos_encoding(seq_len).to(x.device)\n","        #print(f\"Positional encoding output: {x.shape}\")\n","\n","        # Transformer layers\n","        for layer in self.transformer_layers:\n","            x = layer(x)\n","            #print(f\"Transformer layer output: {x.shape}\")\n","\n","        # Classification head\n","        x = x.mean(dim=1)\n","        x = self.classifier(x)\n","        #print(f\"Classifier output: {x.shape}\")\n","        return x\n","\n","# Hyperparameters\n","TIMESTEPS = 151\n","CHANNELS = 4\n","n_classes = 9\n","\n","# Initialize positional encoding\n","pos_embed = FixedPositionalEncoding(d_model=128, max_len=TIMESTEPS)\n","\n","# Define the model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=8,\n","    embedding_dim=128,\n","    num_layers=4,\n","    num_heads=8,\n","    dim_feedforward=512,\n","    dropout=0.1,\n","    num_classes=n_classes,\n","    attention_type='tupe',\n","    pos_encoding=pos_embed,\n",")\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training and validation loop\n","num_epochs = 10\n","best_valid_acc = 0.0\n","\n","for epoch in range(num_epochs):\n","    # Training\n","    model.train()\n","    train_loss = 0.0\n","    train_preds = []\n","    train_labels = []\n","    for batch in train_loader:\n","        inputs, labels = batch\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * inputs.size(0)\n","        train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n","        train_labels.extend(labels.cpu().numpy())\n","\n","    train_loss /= len(train_loader.dataset)\n","    train_acc = accuracy_score(train_labels, train_preds)\n","\n","    # Validation\n","    model.eval()\n","    valid_loss = 0.0\n","    valid_preds = []\n","    valid_labels = []\n","    with torch.no_grad():\n","        for batch in valid_loader:\n","            inputs, labels = batch\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            valid_loss += loss.item() * inputs.size(0)\n","            valid_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n","            valid_labels.extend(labels.cpu().numpy())\n","\n","    valid_loss /= len(valid_loader.dataset)\n","    valid_acc = accuracy_score(valid_labels, valid_preds)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, '\n","          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n","          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')\n","\n","    # Save the best model\n","    if valid_acc > best_valid_acc:\n","        best_valid_acc = valid_acc\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TgP2Ge01OTt0","executionInfo":{"status":"ok","timestamp":1721077825690,"user_tz":300,"elapsed":74458,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"944e38a0-50e8-49f0-c403-c979deac3818"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Train Loss: 1.0493, Train Acc: 0.5805, Valid Loss: 0.9030, Valid Acc: 0.6420\n","Epoch 2/10, Train Loss: 0.6090, Train Acc: 0.7628, Valid Loss: 1.0519, Valid Acc: 0.6420\n","Epoch 3/10, Train Loss: 0.4910, Train Acc: 0.8158, Valid Loss: 1.1174, Valid Acc: 0.6342\n","Epoch 4/10, Train Loss: 0.4502, Train Acc: 0.8299, Valid Loss: 0.9228, Valid Acc: 0.6776\n","Epoch 5/10, Train Loss: 0.3754, Train Acc: 0.8636, Valid Loss: 0.7835, Valid Acc: 0.7095\n","Epoch 6/10, Train Loss: 0.3725, Train Acc: 0.8598, Valid Loss: 0.9433, Valid Acc: 0.6932\n","Epoch 7/10, Train Loss: 0.3492, Train Acc: 0.8691, Valid Loss: 0.7877, Valid Acc: 0.7450\n","Epoch 8/10, Train Loss: 0.2854, Train Acc: 0.8895, Valid Loss: 0.7008, Valid Acc: 0.7628\n","Epoch 9/10, Train Loss: 0.2384, Train Acc: 0.9177, Valid Loss: 0.7519, Valid Acc: 0.7670\n","Epoch 10/10, Train Loss: 0.2293, Train Acc: 0.9148, Valid Loss: 0.7832, Valid Acc: 0.7741\n","Best Validation Accuracy: 0.7741\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import math\n","from typing import Optional\n","\n","class SineSPE(nn.Module):\n","    def __init__(self, num_heads: int = 8, in_features: int = 64, num_realizations: int = 256):\n","        super(SineSPE, self).__init__()\n","        self.num_heads = num_heads\n","        self.in_features = in_features\n","        self.num_realizations = num_realizations\n","\n","    def forward(self, shape, num_realizations=None):\n","        if num_realizations is None:\n","            num_realizations = self.num_realizations\n","        z_shape = [1, *shape[1:], self.num_heads, self.in_features, num_realizations]\n","        qbar = torch.randn(z_shape, device='cpu')\n","        kbar = torch.randn(z_shape, device='cpu')\n","        scale = (num_realizations * self.in_features) ** 0.25\n","        return (qbar / scale, kbar / scale)\n","\n","\n","\n","class ConvSPE(nn.Module):\n","    def __init__(self, ndim: int, num_heads: int = 8, in_features: int = 64, kernel_size: int = 3, num_realizations: int = 256, conv_class=nn.Conv1d):\n","        super(ConvSPE, self).__init__()\n","        self.ndim = ndim\n","        self.num_heads = num_heads\n","        self.in_features = in_features\n","        self.kernel_size = kernel_size\n","        self.num_realizations = num_realizations\n","        self.conv_q = conv_class(in_channels=num_heads * in_features, out_channels=num_heads * in_features, stride=1, kernel_size=kernel_size, padding=0, bias=False, groups=num_heads * in_features)\n","        self.conv_k = conv_class(in_channels=num_heads * in_features, out_channels=num_heads * in_features, stride=1, kernel_size=kernel_size, padding=0, bias=False, groups=num_heads * in_features)\n","\n","        with torch.no_grad():\n","            self.conv_q.weight.zero_()\n","            self.conv_k.weight.zero_()\n","            center_slice = self.kernel_size // 2\n","            for idx in range(self.conv_q.weight.size(0)):\n","                self.conv_q.weight[idx, :, center_slice] = 1.\n","                self.conv_k.weight[idx, :, center_slice] = 1.\n","\n","    def forward(self, shape, num_realizations=None):\n","        if num_realizations is None:\n","            num_realizations = self.num_realizations\n","        query_shape = shape[1:]\n","        z_shape = [1, self.num_heads * self.in_features, *query_shape, num_realizations]\n","        z = torch.randn(z_shape, device='cpu')\n","        qbar = self.conv_q(z.flatten(-2, -1)).view(*z.shape[:-2], -1)\n","        kbar = self.conv_k(z.flatten(-2, -1)).view(*z.shape[:-2], -1)\n","        scale = (num_realizations * self.in_features) ** 0.25\n","        return (qbar / scale, kbar / scale)\n","\n","\n","class RandomSPE(nn.Module):\n","    def __init__(self, num_heads: int = 8, in_features: int = 64, num_realizations: int = 256):\n","        super(RandomSPE, self).__init__()\n","        self.num_heads = num_heads\n","        self.in_features = in_features\n","        self.num_realizations = num_realizations\n","\n","    def forward(self, shape, num_realizations=None):\n","        if num_realizations is None:\n","            num_realizations = self.num_realizations\n","        z_shape = [1, *shape[1:], self.num_heads, self.in_features, num_realizations]\n","        qbar = torch.randn(z_shape, device='cpu')\n","        kbar = torch.randn(z_shape, device='cpu')\n","        scale = (num_realizations * self.in_features) ** 0.25\n","        return (qbar / scale, kbar / scale)\n","\n","\n","class AttentionSPE(nn.Module):\n","    def __init__(self, attention: nn.Module, spe_type: str, spe_params: dict):\n","        super(AttentionSPE, self).__init__()\n","        self.attention = attention\n","        self.spe_type = spe_type\n","        if self.spe_type == 'sine':\n","            self.position_encoding = SineSPE(**spe_params)\n","        elif self.spe_type == 'conv':\n","            self.position_encoding = ConvSPE(**spe_params)\n","        elif self.spe_type == 'random':\n","            self.position_encoding = RandomSPE(**spe_params)\n","        else:\n","            raise ValueError(f\"Unknown spe_type: {self.spe_type}\")\n","\n","    def forward(self, *args, **kwargs):\n","        # Generate the stochastic positional encoding\n","        pe_shape = args[0].shape if len(args) > 0 else kwargs.get('shape', None)\n","        if pe_shape is None:\n","            raise ValueError(\"Shape of the input sequence must be provided to generate positional encoding\")\n","        qbar, kbar = self.position_encoding(pe_shape)\n","        return self.attention(*args, **kwargs, qbar=qbar, kbar=kbar)\n","\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_dim: int, embed_dim: int, num_heads: int, num_encoder_layers: int, dropout: float = 0.1, max_length: int = 5000, pos_encoding: str = 'sinusoidal', spe_params: Optional[dict] = None):\n","        super(TransformerModel, self).__init__()\n","        self.embedding = nn.Linear(input_dim, embed_dim)\n","        self.pos_encoding = pos_encoding\n","\n","        if pos_encoding == 'sinusoidal':\n","            self.position_encoding = self.sinusoidal_position_encoding(max_length, embed_dim)\n","        elif pos_encoding == 'spe':\n","            self.spe_type = spe_params.get('type', 'sine')\n","            num_heads = spe_params.get('num_heads', num_heads)\n","            in_features = spe_params.get('in_features', embed_dim // num_heads)\n","            filtered_params = {k: v for k, v in spe_params.items() if k not in ['type', 'num_heads', 'in_features']}\n","\n","            if self.spe_type == 'sine':\n","                self.position_encoding = SineSPE(num_heads=num_heads, in_features=in_features, **filtered_params)\n","            elif self.spe_type == 'conv':\n","                self.position_encoding = ConvSPE(ndim=1, num_heads=num_heads, in_features=in_features, **filtered_params)\n","            elif self.spe_type == 'random':\n","                self.position_encoding = RandomSPE(num_heads=num_heads, in_features=in_features, **filtered_params)\n","            else:\n","                raise ValueError(f\"Unknown spe_type: {self.spe_type}\")\n","        else:\n","            raise ValueError(f\"Unknown pos_encoding: {pos_encoding}\")\n","\n","        encoder_layers = nn.TransformerEncoderLayer(embed_dim, num_heads, embed_dim * 4, dropout)\n","        self.encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n","\n","    def sinusoidal_position_encoding(self, max_length, embed_dim):\n","        position = torch.arange(0, max_length).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\n","        pe = torch.zeros(max_length, embed_dim)\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        return pe.unsqueeze(0)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        if self.pos_encoding == 'sinusoidal':\n","            pos_enc = self.position_encoding[:, :x.size(1)]\n","            x = x + pos_enc\n","        elif self.pos_encoding == 'spe':\n","            pe = self.position_encoding(x.shape)[0]  # Generate positional encoding\n","            batch_size, seq_length, _ = x.size()\n","            if pe.size(1) != seq_length:\n","                pe = pe[:, :seq_length, :, :]  # Slice to match the sequence length\n","            if pe.size(-1) != x.size(-1):\n","                pe = pe[:, :, :, :x.size(-1)]  # Slice to match the embedding dimension\n","            x = x + pe\n","        return self.encoder(x)\n","\n","\n","\n","# Define the model\n","model_spe = TransformerModel(\n","    input_dim=X_train.shape[2],\n","    embed_dim=128,  # Example embedding dimension, adjust as needed\n","    num_heads=8,\n","    num_encoder_layers=4,\n","    dropout=0.1,\n","    max_length=X_train.shape[1],\n","    pos_encoding='spe',\n","    spe_params={\n","        'type': 'sine',\n","        'num_heads': 8,\n","        'in_features': 128 // 8,\n","        'num_realizations': 4\n","    }\n",")\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model_spe.parameters(), lr=0.001)\n","\n","# Training and validation loop\n","num_epochs = 10\n","best_valid_acc = 0.0\n","\n","for epoch in range(num_epochs):\n","    # Training\n","    model_spe.train()\n","    train_loss = 0.0\n","    train_preds = []\n","    train_labels = []\n","    for batch in train_loader:\n","        inputs, labels = batch\n","        optimizer.zero_grad()\n","        outputs = model_spe(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * inputs.size(0)\n","        train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n","        train_labels.extend(labels.cpu().numpy())\n","\n","    train_loss /= len(train_loader.dataset)\n","    train_acc = accuracy_score(train_labels, train_preds)\n","\n","    # Validation\n","    model_spe.eval()\n","    valid_loss = 0.0\n","    valid_preds = []\n","    valid_labels = []\n","    with torch.no_grad():\n","        for batch in valid_loader:\n","            inputs, labels = batch\n","            outputs = model_spe(inputs)\n","            loss = criterion(outputs, labels)\n","            valid_loss += loss.item() * inputs.size(0)\n","            valid_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n","            valid_labels.extend(labels.cpu().numpy())\n","\n","    valid_loss /= len(valid_loader.dataset)\n","    valid_acc = accuracy_score(valid_labels, valid_preds)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, '\n","          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n","          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')\n","\n","    # Save the best model\n","    if valid_acc > best_valid_acc:\n","        best_valid_acc = valid_acc\n","        torch.save(model_spe.state_dict(), 'best_model_spe.pth')\n","\n","print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"3tn92J5FshF5","executionInfo":{"status":"error","timestamp":1721489586787,"user_tz":300,"elapsed":800,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"ed9ed605-47bd-4e08-e6dc-ef8816315ba8"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (128) must match the size of tensor b (4) at non-singleton dimension 5","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-4be0166ae502>\u001b[0m in \u001b[0;36m<cell line: 172>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_spe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-4be0166ae502>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Slice to match the embedding dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (4) at non-singleton dimension 5"]}]},{"cell_type":"code","source":["class TimeSeriesTransformer(nn.Module):\n","    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_layers, num_heads, dim_feedforward, dropout, num_classes, pos_encoding_type, spe_params):\n","        super(TimeSeriesTransformer, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.num_layers = num_layers\n","        self.patch_size = patch_size\n","        self.in_channels = in_channels\n","        self.num_classes = num_classes\n","\n","        # Patch embedding\n","        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n","\n","        # Positional encoding\n","        if pos_encoding_type == 'sine':\n","            self.pos_encoding = SineSPE(num_heads=num_heads, in_features=embedding_dim // num_heads, **spe_params)\n","        elif pos_encoding_type == 'conv':\n","            self.pos_encoding = ConvSPE(ndim=1, num_heads=num_heads, in_features=embedding_dim // num_heads, **spe_params)\n","        else:\n","            raise ValueError(\"Invalid positional encoding type\")\n","\n","        # Transformer layers with TUPEMultiHeadAttention\n","        self.transformer_layers = nn.ModuleList([\n","            TUPEMultiHeadAttention(\n","                config=TUPEConfig(),\n","                pos_embed=self.pos_encoding\n","            )\n","            for _ in range(num_layers)\n","        ])\n","\n","        # Classification head\n","        self.classifier = nn.Linear(embedding_dim, num_classes)\n","\n","    def forward(self, x):\n","        # Patch embedding\n","        x = x.permute(0, 2, 1)  # Change shape from (batch_size, seq_len, in_channels) to (batch_size, in_channels, seq_len)\n","        x = self.patch_embedding(x)  # Apply Conv1d\n","        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, embedding_dim)\n","\n","        # Positional encoding\n","        seq_len = x.size(1)\n","        pos_enc = self.pos_encoding(seq_len).to(x.device)\n","        x = x + pos_enc\n","\n","        # Transformer layers\n","        for layer in self.transformer_layers:\n","            x = layer(x)\n","\n","        # Classification head\n","        x = x.mean(dim=1)\n","        x = self.classifier(x)\n","        return x\n"],"metadata":{"id":"pMTQ5_2nyjht"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SineSPE(nn.Module):\n","    def __init__(self, num_heads, in_features, max_len=512):\n","        super(SineSPE, self).__init__()\n","        self.num_heads = num_heads\n","        self.in_features = in_features\n","        self.max_len = max_len\n","        self.position = nn.Parameter(torch.zeros(1, max_len, in_features))\n","        self.register_buffer('sine', self._generate_sine_encoding())\n","\n","    def _generate_sine_encoding(self):\n","        position = torch.arange(self.max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, self.in_features, 2).float() * -(math.log(10000.0) / self.in_features))\n","        encoding = torch.zeros(self.max_len, self.in_features)\n","        encoding[:, 0::2] = torch.sin(position * div_term)\n","        encoding[:, 1::2] = torch.cos(position * div_term)\n","        return encoding\n","\n","    def forward(self, seq_len):\n","        return self.sine[:seq_len, :].unsqueeze(0).repeat(self.num_heads, 1, 1)\n","\n","class ConvSPE(nn.Module):\n","    def __init__(self, ndim, num_heads, in_features, kernel_size=7, padding=3):\n","        super(ConvSPE, self).__init__()\n","        self.conv = nn.Conv1d(in_features, in_features, kernel_size=kernel_size, padding=padding)\n","        self.num_heads = num_heads\n","\n","    def forward(self, seq_len):\n","        # Generate sinusoidal encoding\n","        x = torch.arange(seq_len).unsqueeze(0).float()\n","        x = x.permute(0, 2, 1)\n","        x = self.conv(x)\n","        x = x.squeeze(0).permute(1, 0).unsqueeze(0).repeat(self.num_heads, 1, 1)\n","        return x\n"],"metadata":{"id":"_qHLmzj7yl1i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","import math\n","\n","# Example of positional encoding classes\n","class SineSPE(nn.Module):\n","    def __init__(self, in_features, max_len=512):\n","        super(SineSPE, self).__init__()\n","        self.in_features = in_features\n","        self.max_len = max_len\n","        self.position = nn.Parameter(torch.zeros(1, max_len, in_features))\n","        self.register_buffer('sine', self._generate_sine_encoding())\n","\n","    def _generate_sine_encoding(self):\n","        position = torch.arange(self.max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, self.in_features, 2).float() * -(math.log(10000.0) / self.in_features))\n","        encoding = torch.zeros(self.max_len, self.in_features)\n","        encoding[:, 0::2] = torch.sin(position * div_term)\n","        encoding[:, 1::2] = torch.cos(position * div_term)\n","        return encoding\n","\n","    def forward(self, seq_len):\n","        return self.sine[:seq_len, :].unsqueeze(0)  # Shape: (1, seq_len, in_features)\n","\n","\n","class ConvSPE(nn.Module):\n","    def __init__(self, num_heads, in_features, kernel_size=3, num_realizations=1):\n","        super(ConvSPE, self).__init__()\n","        padding = kernel_size // 2  # This ensures that the output size matches the input size if stride=1\n","\n","        # Define a 1D convolutional layer\n","        self.conv = nn.Conv1d(in_features, in_features, kernel_size=kernel_size, padding=padding)\n","\n","        self.num_heads = num_heads\n","        self.in_features = in_features\n","        self.kernel_size = kernel_size\n","        self.num_realizations = num_realizations\n","\n","    def forward(self, x):\n","        # x should be of shape (batch_size, seq_len, in_features)\n","        x = x.permute(0, 2, 1)  # Change shape to (batch_size, in_features, seq_len)\n","        x = self.conv(x)  # Apply convolution\n","        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, in_features)\n","        return x\n","\n","\n","# Model Definition\n","class TimeSeriesTransformer(nn.Module):\n","    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2, pos_encoding_type='sine', spe_params={}):\n","        super(TimeSeriesTransformer, self).__init__()\n","\n","        # Embedding layer\n","        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n","\n","        # Positional encoding\n","        if pos_encoding_type == 'sineSPE':\n","            self.pos_encoding = SineSPE(embedding_dim, **spe_params)\n","        elif pos_encoding_type == 'convSPE':\n","            self.pos_encoding = ConvSPE(num_heads=num_heads, in_features=embedding_dim, **spe_params)\n","        else:\n","            raise ValueError(\"Invalid positional encoding type\")\n","\n","        # Calculate the number of patches\n","        self.num_patches = -(-input_timesteps // patch_size)  # Ceiling division\n","\n","        # Transformer Encoder\n","        encoder_layers = TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n","        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n","\n","        # Feedforward layer\n","        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n","        # Classifier Head\n","        self.classifier = nn.Linear(dim_feedforward, num_classes)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, in_channels, input_timesteps)\n","\n","        # Get patch embeddings\n","        x = x.permute(0, 2, 1)  # Change shape from (batch_size, in_channels, input_timesteps) to (batch_size, input_timesteps, in_channels)\n","        x = self.patch_embedding(x)  # Apply Conv1d\n","        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, num_patches, embedding_dim)\n","\n","        # Apply positional encoding\n","        seq_len = x.size(1)\n","        pos_enc = self.pos_encoding(seq_len).to(x.device)\n","        x = x + pos_enc\n","\n","        # Apply Transformer Encoder\n","        x = self.transformer_encoder(x)  # Output shape: (batch_size, num_patches, embedding_dim)\n","\n","        # Use the output corresponding to the class token for classification\n","        class_token_output = x[:, 0, :]  # Select the class token for each item in the batch\n","\n","        # Feedforward layer\n","        x = self.ff_layer(class_token_output)\n","\n","        # Classifier head\n","        output = self.classifier(x)  # Output shape: (batch_size, num_classes)\n","\n","        return output\n"],"metadata":{"id":"3dIJpcJ3ypDM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters\n","TIMESTEPS = 151\n","CHANNELS = 4\n","patch_size = 7\n","embedding_dim = 128\n","num_transformer_layers = 6\n","num_heads = 8\n","dim_feedforward = 128\n","dropout = 0.1\n","num_classes = 9\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Instantiate model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=patch_size,\n","    embedding_dim=embedding_dim,\n","    num_transformer_layers=num_transformer_layers,\n","    num_heads=num_heads,\n","    dim_feedforward=dim_feedforward,\n","    dropout=dropout,\n","    num_classes=num_classes,\n","    pos_encoding_type='sineSPE',\n","    spe_params={'max_len': TIMESTEPS}\n",").to(device)\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","\n","# Training function\n","def train_epoch(model, data_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    for inputs, labels in data_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","\n","        # Compute loss\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","\n","        # Update parameters\n","        optimizer.step()\n","\n","        # Update metrics\n","        running_loss += loss.item() * inputs.size(0)\n","        _, predicted = torch.max(outputs, 1)\n","        all_preds.extend(predicted.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    train_loss = running_loss / len(data_loader.dataset)\n","    train_acc = accuracy_score(all_labels, all_preds)\n","\n","    return train_loss, train_acc\n","\n","# Evaluation function\n","def evaluate(model, data_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","\n","            # Compute loss\n","            loss = criterion(outputs, labels)\n","\n","            # Update metrics\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    valid_loss = running_loss / len(data_loader.dataset)\n","    valid_acc = accuracy_score(all_labels, all_preds)\n","\n","    return valid_loss, valid_acc\n","\n","# Training and Evaluation Loop\n","num_epochs = 20  # Adjust as needed\n","best_valid_acc = 0.0\n","\n","for epoch in range(num_epochs):\n","    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n","    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion, device)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, '\n","          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n","          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n","\n","    # Save the best model\n","    if valid_accuracy > best_valid_acc:\n","        best_valid_acc = valid_accuracy\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlTvPqd6zvXw","executionInfo":{"status":"ok","timestamp":1721492366847,"user_tz":300,"elapsed":317330,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"b7dc86b0-bb83-4edc-f759-ab4991006e84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Train Loss: 1.7508, Train Acc: 0.3576, Valid Loss: 1.4237, Valid Acc: 0.4510\n","Epoch 2/20, Train Loss: 1.0956, Train Acc: 0.6048, Valid Loss: 1.0276, Valid Acc: 0.6286\n","Epoch 3/20, Train Loss: 0.6951, Train Acc: 0.7568, Valid Loss: 0.8192, Valid Acc: 0.7116\n","Epoch 4/20, Train Loss: 0.5209, Train Acc: 0.8118, Valid Loss: 0.8004, Valid Acc: 0.7429\n","Epoch 5/20, Train Loss: 0.4250, Train Acc: 0.8411, Valid Loss: 0.7663, Valid Acc: 0.7472\n","Epoch 6/20, Train Loss: 0.3676, Train Acc: 0.8618, Valid Loss: 0.7197, Valid Acc: 0.7734\n","Epoch 7/20, Train Loss: 0.3210, Train Acc: 0.8803, Valid Loss: 0.7043, Valid Acc: 0.7805\n","Epoch 8/20, Train Loss: 0.2754, Train Acc: 0.8996, Valid Loss: 0.8365, Valid Acc: 0.7528\n","Epoch 9/20, Train Loss: 0.2662, Train Acc: 0.8955, Valid Loss: 0.9164, Valid Acc: 0.7436\n","Epoch 10/20, Train Loss: 0.2439, Train Acc: 0.9082, Valid Loss: 0.6942, Valid Acc: 0.7947\n","Epoch 11/20, Train Loss: 0.2112, Train Acc: 0.9199, Valid Loss: 0.8725, Valid Acc: 0.7621\n","Epoch 12/20, Train Loss: 0.2004, Train Acc: 0.9228, Valid Loss: 0.8559, Valid Acc: 0.7834\n","Epoch 13/20, Train Loss: 0.2027, Train Acc: 0.9232, Valid Loss: 0.9727, Valid Acc: 0.7486\n","Epoch 14/20, Train Loss: 0.1866, Train Acc: 0.9285, Valid Loss: 0.8845, Valid Acc: 0.7649\n","Epoch 15/20, Train Loss: 0.1700, Train Acc: 0.9377, Valid Loss: 0.9557, Valid Acc: 0.7798\n","Epoch 16/20, Train Loss: 0.1610, Train Acc: 0.9410, Valid Loss: 0.8790, Valid Acc: 0.7841\n","Epoch 17/20, Train Loss: 0.1425, Train Acc: 0.9516, Valid Loss: 0.8343, Valid Acc: 0.8082\n","Epoch 18/20, Train Loss: 0.1535, Train Acc: 0.9428, Valid Loss: 0.8827, Valid Acc: 0.7855\n","Epoch 19/20, Train Loss: 0.1321, Train Acc: 0.9522, Valid Loss: 1.0394, Valid Acc: 0.7862\n","Epoch 20/20, Train Loss: 0.1304, Train Acc: 0.9525, Valid Loss: 1.0524, Valid Acc: 0.7564\n","Best Validation Accuracy: 0.8082\n"]}]},{"cell_type":"code","source":["# Hyperparameters\n","TIMESTEPS = 151\n","CHANNELS = 4\n","patch_size = 7\n","embedding_dim = 128\n","num_transformer_layers = 6\n","num_heads = 8\n","dim_feedforward = 128\n","dropout = 0.1\n","num_classes = 9\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Instantiate model\n","model = TimeSeriesTransformer(\n","    input_timesteps=TIMESTEPS,\n","    in_channels=CHANNELS,\n","    patch_size=patch_size,\n","    embedding_dim=embedding_dim,\n","    num_transformer_layers=num_transformer_layers,\n","    num_heads=num_heads,\n","    dim_feedforward=dim_feedforward,\n","    dropout=dropout,\n","    num_classes=num_classes,\n","    pos_encoding_type='sineSPE',\n","    spe_params={'max_len': TIMESTEPS}\n",").to(device)\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","\n","# Training function\n","def train_epoch(model, data_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    for inputs, labels in data_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","\n","        # Compute loss\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","\n","        # Update parameters\n","        optimizer.step()\n","\n","        # Update metrics\n","        running_loss += loss.item() * inputs.size(0)\n","        _, predicted = torch.max(outputs, 1)\n","        all_preds.extend(predicted.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    train_loss = running_loss / len(data_loader.dataset)\n","    train_acc = accuracy_score(all_labels, all_preds)\n","\n","    return train_loss, train_acc\n","\n","# Evaluation function\n","def evaluate(model, data_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","\n","            # Compute loss\n","            loss = criterion(outputs, labels)\n","\n","            # Update metrics\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    valid_loss = running_loss / len(data_loader.dataset)\n","    valid_acc = accuracy_score(all_labels, all_preds)\n","\n","    return valid_loss, valid_acc\n","\n","# Training and Evaluation Loop\n","num_epochs = 10  # Adjust as needed\n","best_valid_acc = 0.0\n","\n","for epoch in range(num_epochs):\n","    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n","    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion, device)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, '\n","          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n","          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n","\n","    # Save the best model\n","    if valid_accuracy > best_valid_acc:\n","        best_valid_acc = valid_accuracy\n","        torch.save(model.state_dict(), 'best_model.pth')\n","\n","print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34GXwW8DBnzG","executionInfo":{"status":"ok","timestamp":1721493966508,"user_tz":300,"elapsed":159000,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"8ead7466-64d9-46c4-d4dc-8fc8842ed813"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Train Loss: 1.7653, Train Acc: 0.3253, Valid Loss: 1.4297, Valid Acc: 0.4084\n","Epoch 2/10, Train Loss: 1.0330, Train Acc: 0.6340, Valid Loss: 0.9171, Valid Acc: 0.6300\n","Epoch 3/10, Train Loss: 0.6492, Train Acc: 0.7606, Valid Loss: 0.7166, Valid Acc: 0.7109\n","Epoch 4/10, Train Loss: 0.5026, Train Acc: 0.8055, Valid Loss: 0.8124, Valid Acc: 0.7060\n","Epoch 5/10, Train Loss: 0.4063, Train Acc: 0.8446, Valid Loss: 0.6082, Valid Acc: 0.7798\n","Epoch 6/10, Train Loss: 0.3572, Train Acc: 0.8666, Valid Loss: 0.6564, Valid Acc: 0.7798\n","Epoch 7/10, Train Loss: 0.3237, Train Acc: 0.8772, Valid Loss: 0.7417, Valid Acc: 0.7564\n","Epoch 8/10, Train Loss: 0.2975, Train Acc: 0.8889, Valid Loss: 0.7974, Valid Acc: 0.7536\n","Epoch 9/10, Train Loss: 0.2612, Train Acc: 0.9007, Valid Loss: 0.7659, Valid Acc: 0.7599\n","Epoch 10/10, Train Loss: 0.2572, Train Acc: 0.9021, Valid Loss: 0.7495, Valid Acc: 0.7741\n","Best Validation Accuracy: 0.7798\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Adjusted hypothetical data from the table\n","datasets = ['TST-PE', 'TST-BN']\n","our_method_accuracy = [80.81, 77.96]\n","baseline_cnn_accuracy = [84.76, 81.62]\n","baseline_lstm_accuracy = [83.14, 81.03]\n","lstnet_accuracy = [83.93, 80.75]\n","lstnet_accuracy1 = [86.07, 83.24]\n","lstnet_accuracy2 = [87.41, 84.02]\n","lstnet_accuracy3 = [87.98, 84.86]\n","\n","\n","\n","\n","\n","# Number of datasets\n","n_datasets = len(datasets)\n","\n","# Setting the positions and width for the bars\n","pos = np.arange(n_datasets)\n","bar_width = 0.1\n","\n","# Attractive color scheme\n","# colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n","# colors = ['#d9d9d9', '#bdbdbd', '#969696', '#636363']  # Light Gray, Medium Gray, Dark Gray, Darker Gray\n","colors = ['#92a8d1', '#f7f5e6', '#f7786b', '#b1cbbb','#b7b7a4', '#a3c1ad', '#f5cac3']  # Powder Blue, Ivory, Blush, Mint\n","\n","# Creating the bar graph with error bars\n","plt.figure(figsize=(12, 6))\n","\n","plt.bar(pos - 2.5*bar_width, our_method_accuracy, bar_width, color=colors[0], capsize=5, label='Fixed')\n","plt.bar(pos - 1.5*bar_width, baseline_cnn_accuracy, bar_width, color=colors[1],  capsize=5, label='Learnable')\n","plt.bar(pos - 0.5*bar_width, baseline_lstm_accuracy, bar_width, color=colors[2],  capsize=5, label='Relative')\n","plt.bar(pos + 0.5*bar_width, lstnet_accuracy, bar_width, color=colors[3], capsize=5, label='tAPE')\n","plt.bar(pos + 1.5*bar_width, lstnet_accuracy1, bar_width, color=colors[4], capsize=5, label='eRPE')\n","plt.bar(pos + 2.5*bar_width, lstnet_accuracy2, bar_width, color=colors[5], capsize=5, label='TUPE')\n","plt.bar(pos + 3.5*bar_width, lstnet_accuracy3, bar_width, color=colors[6], capsize=5, label='SPE')\n","\n","# Adding labels and title\n","plt.xlabel('Architectures')\n","plt.ylabel('Accuracy (%)')\n","plt.xticks(pos, datasets)\n","plt.legend()\n","\n","# Adjusting y-axis to show a range of accuracies from 80% to 100%\n","plt.ylim(75, 90)\n","\n","# Displaying the plot\n","plt.tight_layout()\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":607},"id":"B_z6WClpc6Z1","executionInfo":{"status":"ok","timestamp":1722630114415,"user_tz":300,"elapsed":911,"user":{"displayName":"Habib Irani","userId":"11435283309592850289"}},"outputId":"92fd4698-695b-42e9-eb64-8e21f99a213e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"ml","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[{"file_id":"1-iOVoL9YigbkHoheB830Z6i2F_tHy4YA","timestamp":1728494575387}]}},"nbformat":4,"nbformat_minor":0}