{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4L4RZaqkWMbr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil  # https://docs.python.org/3/library/shutil.html\n",
        "from shutil import unpack_archive  # to unzip\n",
        "# from shutil import make_archive # to create zip for storage\n",
        "import requests  # for downloading zip file\n",
        "from scipy import io  # for loadmat, matlab conversion\n",
        "import numpy as np\n",
        "\n",
        "# credit https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
        "# many other methods I tried failed to download the file properly\n",
        "\n",
        "\n",
        "def download_url(url, save_path, chunk_size=128):\n",
        "    r = requests.get(url, stream=True)\n",
        "    with open(save_path, 'wb') as fd:\n",
        "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "            fd.write(chunk)\n",
        "\n",
        "\n",
        "def unimib_load_dataset(\n",
        "        verbose=True,\n",
        "        incl_xyz_accel=False,  # include component accel_x/y/z in ____X data\n",
        "        # add rms value (total accel) of accel_x/y/z in ____X data\n",
        "        incl_rms_accel=True,\n",
        "        incl_val_group=False,  # True => returns x/y_test, x/y_validation, x/y_train\n",
        "    # False => combine test & validation groups\n",
        "        split_subj=dict\n",
        "    (train_subj=[4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 19, 20, 21, 22, 24, 26, 27, 29],\n",
        "     validation_subj=[1, 9, 16, 23, 25, 28],\n",
        "     test_subj=[2, 3, 13, 17, 18, 30]),\n",
        "        one_hot_encode=True):\n",
        "\n",
        "    # Download and unzip original dataset\n",
        "    if (not os.path.isfile('./UniMiB-SHAR.zip')):\n",
        "        print(\"Downloading UniMiB-SHAR.zip file\")\n",
        "        # invoking the shell command fails when exported to .py file\n",
        "        # redirect link https://www.dropbox.com/s/raw/x2fpfqj0bpf8ep6/UniMiB-SHAR.zip\n",
        "        #!wget https://www.dropbox.com/s/x2fpfqj0bpf8ep6/UniMiB-SHAR.zip\n",
        "        download_url(\n",
        "            'https://www.dropbox.com/s/raw/x2fpfqj0bpf8ep6/UniMiB-SHAR.zip', './UniMiB-SHAR.zip')\n",
        "    if (not os.path.isdir('./UniMiB-SHAR')):\n",
        "        shutil.unpack_archive('./UniMiB-SHAR.zip', '.', 'zip')\n",
        "    # Convert .mat files to numpy ndarrays\n",
        "    path_in = './UniMiB-SHAR/data'\n",
        "    # loadmat loads matlab files as dictionary, keys: header, version, globals, data\n",
        "    adl_data = io.loadmat(path_in + '/adl_data.mat')['adl_data']\n",
        "    adl_names = io.loadmat(path_in + '/adl_names.mat',\n",
        "                           chars_as_strings=True)['adl_names']\n",
        "    adl_labels = io.loadmat(path_in + '/adl_labels.mat')['adl_labels']\n",
        "\n",
        "    # Reshape data and compute total (rms) acceleration\n",
        "    num_samples = 151\n",
        "    # UniMiB SHAR has fixed size of 453 which is 151 accelX, 151 accely, 151 accelz\n",
        "    adl_data = np.reshape(adl_data, (-1, num_samples, 3),\n",
        "                          order='F')  # uses Fortran order\n",
        "    if (incl_rms_accel):\n",
        "        rms_accel = np.sqrt(\n",
        "            (adl_data[:, :, 0]**2) + (adl_data[:, :, 1]**2) + (adl_data[:, :, 2]**2))\n",
        "        adl_data = np.dstack((adl_data, rms_accel))\n",
        "\n",
        "    # remove component accel if needed\n",
        "    if (not incl_xyz_accel):\n",
        "        adl_data = np.delete(adl_data, [0, 1, 2], 2)\n",
        "\n",
        "    # matlab source was 1 indexed, change to 0 indexed\n",
        "    act_num = (adl_labels[:, 0])-1\n",
        "    sub_num = (adl_labels[:, 1])  # subject numbers are in column 1 of labels\n",
        "\n",
        "    if (not incl_val_group):\n",
        "        train_index = np.nonzero(np.isin(sub_num, split_subj['train_subj'] +\n",
        "                                         split_subj['validation_subj']))\n",
        "        x_train = adl_data[train_index]\n",
        "        y_train = act_num[train_index]\n",
        "    else:\n",
        "        train_index = np.nonzero(np.isin(sub_num, split_subj['train_subj']))\n",
        "        x_train = adl_data[train_index]\n",
        "        y_train = act_num[train_index]\n",
        "\n",
        "        validation_index = np.nonzero(\n",
        "            np.isin(sub_num, split_subj['validation_subj']))\n",
        "        x_validation = adl_data[validation_index]\n",
        "        y_validation = act_num[validation_index]\n",
        "\n",
        "    test_index = np.nonzero(np.isin(sub_num, split_subj['test_subj']))\n",
        "    x_test = adl_data[test_index]\n",
        "    y_test = act_num[test_index]\n",
        "\n",
        "    if (verbose):\n",
        "        print(\"x/y_train shape \", x_train.shape, y_train.shape)\n",
        "    if (incl_val_group):\n",
        "        print(\"x/y_validation shape \", x_validation.shape, y_validation.shape)\n",
        "    print(\"x/y_test shape  \", x_test.shape, y_test.shape)\n",
        "\n",
        "    if (incl_val_group):\n",
        "        return x_train, y_train, x_validation, y_validation, x_test, y_test\n",
        "    else:\n",
        "        return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oglWkA3WMbs",
        "outputId": "753f7b9b-1849-4ae9-9f7a-6e93a4b7b1a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading UniMiB-SHAR.zip file\n",
            "x/y_train shape  (4601, 151, 4) (4601,)\n",
            "x/y_validation shape  (1454, 151, 4) (1454,)\n",
            "x/y_test shape   (1524, 151, 4) (1524,)\n",
            "Number of classes: 9\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train, X_valid, y_valid, X_test, y_test = unimib_load_dataset(\n",
        "    incl_xyz_accel=True, incl_val_group=True, one_hot_encode=False, verbose=True\n",
        ")\n",
        "\n",
        "# Number of classes\n",
        "n_classes = len(np.unique(y_train))\n",
        "print(f\"Number of classes: {n_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzMPbGeUWMbt"
      },
      "source": [
        "## 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QAP9J-zwWMbt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv80iRYUWMbt"
      },
      "source": [
        "### Load the train, validation, and test sets into PyTorch DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0UV-Y5vfWMbt"
      },
      "outputs": [],
      "source": [
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "Y_train = torch.tensor(y_train, dtype=torch.int64)\n",
        "\n",
        "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "Y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train, Y_train)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "valid_dataset = TensorDataset(X_valid, Y_valid)\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, Y_test)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1tFm3BmWMbu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class TimeSeriesPatchEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Calculate the number of patches, adjusting for padding if necessary\n",
        "        # Ceiling division to account for padding\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "        self.padding = (\n",
        "            self.num_patches * patch_size\n",
        "        ) - input_timesteps  # Calculate padding length\n",
        "\n",
        "        self.conv_layer = nn.Conv1d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embedding_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "        )\n",
        "\n",
        "        self.class_token_embeddings = nn.Parameter(\n",
        "            torch.randn((1, 1, embedding_dim), requires_grad=True)\n",
        "        )\n",
        "        self.position_embeddings = PositionalEncoding(embedding_dim, dropout=0.1, max_len=input_timesteps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pad the input sequence if necessary\n",
        "        if self.padding > 0:\n",
        "            x = nn.functional.pad(x, (0, 0, 0, self.padding))  # Pad the second to last dimension, which is input_timesteps\n",
        "\n",
        "        # We use a Conv1d layer to generate the patch embeddings\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, timesteps)\n",
        "        conv_output = self.conv_layer(x)\n",
        "        conv_output = conv_output.permute(0, 2, 1)  # (batch, timesteps, features)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n",
        "        output = torch.cat((class_tokens, conv_output), dim=1)\n",
        "\n",
        "        output = self.position_embeddings(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def pos_encoding(self, q_len, d_model, normalize=True):\n",
        "        pe = torch.zeros(q_len, d_model)\n",
        "        position = torch.arange(0, q_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        if normalize:\n",
        "            pe = pe - pe.mean()\n",
        "            pe = pe / (pe.std() * 10)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_encoding(q_len = x.size(1), d_model = x.size(2))\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(LearnedPositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "def get_pos_encoder(pos_encoding):\n",
        "    if pos_encoding == 'fixed':\n",
        "        return PositionalEncoding\n",
        "    elif pos_encoding == 'learned':\n",
        "        return LearnedPositionalEncoding\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8MKozUTWMbu",
        "outputId": "a7b1cf8c-0d9e-4986-fb9b-36acff1910ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=================================================================================================================================================\n",
              "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
              "=================================================================================================================================================\n",
              "TimeSeriesPatchEmbeddingLayer (TimeSeriesPatchEmbeddingLayer)     [64, 151, 4]         [64, 20, 32]         32                   True\n",
              "├─Conv1d (conv_layer)                                             [64, 4, 152]         [64, 32, 19]         1,056                True\n",
              "├─PositionalEncoding (position_embeddings)                        [64, 20, 32]         [64, 20, 32]         --                   --\n",
              "│    └─Dropout (dropout)                                          [64, 20, 32]         [64, 20, 32]         --                   --\n",
              "=================================================================================================================================================\n",
              "Total params: 1,088\n",
              "Trainable params: 1,088\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.28\n",
              "=================================================================================================================================================\n",
              "Input size (MB): 0.15\n",
              "Forward/backward pass size (MB): 0.31\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.47\n",
              "================================================================================================================================================="
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random_instances, random_labels = next(iter(train_loader))\n",
        "random_instance = random_instances[0]\n",
        "\n",
        "BATCH_SIZE = random_instances.shape[0]\n",
        "TIMESTEPS = random_instance.shape[0]\n",
        "CHANNELS = random_instance.shape[1]\n",
        "PATCH_SIZE = 8\n",
        "\n",
        "patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    embedding_dim=CHANNELS * PATCH_SIZE,\n",
        "    input_timesteps=TIMESTEPS,\n",
        ")\n",
        "\n",
        "patch_embeddings = patch_embedding_layer(random_instances)\n",
        "patch_embeddings.shape\n",
        "\n",
        "summary(\n",
        "    model=patch_embedding_layer,\n",
        "    # (batch_size, input_channels, input_timesteps)\n",
        "    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s30BKFOOWMbu"
      },
      "outputs": [],
      "source": [
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps)\n",
        "\n",
        "        # Calculate the number of patches\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        # Setting batch_first=True to accommodate inputs with batch dimension first\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n",
        "\n",
        "        # Feedforward layer\n",
        "        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n",
        "        # Classifier Head\n",
        "        self.classifier = nn.Linear(dim_feedforward, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, in_channels, input_timesteps)\n",
        "\n",
        "        # Get patch embeddings\n",
        "        x = self.patch_embedding(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n",
        "\n",
        "        # Apply Transformer Encoder with batch first\n",
        "        x = self.transformer_encoder(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n",
        "\n",
        "        # Use the output corresponding to the class token for classification\n",
        "        class_token_output = x[:, 0, :]  # Select the class token for each item in the batch\n",
        "\n",
        "        # Feedforward layer\n",
        "        x = self.ff_layer(class_token_output)\n",
        "\n",
        "        # Classifier head\n",
        "        output = self.classifier(x)  # Output shape: (batch_size, num_classes)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3tGq9kmWMbu",
        "outputId": "e449b236-dd43-473a-b4f2-d799a9c15ff6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=======================================================================================================================================\n",
              "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
              "=======================================================================================================================================\n",
              "TimeSeriesTransformer (TimeSeriesTransformer)           [64, 151, 4]         [64, 9]              --                   True\n",
              "├─TimeSeriesPatchEmbeddingLayer (patch_embedding)       [64, 151, 4]         [64, 20, 32]         32                   True\n",
              "│    └─Conv1d (conv_layer)                              [64, 4, 152]         [64, 32, 19]         1,056                True\n",
              "│    └─PositionalEncoding (position_embeddings)         [64, 20, 32]         [64, 20, 32]         --                   --\n",
              "│    │    └─Dropout (dropout)                           [64, 20, 32]         [64, 20, 32]         --                   --\n",
              "├─TransformerEncoder (transformer_encoder)              [64, 20, 32]         [64, 20, 32]         --                   True\n",
              "│    └─ModuleList (layers)                              --                   --                   --                   True\n",
              "│    │    └─TransformerEncoderLayer (0)                 [64, 20, 32]         [64, 20, 32]         12,704               True\n",
              "│    │    └─TransformerEncoderLayer (1)                 [64, 20, 32]         [64, 20, 32]         12,704               True\n",
              "│    │    └─TransformerEncoderLayer (2)                 [64, 20, 32]         [64, 20, 32]         12,704               True\n",
              "│    │    └─TransformerEncoderLayer (3)                 [64, 20, 32]         [64, 20, 32]         12,704               True\n",
              "├─Linear (ff_layer)                                     [64, 32]             [64, 128]            4,224                True\n",
              "├─Linear (classifier)                                   [64, 128]            [64, 9]              1,161                True\n",
              "=======================================================================================================================================\n",
              "Total params: 57,289\n",
              "Trainable params: 57,289\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.63\n",
              "=======================================================================================================================================\n",
              "Input size (MB): 0.15\n",
              "Forward/backward pass size (MB): 0.38\n",
              "Params size (MB): 0.03\n",
              "Estimated Total Size (MB): 0.56\n",
              "======================================================================================================================================="
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler to reduce the learning rate by the specified step size and factor (gamma) every step_size epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "summary(\n",
        "    model=model,\n",
        "    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHiMwsYvVAzp"
      },
      "source": [
        "## Attentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yD7K5F3D-Hji"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "import pandas as pd\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Scl(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.relative_bias_table = nn.Parameter(torch.zeros((2 * self.seq_len - 1), num_heads))\n",
        "        coords = torch.meshgrid((torch.arange(1), torch.arange(self.seq_len)))\n",
        "        coords = torch.flatten(torch.stack(coords), 1)\n",
        "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
        "        relative_coords[1] += self.seq_len - 1\n",
        "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
        "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
        "        self.register_buffer(\"relative_index\", relative_index)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, 8))\n",
        "        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=1 * self.seq_len, w=1 * self.seq_len)\n",
        "        attn = attn + relative_bias\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Vec(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.Er = nn.Parameter(torch.randn(self.seq_len, int(emb_size / num_heads)))\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.tril(torch.ones(self.seq_len, self.seq_len))\n",
        "                .unsqueeze(0).unsqueeze(0)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        QEr = torch.matmul(q, self.Er.transpose(0, 1))\n",
        "        Srel = self.skew(QEr)\n",
        "\n",
        "        attn = torch.matmul(q, k)\n",
        "        attn = (attn + Srel) * self.scale\n",
        "\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "    def skew(self, QEr):\n",
        "        padded = nn.functional.pad(QEr, (1, 0))\n",
        "        batch_size, num_heads, num_rows, num_cols = padded.shape\n",
        "        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
        "        Srel = reshaped[:, :, 1:, :]\n",
        "        return Srel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B2e6n_Yj_38p"
      },
      "outputs": [],
      "source": [
        "class TUPEConfig:\n",
        "    num_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    d_model: int = 128\n",
        "    d_head: int = 0\n",
        "    max_len: int = 256\n",
        "    dropout: float = 0.1\n",
        "    expansion_factor: int = 1\n",
        "    relative_bias: bool = True\n",
        "    bidirectional_bias: bool = True\n",
        "    num_buckets: int = 32\n",
        "    max_distance: int = 128\n",
        "\n",
        "    def __post_init__(self):\n",
        "        d_head, remainder = divmod(self.d_model, self.num_heads)\n",
        "        assert remainder == 0, \"`d_model` should be divisible by `num_heads`\"\n",
        "        self.d_head = d_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1QbW7ihvBByl"
      },
      "outputs": [],
      "source": [
        "def _get_relative_position_bucket(\n",
        "    relative_position, bidirectional, num_buckets, max_distance\n",
        "):\n",
        "    \"\"\"\n",
        "    from https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/modeling_t5.py\n",
        "    \"\"\"\n",
        "    relative_buckets = 0\n",
        "    if bidirectional:\n",
        "        num_buckets //= 2\n",
        "        relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n",
        "        relative_position = torch.abs(relative_position)\n",
        "    else:\n",
        "        relative_position = -torch.min(\n",
        "            relative_position, torch.zeros_like(relative_position)\n",
        "        )\n",
        "    # now relative_position is in the range [0, inf)\n",
        "\n",
        "    # half of the buckets are for exact increments in positions\n",
        "    max_exact = num_buckets // 2\n",
        "    is_small = relative_position < max_exact\n",
        "\n",
        "    # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n",
        "    relative_postion_if_large = max_exact + (\n",
        "        torch.log(relative_position.float() / max_exact)\n",
        "        / math.log(max_distance / max_exact)\n",
        "        * (num_buckets - max_exact)\n",
        "    ).to(torch.long)\n",
        "    relative_postion_if_large = torch.min(\n",
        "        relative_postion_if_large,\n",
        "        torch.full_like(relative_postion_if_large, num_buckets - 1),\n",
        "    )\n",
        "\n",
        "    relative_buckets += torch.where(\n",
        "        is_small, relative_position, relative_postion_if_large\n",
        "    )\n",
        "    return relative_buckets\n",
        "\n",
        "\n",
        "def get_relative_positions(\n",
        "    seq_len, bidirectional=True, num_buckets=32, max_distance=128\n",
        "):\n",
        "    x = torch.arange(seq_len)[None, :]\n",
        "    y = torch.arange(seq_len)[:, None]\n",
        "    relative_positions = _get_relative_position_bucket(\n",
        "        x - y, bidirectional, num_buckets, max_distance\n",
        "    )\n",
        "    return relative_positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9uW0SNXu_oda"
      },
      "outputs": [],
      "source": [
        "class TUPEMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config: TUPEConfig, pos_embed: nn.Module) -> None:\n",
        "        super().__init__()\n",
        "        self.max_len = config.max_len\n",
        "        self.num_heads = config.num_heads\n",
        "        self.num_buckets = config.num_buckets\n",
        "        self.max_distance = config.max_distance\n",
        "        self.bidirectional = config.bidirectional_bias\n",
        "        self.scale = math.sqrt(2 * config.d_head)\n",
        "\n",
        "        self.pos_embed = pos_embed\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # kqv in one pass\n",
        "        self.pos_kq = nn.Linear(config.d_model, 2 * config.d_model, bias=False)\n",
        "        self.tok_kqv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n",
        "\n",
        "        self.relative_bias = config.relative_bias\n",
        "        if config.relative_bias:\n",
        "            self.bias = nn.Embedding(config.max_len * 2, config.num_heads)\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        pos_embed = self.pos_embed(seq_len).repeat(batch_size, 1, 1)\n",
        "        # pos_embed.shape == (batch_size, seq_len, d_model)\n",
        "        pos_key, pos_query = self.pos_kq(pos_embed).chunk(2, dim=-1)\n",
        "        pos_key = pos_key.view(batch_size, seq_len, self.num_heads, -1).permute(\n",
        "            0, 2, 3, 1\n",
        "        )\n",
        "        # pos_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        pos_query = pos_query.view(batch_size, seq_len, self.num_heads, -1).transpose(\n",
        "            1, 2\n",
        "        )\n",
        "        # pos_query.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        pos_attn = torch.matmul(pos_query, pos_key)\n",
        "        # pos_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        tok_key, tok_query, tok_value = self.tok_kqv(x).chunk(3, dim=-1)\n",
        "        tok_key = tok_key.view(batch_size, seq_len, self.num_heads, -1).permute(\n",
        "            0, 2, 3, 1\n",
        "        )\n",
        "        # tok_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        tok_query = tok_query.view(batch_size, seq_len, self.num_heads, -1).transpose(\n",
        "            1, 2\n",
        "        )\n",
        "        tok_value = tok_value.view(batch_size, seq_len, self.num_heads, -1).transpose(\n",
        "            1, 2\n",
        "        )\n",
        "        # tok_qv.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        tok_attn = torch.matmul(tok_query, tok_key)\n",
        "        # tok_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        attn = (tok_attn + pos_attn) / self.scale\n",
        "        if self.relative_bias:\n",
        "            relative_positions = get_relative_positions(\n",
        "                seq_len, self.bidirectional, self.num_buckets, self.max_distance\n",
        "            ).to(attn.device)\n",
        "            # relative_positions.shape == (seq_len, seq_len)\n",
        "            bias = self.bias(relative_positions + self.max_len)\n",
        "            # bias.shape == (seq_len, seq_len, num_heads)\n",
        "            bias = bias.permute(2, 0, 1).unsqueeze(0)\n",
        "            # bias.shape == (1, num_heads, seq_len, seq_len)\n",
        "            attn = attn + bias\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "        out = torch.matmul(attn, tok_value)\n",
        "        # out.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
        "        # out.shape == (batch_size, seq_len, d_model)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positional Encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeNs_BKxfx0i",
        "outputId": "182864b1-46da-46cf-bb3a-7ce043a3ee1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 1.6501, Train Acc: 0.3796\n",
            "Epoch 1: New best model saved with validation accuracy: 0.5142\n",
            "Epoch 1, Val Loss: 1.3530, Val Acc: 0.5142\n",
            "Epoch 1, Train Loss: 1.1061, Train Acc: 0.5761\n",
            "Epoch 2: New best model saved with validation accuracy: 0.5639\n",
            "Epoch 2, Val Loss: 1.1899, Val Acc: 0.5639\n",
            "Epoch 1, Train Loss: 0.9070, Train Acc: 0.6477\n",
            "Epoch 3: New best model saved with validation accuracy: 0.5987\n",
            "Epoch 3, Val Loss: 1.0092, Val Acc: 0.5987\n",
            "Epoch 1, Train Loss: 0.7754, Train Acc: 0.6879\n",
            "Epoch 4, Val Loss: 1.1997, Val Acc: 0.5717\n",
            "Epoch 1, Train Loss: 0.7002, Train Acc: 0.7216\n",
            "Epoch 5: New best model saved with validation accuracy: 0.6193\n",
            "Epoch 5, Val Loss: 1.0781, Val Acc: 0.6193\n",
            "Epoch 1, Train Loss: 0.6234, Train Acc: 0.7542\n",
            "Epoch 6: New best model saved with validation accuracy: 0.7081\n",
            "Epoch 6, Val Loss: 0.8268, Val Acc: 0.7081\n",
            "Epoch 1, Train Loss: 0.6098, Train Acc: 0.7590\n",
            "Epoch 7, Val Loss: 0.9523, Val Acc: 0.6925\n",
            "Epoch 1, Train Loss: 0.5439, Train Acc: 0.7881\n",
            "Epoch 8: New best model saved with validation accuracy: 0.7472\n",
            "Epoch 8, Val Loss: 0.8053, Val Acc: 0.7472\n",
            "Epoch 1, Train Loss: 0.4932, Train Acc: 0.8059\n",
            "Epoch 9, Val Loss: 0.9158, Val Acc: 0.7251\n",
            "Epoch 1, Train Loss: 0.4799, Train Acc: 0.8048\n",
            "Epoch 10, Val Loss: 1.0091, Val Acc: 0.6989\n",
            "Loaded best model for testing or further use.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import math\n",
        "from torchsummary import summary\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def pos_encoding(self, q_len, d_model, normalize=True):\n",
        "        pe = torch.zeros(q_len, d_model)\n",
        "        position = torch.arange(0, q_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        if normalize:\n",
        "            pe = pe - pe.mean()\n",
        "            pe = pe / (pe.std() * 10)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_encoding(q_len=x.size(1), d_model=x.size(2))\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Learned Positional Encoding\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(LearnedPositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Time Series Patch Embedding Layer\n",
        "class TimeSeriesPatchEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps, pos_encoding='fixed'):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "        self.padding = (self.num_patches * patch_size) - input_timesteps\n",
        "\n",
        "        self.conv_layer = nn.Conv1d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embedding_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "        )\n",
        "\n",
        "        self.class_token_embeddings = nn.Parameter(\n",
        "            torch.randn((1, 1, embedding_dim), requires_grad=True)\n",
        "        )\n",
        "\n",
        "        # Instantiate the positional encoding\n",
        "        pos_encoder_class = get_pos_encoder(pos_encoding)\n",
        "        self.position_embeddings = pos_encoder_class(embedding_dim, dropout=0.1, max_len=input_timesteps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.padding > 0:\n",
        "            x = nn.functional.pad(x, (0, 0, 0, self.padding))\n",
        "\n",
        "        x = x.permute(0, 2, 1)\n",
        "        conv_output = self.conv_layer(x)\n",
        "        conv_output = conv_output.permute(0, 2, 1)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n",
        "        output = torch.cat((class_tokens, conv_output), dim=1)\n",
        "\n",
        "        output = self.position_embeddings(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Get Positional Encoding\n",
        "def get_pos_encoder(pos_encoding):\n",
        "    if pos_encoding == 'fixed':\n",
        "        return PositionalEncoding\n",
        "    elif pos_encoding == 'learned':\n",
        "        return LearnedPositionalEncoding\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, pos_encoding='fixed', num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2, attention_type='absolute'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps, pos_encoding)\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "\n",
        "        if attention_type == 'relative_scl':\n",
        "            self.attention_layer = Attention_Rel_Scl(embedding_dim, num_heads, self.num_patches + 1, dropout)\n",
        "        elif attention_type == 'relative_vec':\n",
        "            self.attention_layer = Attention_Rel_Vec(embedding_dim, num_heads, self.num_patches + 1, dropout)\n",
        "        elif attention_type == 'tupe':\n",
        "            self.attention_layer = TUPEMultiHeadAttention(tupe_config, pos_embed)\n",
        "        else:\n",
        "            self.attention_layer = Attention(embedding_dim, num_heads, dropout)\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n",
        "\n",
        "        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n",
        "        self.classifier = nn.Linear(dim_feedforward, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        x = self.attention_layer(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        class_token_output = x[:, 0, :]\n",
        "        x = self.ff_layer(class_token_output)\n",
        "        output = self.classifier(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "random_instances, random_labels = next(iter(train_loader))\n",
        "random_instance = random_instances[0]\n",
        "\n",
        "BATCH_SIZE = random_instances.shape[0]\n",
        "TIMESTEPS = random_instance.shape[0]\n",
        "CHANNELS = random_instance.shape[1]\n",
        "PATCH_SIZE = 8\n",
        "\n",
        "patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    embedding_dim=CHANNELS * PATCH_SIZE,\n",
        "    input_timesteps=TIMESTEPS,\n",
        ")\n",
        "\n",
        "patch_embeddings = patch_embedding_layer(random_instances)\n",
        "patch_embeddings.shape\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    pos_encoding='fixed',\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "    val_loss = total_loss / len(valid_loader)\n",
        "    val_acc = correct / total_val\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n",
        "    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n",
        "\n",
        "    if val_acc > best_validation_acc:\n",
        "        best_validation_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dplhsyvE-jR3"
      },
      "outputs": [],
      "source": [
        "class tAPE(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n",
        "        super(tAPE, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin((position * div_term) * (d_model / max_len))\n",
        "        pe[:, 1::2] = torch.cos((position * div_term) * (d_model / max_len))\n",
        "        pe = scale_factor * pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class AbsolutePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n",
        "        super(AbsolutePositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = scale_factor * pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024):\n",
        "        super(LearnablePositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pe = nn.Parameter(torch.empty(max_len, d_model))\n",
        "        nn.init.uniform_(self.pe, -0.02, 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(1), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "q9sb14F6-kc8"
      },
      "outputs": [],
      "source": [
        "def get_pos_encoder(pos_encoding):\n",
        "    if pos_encoding == 'fixed':\n",
        "        return PositionalEncoding\n",
        "    elif pos_encoding == 'learned':\n",
        "        return LearnedPositionalEncoding\n",
        "    elif pos_encoding == 'tAPE':\n",
        "        return tAPE\n",
        "    elif pos_encoding == 'absolute':\n",
        "        return AbsolutePositionalEncoding\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results with Absolute positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egoVdYVX-mq4",
        "outputId": "5b5b8ae0-c4c4-42b2-f2d5-addb33d88ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 1.5954, Train Acc: 0.4065\n",
            "Epoch 1: New best model saved with validation accuracy: 0.4759\n",
            "Epoch 1, Val Loss: 1.2750, Val Acc: 0.4759\n",
            "Epoch 1, Train Loss: 1.0390, Train Acc: 0.5885\n",
            "Epoch 2: New best model saved with validation accuracy: 0.5923\n",
            "Epoch 2, Val Loss: 1.1304, Val Acc: 0.5923\n",
            "Epoch 1, Train Loss: 0.8391, Train Acc: 0.6736\n",
            "Epoch 3: New best model saved with validation accuracy: 0.6129\n",
            "Epoch 3, Val Loss: 1.0498, Val Acc: 0.6129\n",
            "Epoch 1, Train Loss: 0.6992, Train Acc: 0.7254\n",
            "Epoch 4: New best model saved with validation accuracy: 0.6463\n",
            "Epoch 4, Val Loss: 1.0714, Val Acc: 0.6463\n",
            "Epoch 1, Train Loss: 0.6145, Train Acc: 0.7650\n",
            "Epoch 5: New best model saved with validation accuracy: 0.6776\n",
            "Epoch 5, Val Loss: 0.9883, Val Acc: 0.6776\n",
            "Epoch 1, Train Loss: 0.5734, Train Acc: 0.7718\n",
            "Epoch 6: New best model saved with validation accuracy: 0.6982\n",
            "Epoch 6, Val Loss: 0.9876, Val Acc: 0.6982\n",
            "Epoch 1, Train Loss: 0.5050, Train Acc: 0.8061\n",
            "Epoch 7: New best model saved with validation accuracy: 0.7393\n",
            "Epoch 7, Val Loss: 0.8125, Val Acc: 0.7393\n",
            "Epoch 1, Train Loss: 0.4658, Train Acc: 0.8191\n",
            "Epoch 8, Val Loss: 0.8750, Val Acc: 0.7344\n",
            "Epoch 1, Train Loss: 0.4291, Train Acc: 0.8327\n",
            "Epoch 9: New best model saved with validation accuracy: 0.7564\n",
            "Epoch 9, Val Loss: 0.9399, Val Acc: 0.7564\n",
            "Epoch 1, Train Loss: 0.4284, Train Acc: 0.8290\n",
            "Epoch 10, Val Loss: 0.8568, Val Acc: 0.7259\n",
            "Loaded best model for testing or further use.\n"
          ]
        }
      ],
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    pos_encoding='absolute',\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "    val_loss = total_loss / len(valid_loader)\n",
        "    val_acc = correct / total_val\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n",
        "    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n",
        "\n",
        "    if val_acc > best_validation_acc:\n",
        "        best_validation_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results with relative_vec attention and fixed positionl encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzFfH3xX8fE_",
        "outputId": "cf00dad9-6098-4a15-9d9c-20e9eba6f7e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 1.6165, Train Acc: 0.4038\n",
            "Epoch 1: New best model saved with validation accuracy: 0.5355\n",
            "Epoch 1, Val Loss: 1.2094, Val Acc: 0.5355\n",
            "Epoch 1, Train Loss: 1.0285, Train Acc: 0.6041\n",
            "Epoch 2: New best model saved with validation accuracy: 0.6378\n",
            "Epoch 2, Val Loss: 0.9732, Val Acc: 0.6378\n",
            "Epoch 1, Train Loss: 0.8162, Train Acc: 0.6897\n",
            "Epoch 3: New best model saved with validation accuracy: 0.6967\n",
            "Epoch 3, Val Loss: 0.7790, Val Acc: 0.6967\n",
            "Epoch 1, Train Loss: 0.6865, Train Acc: 0.7300\n",
            "Epoch 4, Val Loss: 1.0164, Val Acc: 0.6676\n",
            "Epoch 1, Train Loss: 0.6465, Train Acc: 0.7520\n",
            "Epoch 5: New best model saved with validation accuracy: 0.7287\n",
            "Epoch 5, Val Loss: 0.8559, Val Acc: 0.7287\n",
            "Epoch 1, Train Loss: 0.5527, Train Acc: 0.7883\n",
            "Epoch 6: New best model saved with validation accuracy: 0.7308\n",
            "Epoch 6, Val Loss: 0.7263, Val Acc: 0.7308\n",
            "Epoch 1, Train Loss: 0.5083, Train Acc: 0.7997\n",
            "Epoch 7: New best model saved with validation accuracy: 0.7649\n",
            "Epoch 7, Val Loss: 0.6703, Val Acc: 0.7649\n",
            "Epoch 1, Train Loss: 0.4711, Train Acc: 0.8129\n",
            "Epoch 8: New best model saved with validation accuracy: 0.7876\n",
            "Epoch 8, Val Loss: 0.6128, Val Acc: 0.7876\n",
            "Epoch 1, Train Loss: 0.4450, Train Acc: 0.8261\n",
            "Epoch 9, Val Loss: 0.6335, Val Acc: 0.7720\n",
            "Epoch 1, Train Loss: 0.4347, Train Acc: 0.8371\n",
            "Epoch 10, Val Loss: 0.7715, Val Acc: 0.7812\n",
            "Loaded best model for testing or further use.\n"
          ]
        }
      ],
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    pos_encoding='fixed',\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        "    attention_type= 'relative_vec',\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "    val_loss = total_loss / len(valid_loader)\n",
        "    val_acc = correct / total_val\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n",
        "    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n",
        "\n",
        "    if val_acc > best_validation_acc:\n",
        "        best_validation_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "alrtTQdHWMbt",
        "tPO8SLCpWMbu",
        "Na4sOhN7WMbv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
