{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Directory where the dataset will be downloaded and extracted\n",
        "DATA_DIR = 'datasets'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def download_dataset(dataset_name, url):\n",
        "    \"\"\"\n",
        "    Downloads and extracts a zip file containing the dataset.\n",
        "    \"\"\"\n",
        "    zip_path = os.path.join(DATA_DIR, f\"{dataset_name}.zip\")\n",
        "    extract_path = os.path.join(DATA_DIR, dataset_name)\n",
        "\n",
        "    # Download the dataset\n",
        "    print(f\"Downloading {dataset_name} from {url}...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "    # Extract the zip file\n",
        "    print(f\"Extracting {dataset_name}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "        file_names = zip_ref.namelist()\n",
        "        print(f\"Files in the zip: {file_names}\")\n",
        "\n",
        "    # Remove the zip file after extraction\n",
        "    os.remove(zip_path)\n",
        "    print(f\"Dataset {dataset_name} extracted to {extract_path}.\")\n",
        "    return extract_path, file_names\n",
        "\n",
        "def load_data(file_path, encoding='ISO-8859-1'):\n",
        "    \"\"\"\n",
        "    Loads the dataset from a CSV file and returns a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from {file_path}\")\n",
        "    df = pd.read_csv(file_path, encoding=encoding, on_bad_lines='skip')\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df, batch_size=64, time_steps=30):\n",
        "    \"\"\"\n",
        "    Preprocesses the data:\n",
        "    - Drops Date and Time columns.\n",
        "    - Uses all remaining features.\n",
        "    - Normalizes the features.\n",
        "    - Reshapes data into 3D tensors (samples, time_steps, features).\n",
        "    - Splits into training, validation, and test sets.\n",
        "    \"\"\"\n",
        "    print(\"Starting preprocessing...\")\n",
        "\n",
        "    # Drop Date and Time columns\n",
        "    df = df.drop(columns=['Date', 'Time'])\n",
        "    print(\"Dropped Date and Time columns.\")\n",
        "\n",
        "    # Use all remaining columns as features\n",
        "    X = df.drop(columns=['Room_Occupancy_Count'])\n",
        "    y = df['Room_Occupancy_Count']\n",
        "    features = X.shape[1]  # Set features dynamically based on remaining columns\n",
        "    print(f\"Total features used: {features}\")\n",
        "\n",
        "    # Ensure total samples are compatible with reshaping\n",
        "    total_samples = (len(X) // time_steps) * time_steps\n",
        "    X, y = X.iloc[:total_samples], y.iloc[:total_samples]\n",
        "    print(f\"Adjusted dataset size: {X.shape}, Labels size: {y.shape}\")\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    print(\"Features normalized.\")\n",
        "\n",
        "    # Reshape X into (samples, time_steps, features)\n",
        "    X = X.reshape(-1, time_steps, features)\n",
        "    y = y.values.reshape(-1, time_steps)[:, 0]  # Ensure y matches the number of sequences in X\n",
        "    print(f\"Reshaped X to: {X.shape}, Reshaped y to: {y.shape}\")\n",
        "\n",
        "    # Split into training, validation, and test sets\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "    print(\"Data split into training, validation, and test sets.\")\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.int64)\n",
        "\n",
        "    X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "    # Output dataset shapes\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    print(\"Preprocessing complete.\")\n",
        "    return train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test\n",
        "\n",
        "# Main function to run the entire process\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_name = 'RoomOccupancy'\n",
        "    dataset_url = 'https://archive.ics.uci.edu/static/public/864/room+occupancy+estimation.zip'\n",
        "\n",
        "    # Download and extract the dataset\n",
        "    extract_path, file_names = download_dataset(dataset_name, dataset_url)\n",
        "\n",
        "    # Load the data\n",
        "    main_file_path = os.path.join(extract_path, file_names[0])  # Assuming the first file is the main CSV\n",
        "    df = load_data(main_file_path)\n",
        "\n",
        "    # Preprocess the data\n",
        "    train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test = preprocess_data(df)\n",
        "\n",
        "    # Output the number of classes\n",
        "    n_classes = len(torch.unique(y_train))\n",
        "    print(f\"Number of classes: {n_classes}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhY7xL6OHwPb",
        "outputId": "f29702dc-3c9b-48cd-9baf-408253e9c68c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading RoomOccupancy from https://archive.ics.uci.edu/static/public/864/room+occupancy+estimation.zip...\n",
            "Extracting RoomOccupancy...\n",
            "Files in the zip: ['Occupancy_Estimation.csv']\n",
            "Dataset RoomOccupancy extracted to datasets/RoomOccupancy.\n",
            "Loading data from datasets/RoomOccupancy/Occupancy_Estimation.csv\n",
            "Starting preprocessing...\n",
            "Dropped Date and Time columns.\n",
            "Total features used: 16\n",
            "Adjusted dataset size: (10110, 16), Labels size: (10110,)\n",
            "Features normalized.\n",
            "Reshaped X to: (337, 30, 16), Reshaped y to: (337,)\n",
            "Data split into training, validation, and test sets.\n",
            "X_train shape: torch.Size([202, 30, 16]), y_train shape: torch.Size([202])\n",
            "X_valid shape: torch.Size([67, 30, 16]), y_valid shape: torch.Size([67])\n",
            "X_test shape: torch.Size([68, 30, 16]), y_test shape: torch.Size([68])\n",
            "Preprocessing complete.\n",
            "Number of classes: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDT8m0OYWpQj",
        "outputId": "c98b1699-c285-4f7b-f7ec-518aa066988b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QAP9J-zwWMbt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchinfo import summary\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j1tFm3BmWMbu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class TimeSeriesPatchEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Calculate the number of patches, adjusting for padding if necessary\n",
        "        # Ceiling division to account for padding\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "        self.padding = (\n",
        "            self.num_patches * patch_size\n",
        "        ) - input_timesteps  # Calculate padding length\n",
        "\n",
        "        self.conv_layer = nn.Conv1d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embedding_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "        )\n",
        "\n",
        "        self.class_token_embeddings = nn.Parameter(\n",
        "            torch.randn((1, 1, embedding_dim), requires_grad=True)\n",
        "        )\n",
        "        self.position_embeddings = PositionalEncoding(embedding_dim, dropout=0.1, max_len=input_timesteps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pad the input sequence if necessary\n",
        "        if self.padding > 0:\n",
        "            x = nn.functional.pad(x, (0, 0, 0, self.padding))  # Pad the second to last dimension, which is input_timesteps\n",
        "\n",
        "        # We use a Conv1d layer to generate the patch embeddings\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, timesteps)\n",
        "        conv_output = self.conv_layer(x)\n",
        "        conv_output = conv_output.permute(0, 2, 1)  # (batch, timesteps, features)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n",
        "        output = torch.cat((class_tokens, conv_output), dim=1)\n",
        "\n",
        "        output = self.position_embeddings(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def pos_encoding(self, q_len, d_model, normalize=True):\n",
        "        pe = torch.zeros(q_len, d_model)\n",
        "        position = torch.arange(0, q_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        if normalize:\n",
        "            pe = pe - pe.mean()\n",
        "            pe = pe / (pe.std() * 10)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_encoding(q_len = x.size(1), d_model = x.size(2))\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(LearnedPositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "def get_pos_encoder(pos_encoding):\n",
        "    if pos_encoding == 'fixed':\n",
        "        return PositionalEncoding\n",
        "    elif pos_encoding == 'learned':\n",
        "        return LearnedPositionalEncoding\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8MKozUTWMbu",
        "outputId": "65f36a52-4982-4e78-be10-1b3ab8b4c3d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================================================================================================\n",
              "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
              "=================================================================================================================================================\n",
              "TimeSeriesPatchEmbeddingLayer (TimeSeriesPatchEmbeddingLayer)     [64, 30, 16]         [64, 5, 128]         128                  True\n",
              "├─Conv1d (conv_layer)                                             [64, 16, 32]         [64, 128, 4]         16,512               True\n",
              "├─PositionalEncoding (position_embeddings)                        [64, 5, 128]         [64, 5, 128]         --                   --\n",
              "│    └─Dropout (dropout)                                          [64, 5, 128]         [64, 5, 128]         --                   --\n",
              "=================================================================================================================================================\n",
              "Total params: 16,640\n",
              "Trainable params: 16,640\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 4.23\n",
              "=================================================================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 0.26\n",
              "Params size (MB): 0.07\n",
              "Estimated Total Size (MB): 0.45\n",
              "================================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "random_instances, random_labels = next(iter(train_loader))\n",
        "random_instance = random_instances[0]\n",
        "\n",
        "BATCH_SIZE = random_instances.shape[0]\n",
        "TIMESTEPS = random_instance.shape[0]\n",
        "CHANNELS = random_instance.shape[1]\n",
        "PATCH_SIZE = 8\n",
        "\n",
        "patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    embedding_dim=CHANNELS * PATCH_SIZE,\n",
        "    input_timesteps=TIMESTEPS,\n",
        ")\n",
        "\n",
        "patch_embeddings = patch_embedding_layer(random_instances)\n",
        "patch_embeddings.shape\n",
        "\n",
        "summary(\n",
        "    model=patch_embedding_layer,\n",
        "    # (batch_size, input_channels, input_timesteps)\n",
        "    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s30BKFOOWMbu"
      },
      "outputs": [],
      "source": [
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps)\n",
        "\n",
        "        # Calculate the number of patches\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        # Setting batch_first=True to accommodate inputs with batch dimension first\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n",
        "\n",
        "        # Feedforward layer\n",
        "        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n",
        "        # Classifier Head\n",
        "        self.classifier = nn.Linear(dim_feedforward, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, in_channels, input_timesteps)\n",
        "\n",
        "        # Get patch embeddings\n",
        "        x = self.patch_embedding(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n",
        "\n",
        "        # Apply Transformer Encoder with batch first\n",
        "        x = self.transformer_encoder(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n",
        "\n",
        "        # Use the output corresponding to the class token for classification\n",
        "        class_token_output = x[:, 0, :]  # Select the class token for each item in the batch\n",
        "\n",
        "        # Feedforward layer\n",
        "        x = self.ff_layer(class_token_output)\n",
        "\n",
        "        # Classifier head\n",
        "        output = self.classifier(x)  # Output shape: (batch_size, num_classes)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3tGq9kmWMbu",
        "outputId": "0f728429-09ec-4026-c7af-88137f8e9419"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=======================================================================================================================================\n",
              "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
              "=======================================================================================================================================\n",
              "TimeSeriesTransformer (TimeSeriesTransformer)           [64, 30, 16]         [64, 4]              --                   True\n",
              "├─TimeSeriesPatchEmbeddingLayer (patch_embedding)       [64, 30, 16]         [64, 5, 32]          32                   True\n",
              "│    └─Conv1d (conv_layer)                              [64, 16, 32]         [64, 32, 4]          4,128                True\n",
              "│    └─PositionalEncoding (position_embeddings)         [64, 5, 32]          [64, 5, 32]          --                   --\n",
              "│    │    └─Dropout (dropout)                           [64, 5, 32]          [64, 5, 32]          --                   --\n",
              "├─TransformerEncoder (transformer_encoder)              [64, 5, 32]          [64, 5, 32]          --                   True\n",
              "│    └─ModuleList (layers)                              --                   --                   --                   True\n",
              "│    │    └─TransformerEncoderLayer (0)                 [64, 5, 32]          [64, 5, 32]          12,704               True\n",
              "│    │    └─TransformerEncoderLayer (1)                 [64, 5, 32]          [64, 5, 32]          12,704               True\n",
              "│    │    └─TransformerEncoderLayer (2)                 [64, 5, 32]          [64, 5, 32]          12,704               True\n",
              "│    │    └─TransformerEncoderLayer (3)                 [64, 5, 32]          [64, 5, 32]          12,704               True\n",
              "├─Linear (ff_layer)                                     [64, 32]             [64, 128]            4,224                True\n",
              "├─Linear (classifier)                                   [64, 128]            [64, 4]              516                  True\n",
              "=======================================================================================================================================\n",
              "Total params: 59,716\n",
              "Trainable params: 59,716\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 3.53\n",
              "=======================================================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 2.43\n",
              "Params size (MB): 0.17\n",
              "Estimated Total Size (MB): 2.72\n",
              "======================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler to reduce the learning rate by the specified step size and factor (gamma) every step_size epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "summary(\n",
        "    model=model,\n",
        "    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smirk3qsWMbu",
        "outputId": "be11cb16-1d10-446e-bbc7-39212cf6dde3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: New best model saved with validation accuracy: 0.8438\n",
            "Epoch 1, Train Loss: 1.2626, Train Acc: 0.4896, Val Loss: 0.7342, Val Acc: 0.8438\n",
            "Epoch 2, Train Loss: 0.6194, Train Acc: 0.8698, Val Loss: 0.5179, Val Acc: 0.8438\n",
            "Epoch 3: New best model saved with validation accuracy: 0.8594\n",
            "Epoch 3, Train Loss: 0.4048, Train Acc: 0.8594, Val Loss: 0.4058, Val Acc: 0.8594\n",
            "Epoch 4, Train Loss: 0.2947, Train Acc: 0.9010, Val Loss: 0.3281, Val Acc: 0.8594\n",
            "Epoch 5, Train Loss: 0.2437, Train Acc: 0.9062, Val Loss: 0.2960, Val Acc: 0.8438\n",
            "Epoch 6, Train Loss: 0.2005, Train Acc: 0.9010, Val Loss: 0.2713, Val Acc: 0.8438\n",
            "Epoch 7, Train Loss: 0.1747, Train Acc: 0.9427, Val Loss: 0.2548, Val Acc: 0.8438\n",
            "Epoch 8: New best model saved with validation accuracy: 0.8750\n",
            "Epoch 8, Train Loss: 0.1598, Train Acc: 0.9375, Val Loss: 0.2413, Val Acc: 0.8750\n",
            "Epoch 9: New best model saved with validation accuracy: 0.8906\n",
            "Epoch 9, Train Loss: 0.1536, Train Acc: 0.9167, Val Loss: 0.2240, Val Acc: 0.8906\n",
            "Epoch 10: New best model saved with validation accuracy: 0.9062\n",
            "Epoch 10, Train Loss: 0.1400, Train Acc: 0.9219, Val Loss: 0.1809, Val Acc: 0.9062\n",
            "Epoch 11: New best model saved with validation accuracy: 0.9219\n",
            "Epoch 11, Train Loss: 0.1503, Train Acc: 0.9271, Val Loss: 0.1482, Val Acc: 0.9219\n",
            "Epoch 12: New best model saved with validation accuracy: 0.9375\n",
            "Epoch 12, Train Loss: 0.1371, Train Acc: 0.9427, Val Loss: 0.1293, Val Acc: 0.9375\n",
            "Epoch 13: New best model saved with validation accuracy: 0.9531\n",
            "Epoch 13, Train Loss: 0.1280, Train Acc: 0.9167, Val Loss: 0.1139, Val Acc: 0.9531\n",
            "Epoch 14, Train Loss: 0.1188, Train Acc: 0.9375, Val Loss: 0.1043, Val Acc: 0.9375\n",
            "Epoch 15, Train Loss: 0.1148, Train Acc: 0.9375, Val Loss: 0.1021, Val Acc: 0.9375\n",
            "Epoch 16, Train Loss: 0.0995, Train Acc: 0.9479, Val Loss: 0.1050, Val Acc: 0.9375\n",
            "Epoch 17, Train Loss: 0.1073, Train Acc: 0.9531, Val Loss: 0.0973, Val Acc: 0.9375\n",
            "Epoch 18, Train Loss: 0.1052, Train Acc: 0.9479, Val Loss: 0.0938, Val Acc: 0.9375\n",
            "Epoch 19, Train Loss: 0.1074, Train Acc: 0.9375, Val Loss: 0.0899, Val Acc: 0.9375\n",
            "Epoch 20: New best model saved with validation accuracy: 0.9688\n",
            "Epoch 20, Train Loss: 0.0910, Train Acc: 0.9635, Val Loss: 0.0837, Val Acc: 0.9688\n",
            "Loaded best model for testing or further use.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-9c1bc2205eaa>:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        }
      ],
      "source": [
        "# Model, loss function, and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Number of epochs\n",
        "n_epochs = 20\n",
        "\n",
        "# Initialize variables for tracking the best model\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    train_losses = []\n",
        "    train_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Training loop\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "        predictions = model(inputs)  # Forward pass\n",
        "        loss = criterion(predictions, labels)  # Calculate loss\n",
        "\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Count the number of correct predictions\n",
        "        train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = np.mean(train_losses)\n",
        "    train_acc = train_correct / total\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        validation_losses = []\n",
        "        validation_correct = 0\n",
        "        total_val = 0\n",
        "\n",
        "        for inputs, labels in valid_loader:\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            validation_losses.append(loss.item())\n",
        "\n",
        "            validation_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "        validation_loss = np.mean(validation_losses)\n",
        "        validation_acc = validation_correct / total_val\n",
        "\n",
        "    # Check if this is the best model so far\n",
        "    if validation_acc > best_validation_acc:\n",
        "        best_validation_acc = validation_acc\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {validation_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {validation_loss:.4f}, Val Acc: {validation_acc:.4f}')\n",
        "\n",
        "# Loading the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyotm-hrWMbv",
        "outputId": "5a41e533-0887-4bdd-90e4-6a1c07522f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99        51\n",
            "           1       0.75      1.00      0.86         3\n",
            "           2       0.57      0.57      0.57         7\n",
            "           3       0.57      0.57      0.57         7\n",
            "\n",
            "    accuracy                           0.90        68\n",
            "   macro avg       0.72      0.78      0.75        68\n",
            "weighted avg       0.90      0.90      0.90        68\n",
            "\n",
            "Confusion matrix:\n",
            "[[50  1  0  0]\n",
            " [ 0  3  0  0]\n",
            " [ 0  0  4  3]\n",
            " [ 0  0  3  4]]\n"
          ]
        }
      ],
      "source": [
        "# Prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    Y_pred_prob = model(X_test)\n",
        "\n",
        "Y_pred = Y_pred_prob.argmax(1)\n",
        "\n",
        "print(classification_report(y_test, Y_pred))\n",
        "confusion = confusion_matrix(y_test, Y_pred)\n",
        "print(f\"Confusion matrix:\\n{confusion}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "LhQeXQWnWMbv",
        "outputId": "5b8cb2c6-0ab5-4831-934a-69e41172bc6e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Visualize the confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Prediction\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encodings"
      ],
      "metadata": {
        "id": "PHiMwsYvVAzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rwyM_sG-KTN",
        "outputId": "11fbe508-9200-47ee-fe51-38bb0b93daa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "import pandas as pd\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Scl(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.relative_bias_table = nn.Parameter(torch.zeros((2 * self.seq_len - 1), num_heads))\n",
        "        coords = torch.meshgrid((torch.arange(1), torch.arange(self.seq_len)))\n",
        "        coords = torch.flatten(torch.stack(coords), 1)\n",
        "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
        "        relative_coords[1] += self.seq_len - 1\n",
        "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
        "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
        "        self.register_buffer(\"relative_index\", relative_index)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, 8))\n",
        "        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=1 * self.seq_len, w=1 * self.seq_len)\n",
        "        attn = attn + relative_bias\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Vec(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.Er = nn.Parameter(torch.randn(self.seq_len, int(emb_size / num_heads)))\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.tril(torch.ones(self.seq_len, self.seq_len))\n",
        "                .unsqueeze(0).unsqueeze(0)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        QEr = torch.matmul(q, self.Er.transpose(0, 1))\n",
        "        Srel = self.skew(QEr)\n",
        "\n",
        "        attn = torch.matmul(q, k)\n",
        "        attn = (attn + Srel) * self.scale\n",
        "\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "    def skew(self, QEr):\n",
        "        padded = nn.functional.pad(QEr, (1, 0))\n",
        "        batch_size, num_heads, num_rows, num_cols = padded.shape\n",
        "        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
        "        Srel = reshaped[:, :, 1:, :]\n",
        "        return Srel\n"
      ],
      "metadata": {
        "id": "yD7K5F3D-Hji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TUPEConfig:\n",
        "    num_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    d_model: int = 128\n",
        "    d_head: int = 0\n",
        "    max_len: int = 256\n",
        "    dropout: float = 0.1\n",
        "    expansion_factor: int = 1\n",
        "    relative_bias: bool = True\n",
        "    bidirectional_bias: bool = True\n",
        "    num_buckets: int = 32\n",
        "    max_distance: int = 128\n",
        "\n",
        "    def __post_init__(self):\n",
        "        d_head, remainder = divmod(self.d_model, self.num_heads)\n",
        "        assert remainder == 0, \"`d_model` should be divisible by `num_heads`\"\n",
        "        self.d_head = d_head"
      ],
      "metadata": {
        "id": "B2e6n_Yj_38p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_relative_position_bucket(\n",
        "    relative_position, bidirectional, num_buckets, max_distance\n",
        "):\n",
        "    \"\"\"\n",
        "    from https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/modeling_t5.py\n",
        "    \"\"\"\n",
        "    relative_buckets = 0\n",
        "    if bidirectional:\n",
        "        num_buckets //= 2\n",
        "        relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n",
        "        relative_position = torch.abs(relative_position)\n",
        "    else:\n",
        "        relative_position = -torch.min(\n",
        "            relative_position, torch.zeros_like(relative_position)\n",
        "        )\n",
        "    # now relative_position is in the range [0, inf)\n",
        "\n",
        "    # half of the buckets are for exact increments in positions\n",
        "    max_exact = num_buckets // 2\n",
        "    is_small = relative_position < max_exact\n",
        "\n",
        "    # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n",
        "    relative_postion_if_large = max_exact + (\n",
        "        torch.log(relative_position.float() / max_exact)\n",
        "        / math.log(max_distance / max_exact)\n",
        "        * (num_buckets - max_exact)\n",
        "    ).to(torch.long)\n",
        "    relative_postion_if_large = torch.min(\n",
        "        relative_postion_if_large,\n",
        "        torch.full_like(relative_postion_if_large, num_buckets - 1),\n",
        "    )\n",
        "\n",
        "    relative_buckets += torch.where(\n",
        "        is_small, relative_position, relative_postion_if_large\n",
        "    )\n",
        "    return relative_buckets\n",
        "\n",
        "\n",
        "def get_relative_positions(\n",
        "    seq_len, bidirectional=True, num_buckets=32, max_distance=128\n",
        "):\n",
        "    x = torch.arange(seq_len)[None, :]\n",
        "    y = torch.arange(seq_len)[:, None]\n",
        "    relative_positions = _get_relative_position_bucket(\n",
        "        x - y, bidirectional, num_buckets, max_distance\n",
        "    )\n",
        "    return relative_positions"
      ],
      "metadata": {
        "id": "1QbW7ihvBByl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TUPEMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config: TUPEConfig, pos_embed: nn.Module) -> None:\n",
        "        super().__init__()\n",
        "        self.max_len = config.max_len\n",
        "        self.num_heads = config.num_heads\n",
        "        self.num_buckets = config.num_buckets\n",
        "        self.max_distance = config.max_distance\n",
        "        self.bidirectional = config.bidirectional_bias\n",
        "        self.scale = math.sqrt(2 * config.d_head)\n",
        "\n",
        "        self.pos_embed = pos_embed\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # kqv in one pass\n",
        "        self.pos_kq = nn.Linear(config.d_model, 2 * config.d_model, bias=False)\n",
        "        self.tok_kqv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n",
        "\n",
        "        self.relative_bias = config.relative_bias\n",
        "        if config.relative_bias:\n",
        "            self.bias = nn.Embedding(config.max_len * 2, config.num_heads)\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        pos_embed = self.pos_embed(seq_len).repeat(batch_size, 1, 1)\n",
        "        # pos_embed.shape == (batch_size, seq_len, d_model)\n",
        "        pos_key, pos_query = self.pos_kq(pos_embed).chunk(2, dim=-1)\n",
        "        pos_key = pos_key.view(batch_size, seq_len, self.num_heads, -1).permute(\n",
        "            0, 2, 3, 1\n",
        "        )\n",
        "        # pos_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        pos_query = pos_query.view(batch_size, seq_len, self.num_heads, -1).transpose(\n",
        "            1, 2\n",
        "        )\n",
        "        # pos_query.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        pos_attn = torch.matmul(pos_query, pos_key)\n",
        "        # pos_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        tok_key, tok_query, tok_value = self.tok_kqv(x).chunk(3, dim=-1)\n",
        "        tok_key = tok_key.view(batch_size, seq_len, self.num_heads, -1).permute(\n",
        "            0, 2, 3, 1\n",
        "        )\n",
        "        # tok_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        tok_query = tok_query.view(batch_size, seq_len, self.num_heads, -1).transpose(\n",
        "            1, 2\n",
        "        )\n",
        "        tok_value = tok_value.view(batch_size, seq_len, self.num_heads, -1).transpose(\n",
        "            1, 2\n",
        "        )\n",
        "        # tok_qv.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        tok_attn = torch.matmul(tok_query, tok_key)\n",
        "        # tok_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        attn = (tok_attn + pos_attn) / self.scale\n",
        "        if self.relative_bias:\n",
        "            relative_positions = get_relative_positions(\n",
        "                seq_len, self.bidirectional, self.num_buckets, self.max_distance\n",
        "            ).to(attn.device)\n",
        "            # relative_positions.shape == (seq_len, seq_len)\n",
        "            bias = self.bias(relative_positions + self.max_len)\n",
        "            # bias.shape == (seq_len, seq_len, num_heads)\n",
        "            bias = bias.permute(2, 0, 1).unsqueeze(0)\n",
        "            # bias.shape == (1, num_heads, seq_len, seq_len)\n",
        "            attn = attn + bias\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "        out = torch.matmul(attn, tok_value)\n",
        "        # out.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
        "        # out.shape == (batch_size, seq_len, d_model)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "9uW0SNXu_oda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import math\n",
        "from torchsummary import summary\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def pos_encoding(self, q_len, d_model, normalize=True):\n",
        "        pe = torch.zeros(q_len, d_model)\n",
        "        position = torch.arange(0, q_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        if normalize:\n",
        "            pe = pe - pe.mean()\n",
        "            pe = pe / (pe.std() * 10)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_encoding(q_len=x.size(1), d_model=x.size(2))\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Learned Positional Encoding\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(LearnedPositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Time Series Patch Embedding Layer\n",
        "class TimeSeriesPatchEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps, pos_encoding='fixed'):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "        self.padding = (self.num_patches * patch_size) - input_timesteps\n",
        "\n",
        "        self.conv_layer = nn.Conv1d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embedding_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "        )\n",
        "\n",
        "        self.class_token_embeddings = nn.Parameter(\n",
        "            torch.randn((1, 1, embedding_dim), requires_grad=True)\n",
        "        )\n",
        "\n",
        "        # Instantiate the positional encoding\n",
        "        pos_encoder_class = get_pos_encoder(pos_encoding)\n",
        "        self.position_embeddings = pos_encoder_class(embedding_dim, dropout=0.1, max_len=input_timesteps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.padding > 0:\n",
        "            x = nn.functional.pad(x, (0, 0, 0, self.padding))\n",
        "\n",
        "        x = x.permute(0, 2, 1)\n",
        "        conv_output = self.conv_layer(x)\n",
        "        conv_output = conv_output.permute(0, 2, 1)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n",
        "        output = torch.cat((class_tokens, conv_output), dim=1)\n",
        "\n",
        "        output = self.position_embeddings(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Get Positional Encoding\n",
        "def get_pos_encoder(pos_encoding):\n",
        "    if pos_encoding == 'fixed':\n",
        "        return PositionalEncoding\n",
        "    elif pos_encoding == 'learned':\n",
        "        return LearnedPositionalEncoding\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, pos_encoding='fixed', num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2, attention_type='absolute'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps, pos_encoding)\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "\n",
        "        if attention_type == 'relative_scl':\n",
        "            self.attention_layer = Attention_Rel_Scl(embedding_dim, num_heads, self.num_patches + 1, dropout)\n",
        "        elif attention_type == 'relative_vec':\n",
        "            self.attention_layer = Attention_Rel_Vec(embedding_dim, num_heads, self.num_patches + 1, dropout)\n",
        "        elif attention_type == 'tupe':\n",
        "            self.attention_layer = TUPEMultiHeadAttention(tupe_config, pos_embed)\n",
        "        else:\n",
        "            self.attention_layer = Attention(embedding_dim, num_heads, dropout)\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n",
        "\n",
        "        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n",
        "        self.classifier = nn.Linear(dim_feedforward, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        x = self.attention_layer(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        class_token_output = x[:, 0, :]\n",
        "        x = self.ff_layer(class_token_output)\n",
        "        output = self.classifier(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "random_instances, random_labels = next(iter(train_loader))\n",
        "random_instance = random_instances[0]\n",
        "\n",
        "BATCH_SIZE = random_instances.shape[0]\n",
        "TIMESTEPS = random_instance.shape[0]\n",
        "CHANNELS = random_instance.shape[1]\n",
        "PATCH_SIZE = 8\n",
        "\n",
        "patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    embedding_dim=CHANNELS * PATCH_SIZE,\n",
        "    input_timesteps=TIMESTEPS,\n",
        ")\n",
        "\n",
        "patch_embeddings = patch_embedding_layer(random_instances)\n",
        "patch_embeddings.shape\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    pos_encoding='fixed',\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "    val_loss = total_loss / len(valid_loader)\n",
        "    val_acc = correct / total_val\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n",
        "    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n",
        "\n",
        "    if val_acc > best_validation_acc:\n",
        "        best_validation_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeNs_BKxfx0i",
        "outputId": "92649eaa-637c-41d3-bc09-60636e9d9a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1.4569, Train Acc: 0.2266\n",
            "Epoch 1: New best model saved with validation accuracy: 0.2656\n",
            "Epoch 1, Val Loss: 1.3812, Val Acc: 0.2656\n",
            "Epoch 1, Train Loss: 1.3724, Train Acc: 0.3125\n",
            "Epoch 2: New best model saved with validation accuracy: 0.3750\n",
            "Epoch 2, Val Loss: 1.3303, Val Acc: 0.3750\n",
            "Epoch 1, Train Loss: 1.3081, Train Acc: 0.3750\n",
            "Epoch 3: New best model saved with validation accuracy: 0.4688\n",
            "Epoch 3, Val Loss: 1.2838, Val Acc: 0.4688\n",
            "Epoch 1, Train Loss: 1.2549, Train Acc: 0.4766\n",
            "Epoch 4, Val Loss: 1.2554, Val Acc: 0.4531\n",
            "Epoch 1, Train Loss: 1.1858, Train Acc: 0.4922\n",
            "Epoch 5: New best model saved with validation accuracy: 0.5000\n",
            "Epoch 5, Val Loss: 1.2337, Val Acc: 0.5000\n",
            "Epoch 1, Train Loss: 1.1196, Train Acc: 0.5703\n",
            "Epoch 6, Val Loss: 1.1852, Val Acc: 0.4844\n",
            "Epoch 1, Train Loss: 1.0403, Train Acc: 0.6250\n",
            "Epoch 7, Val Loss: 1.1052, Val Acc: 0.4844\n",
            "Epoch 1, Train Loss: 1.0115, Train Acc: 0.5703\n",
            "Epoch 8: New best model saved with validation accuracy: 0.5156\n",
            "Epoch 8, Val Loss: 1.0460, Val Acc: 0.5156\n",
            "Epoch 1, Train Loss: 0.9153, Train Acc: 0.6875\n",
            "Epoch 9: New best model saved with validation accuracy: 0.5781\n",
            "Epoch 9, Val Loss: 1.0105, Val Acc: 0.5781\n",
            "Epoch 1, Train Loss: 0.8802, Train Acc: 0.6484\n",
            "Epoch 10, Val Loss: 0.9813, Val Acc: 0.5781\n",
            "Loaded best model for testing or further use.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-b06036bd6b7c>:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "import pandas as pd\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Scl(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.relative_bias_table = nn.Parameter(torch.zeros((2 * self.seq_len - 1), num_heads))\n",
        "        coords = torch.meshgrid((torch.arange(1), torch.arange(self.seq_len)))\n",
        "        coords = torch.flatten(torch.stack(coords), 1)\n",
        "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
        "        relative_coords[1] += self.seq_len - 1\n",
        "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
        "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
        "        self.register_buffer(\"relative_index\", relative_index)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, 8))\n",
        "        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=1 * self.seq_len, w=1 * self.seq_len)\n",
        "        attn = attn + relative_bias\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Vec(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.Er = nn.Parameter(torch.randn(self.seq_len, int(emb_size / num_heads)))\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.tril(torch.ones(self.seq_len, self.seq_len))\n",
        "                .unsqueeze(0).unsqueeze(0)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        QEr = torch.matmul(q, self.Er.transpose(0, 1))\n",
        "        Srel = self.skew(QEr)\n",
        "\n",
        "        attn = torch.matmul(q, k)\n",
        "        attn = (attn + Srel) * self.scale\n",
        "\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "    def skew(self, QEr):\n",
        "        padded = nn.functional.pad(QEr, (1, 0))\n",
        "        batch_size, num_heads, num_rows, num_cols = padded.shape\n",
        "        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
        "        Srel = reshaped[:, :, 1:, :]\n",
        "        return Srel\n"
      ],
      "metadata": {
        "id": "ESOXdCAqaM_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class tAPE(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n",
        "        super(tAPE, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin((position * div_term) * (d_model / max_len))\n",
        "        pe[:, 1::2] = torch.cos((position * div_term) * (d_model / max_len))\n",
        "        pe = scale_factor * pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class AbsolutePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n",
        "        super(AbsolutePositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = scale_factor * pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024):\n",
        "        super(LearnablePositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pe = nn.Parameter(torch.empty(max_len, d_model))\n",
        "        nn.init.uniform_(self.pe, -0.02, 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(1), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "dplhsyvE-jR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_encoder(pos_encoding):\n",
        "    if pos_encoding == 'fixed':\n",
        "        return PositionalEncoding\n",
        "    elif pos_encoding == 'learned':\n",
        "        return LearnedPositionalEncoding\n",
        "    elif pos_encoding == 'tAPE':\n",
        "        return tAPE\n",
        "    elif pos_encoding == 'absolute':\n",
        "        return AbsolutePositionalEncoding\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n"
      ],
      "metadata": {
        "id": "q9sb14F6-kc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    pos_encoding='tAPE',\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "    val_loss = total_loss / len(valid_loader)\n",
        "    val_acc = correct / total_val\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 20\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n",
        "    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n",
        "\n",
        "    if val_acc > best_validation_acc:\n",
        "        best_validation_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egoVdYVX-mq4",
        "outputId": "153161d4-30bf-4814-d416-16b129503be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1.4311, Train Acc: 0.2266\n",
            "Epoch 1: New best model saved with validation accuracy: 0.3281\n",
            "Epoch 1, Val Loss: 1.3790, Val Acc: 0.3281\n",
            "Epoch 1, Train Loss: 1.3741, Train Acc: 0.2891\n",
            "Epoch 2: New best model saved with validation accuracy: 0.3438\n",
            "Epoch 2, Val Loss: 1.3855, Val Acc: 0.3438\n",
            "Epoch 1, Train Loss: 1.3881, Train Acc: 0.2812\n",
            "Epoch 3, Val Loss: 1.3670, Val Acc: 0.3281\n",
            "Epoch 1, Train Loss: 1.3506, Train Acc: 0.2969\n",
            "Epoch 4, Val Loss: 1.3426, Val Acc: 0.2969\n",
            "Epoch 1, Train Loss: 1.3434, Train Acc: 0.3125\n",
            "Epoch 5: New best model saved with validation accuracy: 0.5312\n",
            "Epoch 5, Val Loss: 1.3242, Val Acc: 0.5312\n",
            "Epoch 1, Train Loss: 1.2763, Train Acc: 0.4688\n",
            "Epoch 6, Val Loss: 1.3288, Val Acc: 0.3125\n",
            "Epoch 1, Train Loss: 1.2578, Train Acc: 0.3984\n",
            "Epoch 7, Val Loss: 1.2999, Val Acc: 0.3438\n",
            "Epoch 1, Train Loss: 1.2230, Train Acc: 0.4219\n",
            "Epoch 8, Val Loss: 1.2241, Val Acc: 0.4375\n",
            "Epoch 1, Train Loss: 1.1781, Train Acc: 0.5078\n",
            "Epoch 9: New best model saved with validation accuracy: 0.5625\n",
            "Epoch 9, Val Loss: 1.1763, Val Acc: 0.5625\n",
            "Epoch 1, Train Loss: 1.1084, Train Acc: 0.5000\n",
            "Epoch 10, Val Loss: 1.1406, Val Acc: 0.5625\n",
            "Epoch 1, Train Loss: 1.0463, Train Acc: 0.5859\n",
            "Epoch 11: New best model saved with validation accuracy: 0.5781\n",
            "Epoch 11, Val Loss: 1.1462, Val Acc: 0.5781\n",
            "Epoch 1, Train Loss: 1.0315, Train Acc: 0.5938\n",
            "Epoch 12, Val Loss: 1.0759, Val Acc: 0.5781\n",
            "Epoch 1, Train Loss: 0.9647, Train Acc: 0.5469\n",
            "Epoch 13: New best model saved with validation accuracy: 0.5938\n",
            "Epoch 13, Val Loss: 0.9922, Val Acc: 0.5938\n",
            "Epoch 1, Train Loss: 0.9757, Train Acc: 0.5781\n",
            "Epoch 14: New best model saved with validation accuracy: 0.6406\n",
            "Epoch 14, Val Loss: 0.9370, Val Acc: 0.6406\n",
            "Epoch 1, Train Loss: 0.9089, Train Acc: 0.6406\n",
            "Epoch 15, Val Loss: 0.9249, Val Acc: 0.6406\n",
            "Epoch 1, Train Loss: 0.9021, Train Acc: 0.6562\n",
            "Epoch 16, Val Loss: 0.9153, Val Acc: 0.6250\n",
            "Epoch 1, Train Loss: 0.8903, Train Acc: 0.6172\n",
            "Epoch 17, Val Loss: 0.8661, Val Acc: 0.6250\n",
            "Epoch 1, Train Loss: 0.8292, Train Acc: 0.6328\n",
            "Epoch 18, Val Loss: 0.8550, Val Acc: 0.5938\n",
            "Epoch 1, Train Loss: 0.8129, Train Acc: 0.6172\n",
            "Epoch 19, Val Loss: 0.8455, Val Acc: 0.6250\n",
            "Epoch 1, Train Loss: 0.7899, Train Acc: 0.6797\n",
            "Epoch 20, Val Loss: 0.7958, Val Acc: 0.6250\n",
            "Loaded best model for testing or further use.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-f4ea5e1fc41b>:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    pos_encoding='fixed',\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        "    attention_type= 'relative_vec',\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "    val_loss = total_loss / len(valid_loader)\n",
        "    val_acc = correct / total_val\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 20\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n",
        "    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n",
        "\n",
        "    if val_acc > best_validation_acc:\n",
        "        best_validation_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzFfH3xX8fE_",
        "outputId": "1664db50-327c-4398-b12b-42ac5221b14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1.4358, Train Acc: 0.2422\n",
            "Epoch 1: New best model saved with validation accuracy: 0.4062\n",
            "Epoch 1, Val Loss: 1.3170, Val Acc: 0.4062\n",
            "Epoch 1, Train Loss: 1.3419, Train Acc: 0.3359\n",
            "Epoch 2: New best model saved with validation accuracy: 0.4844\n",
            "Epoch 2, Val Loss: 1.2702, Val Acc: 0.4844\n",
            "Epoch 1, Train Loss: 1.2761, Train Acc: 0.3984\n",
            "Epoch 3, Val Loss: 1.1982, Val Acc: 0.4688\n",
            "Epoch 1, Train Loss: 1.2040, Train Acc: 0.4609\n",
            "Epoch 4: New best model saved with validation accuracy: 0.5625\n",
            "Epoch 4, Val Loss: 1.1222, Val Acc: 0.5625\n",
            "Epoch 1, Train Loss: 1.1163, Train Acc: 0.5156\n",
            "Epoch 5: New best model saved with validation accuracy: 0.6406\n",
            "Epoch 5, Val Loss: 1.0442, Val Acc: 0.6406\n",
            "Epoch 1, Train Loss: 1.0287, Train Acc: 0.5391\n",
            "Epoch 6, Val Loss: 0.9654, Val Acc: 0.5781\n",
            "Epoch 1, Train Loss: 0.9441, Train Acc: 0.5938\n",
            "Epoch 7, Val Loss: 0.9070, Val Acc: 0.5781\n",
            "Epoch 1, Train Loss: 0.9253, Train Acc: 0.6328\n",
            "Epoch 8, Val Loss: 0.8725, Val Acc: 0.6094\n",
            "Epoch 1, Train Loss: 0.8254, Train Acc: 0.6641\n",
            "Epoch 9, Val Loss: 0.8457, Val Acc: 0.5625\n",
            "Epoch 1, Train Loss: 0.8502, Train Acc: 0.6406\n",
            "Epoch 10, Val Loss: 0.8050, Val Acc: 0.5781\n",
            "Epoch 1, Train Loss: 0.7219, Train Acc: 0.6641\n",
            "Epoch 11, Val Loss: 0.7731, Val Acc: 0.6406\n",
            "Epoch 1, Train Loss: 0.7538, Train Acc: 0.6562\n",
            "Epoch 12: New best model saved with validation accuracy: 0.7031\n",
            "Epoch 12, Val Loss: 0.7465, Val Acc: 0.7031\n",
            "Epoch 1, Train Loss: 0.7807, Train Acc: 0.6719\n",
            "Epoch 13, Val Loss: 0.7470, Val Acc: 0.6562\n",
            "Epoch 1, Train Loss: 0.6735, Train Acc: 0.7266\n",
            "Epoch 14, Val Loss: 0.7429, Val Acc: 0.6406\n",
            "Epoch 1, Train Loss: 0.6290, Train Acc: 0.7500\n",
            "Epoch 15, Val Loss: 0.7430, Val Acc: 0.6719\n",
            "Epoch 1, Train Loss: 0.6027, Train Acc: 0.7500\n",
            "Epoch 16, Val Loss: 0.7474, Val Acc: 0.6875\n",
            "Epoch 1, Train Loss: 0.6253, Train Acc: 0.7422\n",
            "Epoch 17, Val Loss: 0.7569, Val Acc: 0.7031\n",
            "Epoch 1, Train Loss: 0.5292, Train Acc: 0.8047\n",
            "Epoch 18, Val Loss: 0.7459, Val Acc: 0.7031\n",
            "Epoch 1, Train Loss: 0.6126, Train Acc: 0.7188\n",
            "Epoch 19: New best model saved with validation accuracy: 0.7188\n",
            "Epoch 19, Val Loss: 0.7281, Val Acc: 0.7188\n",
            "Epoch 1, Train Loss: 0.4622, Train Acc: 0.8203\n",
            "Epoch 20, Val Loss: 0.7186, Val Acc: 0.7031\n",
            "Loaded best model for testing or further use.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-014626430eba>:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TUPE"
      ],
      "metadata": {
        "id": "ntF6fok4KzMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class TUPEConfig:\n",
        "    def __init__(self):\n",
        "        self.d_model = 128\n",
        "        self.num_heads = 8\n",
        "        self.dim_feedforward = 512\n",
        "        self.dropout = 0.1\n",
        "        self.max_len = 5000\n",
        "        self.num_buckets = 32\n",
        "        self.max_distance = 128\n",
        "        self.relative_bias = True\n",
        "        self.bidirectional_bias = True\n",
        "\n",
        "class TUPEMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config: TUPEConfig, pos_embed: nn.Module) -> None:\n",
        "        super().__init__()\n",
        "        self.max_len = config.max_len\n",
        "        self.num_heads = config.num_heads\n",
        "        self.num_buckets = config.num_buckets\n",
        "        self.max_distance = config.max_distance\n",
        "        self.bidirectional = config.bidirectional_bias\n",
        "        self.scale = math.sqrt(2 * config.d_model // config.num_heads)\n",
        "\n",
        "        self.pos_embed = pos_embed\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # kqv in one pass\n",
        "        self.pos_kq = nn.Linear(config.d_model, 2 * config.d_model, bias=False)\n",
        "        self.tok_kqv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n",
        "\n",
        "        self.relative_bias = config.relative_bias\n",
        "        if config.relative_bias:\n",
        "            self.bias = nn.Embedding(config.max_len * 2, config.num_heads)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        pos_embed = self.pos_embed(seq_len).repeat(batch_size, 1, 1)\n",
        "        # pos_embed.shape == (batch_size, seq_len, d_model)\n",
        "        pos_key, pos_query = self.pos_kq(pos_embed).chunk(2, dim=-1)\n",
        "        pos_key = pos_key.view(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        # pos_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        pos_query = pos_query.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        # pos_query.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        pos_attn = torch.matmul(pos_query, pos_key)\n",
        "        # pos_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        tok_key, tok_query, tok_value = self.tok_kqv(x).chunk(3, dim=-1)\n",
        "        tok_key = tok_key.view(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        # tok_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        tok_query = tok_query.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        tok_value = tok_value.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        # tok_qv.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        tok_attn = torch.matmul(tok_query, tok_key)\n",
        "        # tok_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        attn = (tok_attn + pos_attn) / self.scale\n",
        "        if self.relative_bias:\n",
        "            relative_positions = self.get_relative_positions(seq_len)\n",
        "            # relative_positions.shape == (seq_len, seq_len)\n",
        "            bias = self.bias(relative_positions + self.max_len)\n",
        "            # bias.shape == (seq_len, seq_len, num_heads)\n",
        "            bias = bias.permute(2, 0, 1).unsqueeze(0)\n",
        "            # bias.shape == (1, num_heads, seq_len, seq_len)\n",
        "            attn = attn + bias\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "        out = torch.matmul(attn, tok_value)\n",
        "        # out.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
        "        # out.shape == (batch_size, seq_len, d_model)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "    def get_relative_positions(self, seq_len):\n",
        "        # Generate relative position encodings\n",
        "        range_vec = torch.arange(seq_len)\n",
        "        distance_mat = range_vec.unsqueeze(1) - range_vec.unsqueeze(0)\n",
        "        distance_mat_clipped = torch.clamp(distance_mat, -self.max_distance, self.max_distance)\n",
        "        final_mat = distance_mat_clipped + self.max_distance\n",
        "        return final_mat\n"
      ],
      "metadata": {
        "id": "LWu8jg32ORdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(FixedPositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)  # (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        return self.encoding[:, :seq_len, :].detach()\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_layers, num_heads, dim_feedforward, dropout, num_classes, attention_type, pos_encoding):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.attention_type = attention_type\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.pos_encoding = pos_encoding\n",
        "\n",
        "        # Transformer layers with TUPEMultiHeadAttention\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TUPEMultiHeadAttention(\n",
        "                config=TUPEConfig(),\n",
        "                pos_embed=FixedPositionalEncoding(embedding_dim, max_len=input_timesteps)\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(f\"Input to model: {x.shape}\")\n",
        "\n",
        "        # Patch embedding\n",
        "        x = x.permute(0, 2, 1)  # Change shape from (batch_size, seq_len, in_channels) to (batch_size, in_channels, seq_len)\n",
        "        #print(f\"Input to Conv1d: {x.shape}\")\n",
        "\n",
        "        x = self.patch_embedding(x)  # Apply Conv1d\n",
        "        #print(f\"Patch embedding output: {x.shape}\")\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, embedding_dim)\n",
        "        #print(f\"Patch embedding output permuted: {x.shape}\")\n",
        "\n",
        "        # Positional encoding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pos_encoding(seq_len).to(x.device)\n",
        "        #print(f\"Positional encoding output: {x.shape}\")\n",
        "\n",
        "        # Transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "            #print(f\"Transformer layer output: {x.shape}\")\n",
        "\n",
        "        # Classification head\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.classifier(x)\n",
        "        #print(f\"Classifier output: {x.shape}\")\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "TIMESTEPS = 30\n",
        "CHANNELS = 6\n",
        "n_classes = 44\n",
        "\n",
        "# Initialize positional encoding\n",
        "pos_embed = FixedPositionalEncoding(d_model=128, max_len=TIMESTEPS)\n",
        "\n",
        "# Define the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=128,\n",
        "    num_layers=4,\n",
        "    num_heads=8,\n",
        "    dim_feedforward=512,\n",
        "    dropout=0.1,\n",
        "    num_classes=n_classes,\n",
        "    attention_type='tupe',\n",
        "    pos_encoding=pos_embed,\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training and validation loop\n",
        "num_epochs = 20\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_preds = []\n",
        "    train_labels = []\n",
        "    for batch in train_loader:\n",
        "        inputs, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    valid_preds = []\n",
        "    valid_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            inputs, labels = batch\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item() * inputs.size(0)\n",
        "            valid_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "            valid_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    valid_loss /= len(valid_loader.dataset)\n",
        "    valid_acc = accuracy_score(valid_labels, valid_preds)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
        "          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgP2Ge01OTt0",
        "outputId": "5a4c7414-9752-4535-9ff4-43d8752a1a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 3.1447, Train Acc: 0.1406, Valid Loss: 2.9109, Valid Acc: 0.3594\n",
            "Epoch 2/20, Train Loss: 2.8448, Train Acc: 0.3438, Valid Loss: 2.4550, Valid Acc: 0.3750\n",
            "Epoch 3/20, Train Loss: 2.3032, Train Acc: 0.3281, Valid Loss: 1.7242, Valid Acc: 0.2969\n",
            "Epoch 4/20, Train Loss: 1.5748, Train Acc: 0.3359, Valid Loss: 1.2022, Valid Acc: 0.4219\n",
            "Epoch 5/20, Train Loss: 1.2348, Train Acc: 0.2969, Valid Loss: 1.0896, Valid Acc: 0.4375\n",
            "Epoch 6/20, Train Loss: 1.1160, Train Acc: 0.3750, Valid Loss: 1.1139, Valid Acc: 0.2344\n",
            "Epoch 7/20, Train Loss: 1.1004, Train Acc: 0.3594, Valid Loss: 1.0432, Valid Acc: 0.5781\n",
            "Epoch 8/20, Train Loss: 1.0629, Train Acc: 0.4531, Valid Loss: 0.9556, Valid Acc: 0.5625\n",
            "Epoch 9/20, Train Loss: 0.9763, Train Acc: 0.4922, Valid Loss: 0.9714, Valid Acc: 0.4062\n",
            "Epoch 10/20, Train Loss: 0.9298, Train Acc: 0.4766, Valid Loss: 0.8266, Valid Acc: 0.5312\n",
            "Epoch 11/20, Train Loss: 0.7606, Train Acc: 0.6016, Valid Loss: 0.7787, Valid Acc: 0.6094\n",
            "Epoch 12/20, Train Loss: 0.7598, Train Acc: 0.5703, Valid Loss: 0.7638, Valid Acc: 0.6094\n",
            "Epoch 13/20, Train Loss: 0.6748, Train Acc: 0.6562, Valid Loss: 0.7068, Valid Acc: 0.6094\n",
            "Epoch 14/20, Train Loss: 0.5889, Train Acc: 0.7031, Valid Loss: 0.6520, Valid Acc: 0.6719\n",
            "Epoch 15/20, Train Loss: 0.4999, Train Acc: 0.7578, Valid Loss: 0.6087, Valid Acc: 0.6875\n",
            "Epoch 16/20, Train Loss: 0.4634, Train Acc: 0.7734, Valid Loss: 0.6138, Valid Acc: 0.6875\n",
            "Epoch 17/20, Train Loss: 0.4516, Train Acc: 0.7734, Valid Loss: 0.5685, Valid Acc: 0.6875\n",
            "Epoch 18/20, Train Loss: 0.3962, Train Acc: 0.7891, Valid Loss: 0.5337, Valid Acc: 0.7188\n",
            "Epoch 19/20, Train Loss: 0.3772, Train Acc: 0.8125, Valid Loss: 0.5571, Valid Acc: 0.7031\n",
            "Epoch 20/20, Train Loss: 0.3137, Train Acc: 0.8516, Valid Loss: 0.5213, Valid Acc: 0.7344\n",
            "Best Validation Accuracy: 0.7344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_layers, num_heads, dim_feedforward, dropout, num_classes, pos_encoding_type, spe_params):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Positional encoding\n",
        "        if pos_encoding_type == 'sine':\n",
        "            self.pos_encoding = SineSPE(num_heads=num_heads, in_features=embedding_dim // num_heads, **spe_params)\n",
        "        elif pos_encoding_type == 'conv':\n",
        "            self.pos_encoding = ConvSPE(ndim=1, num_heads=num_heads, in_features=embedding_dim // num_heads, **spe_params)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid positional encoding type\")\n",
        "\n",
        "        # Transformer layers with TUPEMultiHeadAttention\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TUPEMultiHeadAttention(\n",
        "                config=TUPEConfig(),\n",
        "                pos_embed=self.pos_encoding\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch embedding\n",
        "        x = x.permute(0, 2, 1)  # Change shape from (batch_size, seq_len, in_channels) to (batch_size, in_channels, seq_len)\n",
        "        x = self.patch_embedding(x)  # Apply Conv1d\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Positional encoding\n",
        "        seq_len = x.size(1)\n",
        "        pos_enc = self.pos_encoding(seq_len).to(x.device)\n",
        "        x = x + pos_enc\n",
        "\n",
        "        # Transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "pMTQ5_2nyjht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SineSPE(nn.Module):\n",
        "    def __init__(self, num_heads, in_features, max_len=512):\n",
        "        super(SineSPE, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.in_features = in_features\n",
        "        self.max_len = max_len\n",
        "        self.position = nn.Parameter(torch.zeros(1, max_len, in_features))\n",
        "        self.register_buffer('sine', self._generate_sine_encoding())\n",
        "\n",
        "    def _generate_sine_encoding(self):\n",
        "        position = torch.arange(self.max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, self.in_features, 2).float() * -(math.log(10000.0) / self.in_features))\n",
        "        encoding = torch.zeros(self.max_len, self.in_features)\n",
        "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        return encoding\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        return self.sine[:seq_len, :].unsqueeze(0).repeat(self.num_heads, 1, 1)\n",
        "\n",
        "class ConvSPE(nn.Module):\n",
        "    def __init__(self, ndim, num_heads, in_features, kernel_size=7, padding=3):\n",
        "        super(ConvSPE, self).__init__()\n",
        "        self.conv = nn.Conv1d(in_features, in_features, kernel_size=kernel_size, padding=padding)\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        # Generate sinusoidal encoding\n",
        "        x = torch.arange(seq_len).unsqueeze(0).float()\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.conv(x)\n",
        "        x = x.squeeze(0).permute(1, 0).unsqueeze(0).repeat(self.num_heads, 1, 1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_qHLmzj7yl1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import math\n",
        "\n",
        "# Example of positional encoding classes\n",
        "class SineSPE(nn.Module):\n",
        "    def __init__(self, in_features, max_len=512):\n",
        "        super(SineSPE, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.max_len = max_len\n",
        "        self.position = nn.Parameter(torch.zeros(1, max_len, in_features))\n",
        "        self.register_buffer('sine', self._generate_sine_encoding())\n",
        "\n",
        "    def _generate_sine_encoding(self):\n",
        "        position = torch.arange(self.max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, self.in_features, 2).float() * -(math.log(10000.0) / self.in_features))\n",
        "        encoding = torch.zeros(self.max_len, self.in_features)\n",
        "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        return encoding\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        return self.sine[:seq_len, :].unsqueeze(0)  # Shape: (1, seq_len, in_features)\n",
        "\n",
        "\n",
        "class ConvSPE(nn.Module):\n",
        "    def __init__(self, num_heads, in_features, kernel_size=3, num_realizations=1):\n",
        "        super(ConvSPE, self).__init__()\n",
        "        padding = kernel_size // 2  # This ensures that the output size matches the input size if stride=1\n",
        "\n",
        "        # Define a 1D convolutional layer\n",
        "        self.conv = nn.Conv1d(in_features, in_features, kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.in_features = in_features\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_realizations = num_realizations\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x should be of shape (batch_size, seq_len, in_features)\n",
        "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, in_features, seq_len)\n",
        "        x = self.conv(x)  # Apply convolution\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, in_features)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model Definition\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2, pos_encoding_type='sine', spe_params={}):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Positional encoding\n",
        "        if pos_encoding_type == 'sineSPE':\n",
        "            self.pos_encoding = SineSPE(embedding_dim, **spe_params)\n",
        "        elif pos_encoding_type == 'convSPE':\n",
        "            self.pos_encoding = ConvSPE(num_heads=num_heads, in_features=embedding_dim, **spe_params)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid positional encoding type\")\n",
        "\n",
        "        # Calculate the number of patches\n",
        "        self.num_patches = -(-input_timesteps // patch_size)  # Ceiling division\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n",
        "\n",
        "        # Feedforward layer\n",
        "        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n",
        "        # Classifier Head\n",
        "        self.classifier = nn.Linear(dim_feedforward, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, in_channels, input_timesteps)\n",
        "\n",
        "        # Get patch embeddings\n",
        "        x = x.permute(0, 2, 1)  # Change shape from (batch_size, in_channels, input_timesteps) to (batch_size, input_timesteps, in_channels)\n",
        "        x = self.patch_embedding(x)  # Apply Conv1d\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, num_patches, embedding_dim)\n",
        "\n",
        "        # Apply positional encoding\n",
        "        seq_len = x.size(1)\n",
        "        pos_enc = self.pos_encoding(seq_len).to(x.device)\n",
        "        x = x + pos_enc\n",
        "\n",
        "        # Apply Transformer Encoder\n",
        "        x = self.transformer_encoder(x)  # Output shape: (batch_size, num_patches, embedding_dim)\n",
        "\n",
        "        # Use the output corresponding to the class token for classification\n",
        "        class_token_output = x[:, 0, :]  # Select the class token for each item in the batch\n",
        "\n",
        "        # Feedforward layer\n",
        "        x = self.ff_layer(class_token_output)\n",
        "\n",
        "        # Classifier head\n",
        "        output = self.classifier(x)  # Output shape: (batch_size, num_classes)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "3dIJpcJ3ypDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "TIMESTEPS = 30\n",
        "CHANNELS = 6\n",
        "patch_size = 7\n",
        "embedding_dim = 128\n",
        "num_transformer_layers = 6\n",
        "num_heads = 8\n",
        "dim_feedforward = 128\n",
        "dropout = 0.1\n",
        "num_classes = 4\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=patch_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_transformer_layers=num_transformer_layers,\n",
        "    num_heads=num_heads,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    dropout=dropout,\n",
        "    num_classes=num_classes,\n",
        "    pos_encoding_type='sineSPE',\n",
        "    spe_params={'max_len': TIMESTEPS}\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for inputs, labels in data_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss = running_loss / len(data_loader.dataset)\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    valid_loss = running_loss / len(data_loader.dataset)\n",
        "    valid_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return valid_loss, valid_acc\n",
        "\n",
        "# Training and Evaluation Loop\n",
        "num_epochs = 20\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
        "          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if valid_accuracy > best_valid_acc:\n",
        "        best_valid_acc = valid_accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlTvPqd6zvXw",
        "outputId": "48f5dbf9-aa11-4814-8da2-880899d8dbb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.2272, Train Acc: 0.2109, Valid Loss: 1.1188, Valid Acc: 0.4375\n",
            "Epoch 2/20, Train Loss: 1.1498, Train Acc: 0.3359, Valid Loss: 1.1045, Valid Acc: 0.3594\n",
            "Epoch 3/20, Train Loss: 1.1307, Train Acc: 0.3438, Valid Loss: 1.0889, Valid Acc: 0.4219\n",
            "Epoch 4/20, Train Loss: 1.0989, Train Acc: 0.4141, Valid Loss: 1.0616, Valid Acc: 0.4531\n",
            "Epoch 5/20, Train Loss: 1.0594, Train Acc: 0.4297, Valid Loss: 1.0348, Valid Acc: 0.4375\n",
            "Epoch 6/20, Train Loss: 1.0368, Train Acc: 0.4766, Valid Loss: 1.0130, Valid Acc: 0.5000\n",
            "Epoch 7/20, Train Loss: 1.0146, Train Acc: 0.5234, Valid Loss: 0.9864, Valid Acc: 0.5781\n",
            "Epoch 8/20, Train Loss: 0.9775, Train Acc: 0.6172, Valid Loss: 0.9599, Valid Acc: 0.6406\n",
            "Epoch 9/20, Train Loss: 0.9468, Train Acc: 0.6250, Valid Loss: 0.9311, Valid Acc: 0.6562\n",
            "Epoch 10/20, Train Loss: 0.8976, Train Acc: 0.6953, Valid Loss: 0.9022, Valid Acc: 0.6875\n",
            "Epoch 11/20, Train Loss: 0.8603, Train Acc: 0.7031, Valid Loss: 0.8717, Valid Acc: 0.6875\n",
            "Epoch 12/20, Train Loss: 0.8045, Train Acc: 0.7266, Valid Loss: 0.8409, Valid Acc: 0.7031\n",
            "Epoch 13/20, Train Loss: 0.7918, Train Acc: 0.7656, Valid Loss: 0.8110, Valid Acc: 0.7344\n",
            "Epoch 14/20, Train Loss: 0.7073, Train Acc: 0.7812, Valid Loss: 0.7802, Valid Acc: 0.7031\n",
            "Epoch 15/20, Train Loss: 0.6928, Train Acc: 0.7734, Valid Loss: 0.7468, Valid Acc: 0.7031\n",
            "Epoch 16/20, Train Loss: 0.6734, Train Acc: 0.7891, Valid Loss: 0.7122, Valid Acc: 0.7500\n",
            "Epoch 17/20, Train Loss: 0.6264, Train Acc: 0.8047, Valid Loss: 0.6785, Valid Acc: 0.7656\n",
            "Epoch 18/20, Train Loss: 0.6000, Train Acc: 0.7891, Valid Loss: 0.6488, Valid Acc: 0.7656\n",
            "Epoch 19/20, Train Loss: 0.5691, Train Acc: 0.8281, Valid Loss: 0.6253, Valid Acc: 0.7656\n",
            "Epoch 20/20, Train Loss: 0.5421, Train Acc: 0.8516, Valid Loss: 0.6050, Valid Acc: 0.7500\n",
            "Best Validation Accuracy: 0.7656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "TIMESTEPS = 30\n",
        "CHANNELS = 6\n",
        "patch_size = 7\n",
        "embedding_dim = 128\n",
        "num_transformer_layers = 6\n",
        "num_heads = 8\n",
        "dim_feedforward = 128\n",
        "dropout = 0.1\n",
        "num_classes = 4\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=patch_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_transformer_layers=num_transformer_layers,\n",
        "    num_heads=num_heads,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    dropout=dropout,\n",
        "    num_classes=num_classes,\n",
        "    pos_encoding_type='sineSPE',\n",
        "    spe_params={'max_len': TIMESTEPS}\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for inputs, labels in data_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss = running_loss / len(data_loader.dataset)\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    valid_loss = running_loss / len(data_loader.dataset)\n",
        "    valid_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return valid_loss, valid_acc\n",
        "\n",
        "# Training and Evaluation Loop\n",
        "num_epochs = 20  # Adjust as needed\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
        "          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if valid_accuracy > best_valid_acc:\n",
        "        best_valid_acc = valid_accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34GXwW8DBnzG",
        "outputId": "83de774e-ec27-401b-a9a6-709f64899856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.2174, Train Acc: 0.2500, Valid Loss: 1.1347, Valid Acc: 0.3750\n",
            "Epoch 2/20, Train Loss: 1.1618, Train Acc: 0.3516, Valid Loss: 1.1233, Valid Acc: 0.2812\n",
            "Epoch 3/20, Train Loss: 1.1508, Train Acc: 0.2969, Valid Loss: 1.1194, Valid Acc: 0.2969\n",
            "Epoch 4/20, Train Loss: 1.1149, Train Acc: 0.3438, Valid Loss: 1.1112, Valid Acc: 0.3125\n",
            "Epoch 5/20, Train Loss: 1.1081, Train Acc: 0.3359, Valid Loss: 1.0925, Valid Acc: 0.3125\n",
            "Epoch 6/20, Train Loss: 1.0875, Train Acc: 0.4141, Valid Loss: 1.0654, Valid Acc: 0.5000\n",
            "Epoch 7/20, Train Loss: 1.0427, Train Acc: 0.5547, Valid Loss: 1.0390, Valid Acc: 0.5938\n",
            "Epoch 8/20, Train Loss: 1.0257, Train Acc: 0.5391, Valid Loss: 1.0139, Valid Acc: 0.5781\n",
            "Epoch 9/20, Train Loss: 0.9856, Train Acc: 0.6016, Valid Loss: 0.9838, Valid Acc: 0.5625\n",
            "Epoch 10/20, Train Loss: 0.9490, Train Acc: 0.6562, Valid Loss: 0.9451, Valid Acc: 0.5781\n",
            "Epoch 11/20, Train Loss: 0.8928, Train Acc: 0.6875, Valid Loss: 0.9071, Valid Acc: 0.6250\n",
            "Epoch 12/20, Train Loss: 0.8673, Train Acc: 0.6875, Valid Loss: 0.8741, Valid Acc: 0.6250\n",
            "Epoch 13/20, Train Loss: 0.8148, Train Acc: 0.6875, Valid Loss: 0.8370, Valid Acc: 0.6406\n",
            "Epoch 14/20, Train Loss: 0.7449, Train Acc: 0.7734, Valid Loss: 0.8035, Valid Acc: 0.6406\n",
            "Epoch 15/20, Train Loss: 0.7219, Train Acc: 0.7422, Valid Loss: 0.7749, Valid Acc: 0.6250\n",
            "Epoch 16/20, Train Loss: 0.6885, Train Acc: 0.7578, Valid Loss: 0.7499, Valid Acc: 0.6250\n",
            "Epoch 17/20, Train Loss: 0.6406, Train Acc: 0.7734, Valid Loss: 0.7213, Valid Acc: 0.6406\n",
            "Epoch 18/20, Train Loss: 0.6127, Train Acc: 0.7656, Valid Loss: 0.6971, Valid Acc: 0.6406\n",
            "Epoch 19/20, Train Loss: 0.5805, Train Acc: 0.7656, Valid Loss: 0.6737, Valid Acc: 0.6875\n",
            "Epoch 20/20, Train Loss: 0.5650, Train Acc: 0.7891, Valid Loss: 0.6531, Valid Acc: 0.6719\n",
            "Best Validation Accuracy: 0.6875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "TIMESTEPS = 30\n",
        "CHANNELS = 6\n",
        "patch_size = 7\n",
        "embedding_dim = 128\n",
        "num_transformer_layers = 6\n",
        "num_heads = 8\n",
        "dim_feedforward = 128\n",
        "dropout = 0.1\n",
        "num_classes = 4\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=patch_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_transformer_layers=num_transformer_layers,\n",
        "    num_heads=num_heads,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    dropout=dropout,\n",
        "    num_classes=num_classes,\n",
        "    pos_encoding_type='sineSPE',\n",
        "    spe_params={'max_len': TIMESTEPS}\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for inputs, labels in data_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss = running_loss / len(data_loader.dataset)\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    valid_loss = running_loss / len(data_loader.dataset)\n",
        "    valid_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return valid_loss, valid_acc\n",
        "\n",
        "# Training and Evaluation Loop\n",
        "num_epochs = 20\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
        "          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if valid_accuracy > best_valid_acc:\n",
        "        best_valid_acc = valid_accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wu69Z0LcAeT",
        "outputId": "c07c3e3a-b7be-4fb9-de27-80df114a9a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.2058, Train Acc: 0.2344, Valid Loss: 1.1697, Valid Acc: 0.2344\n",
            "Epoch 2/20, Train Loss: 1.1803, Train Acc: 0.2344, Valid Loss: 1.1434, Valid Acc: 0.2188\n",
            "Epoch 3/20, Train Loss: 1.1436, Train Acc: 0.3047, Valid Loss: 1.1222, Valid Acc: 0.2500\n",
            "Epoch 4/20, Train Loss: 1.1292, Train Acc: 0.3281, Valid Loss: 1.0970, Valid Acc: 0.3438\n",
            "Epoch 5/20, Train Loss: 1.0987, Train Acc: 0.4219, Valid Loss: 1.0694, Valid Acc: 0.4375\n",
            "Epoch 6/20, Train Loss: 1.0743, Train Acc: 0.5078, Valid Loss: 1.0399, Valid Acc: 0.5156\n",
            "Epoch 7/20, Train Loss: 1.0430, Train Acc: 0.5547, Valid Loss: 1.0088, Valid Acc: 0.5938\n",
            "Epoch 8/20, Train Loss: 0.9965, Train Acc: 0.6484, Valid Loss: 0.9759, Valid Acc: 0.6094\n",
            "Epoch 9/20, Train Loss: 0.9623, Train Acc: 0.6250, Valid Loss: 0.9420, Valid Acc: 0.6406\n",
            "Epoch 10/20, Train Loss: 0.9373, Train Acc: 0.6875, Valid Loss: 0.9087, Valid Acc: 0.6719\n",
            "Epoch 11/20, Train Loss: 0.8977, Train Acc: 0.6328, Valid Loss: 0.8719, Valid Acc: 0.6719\n",
            "Epoch 12/20, Train Loss: 0.8450, Train Acc: 0.6641, Valid Loss: 0.8333, Valid Acc: 0.6562\n",
            "Epoch 13/20, Train Loss: 0.8001, Train Acc: 0.7031, Valid Loss: 0.7958, Valid Acc: 0.6562\n",
            "Epoch 14/20, Train Loss: 0.7577, Train Acc: 0.7578, Valid Loss: 0.7622, Valid Acc: 0.6562\n",
            "Epoch 15/20, Train Loss: 0.7406, Train Acc: 0.7422, Valid Loss: 0.7306, Valid Acc: 0.6719\n",
            "Epoch 16/20, Train Loss: 0.6677, Train Acc: 0.7656, Valid Loss: 0.7001, Valid Acc: 0.6562\n",
            "Epoch 17/20, Train Loss: 0.6426, Train Acc: 0.7500, Valid Loss: 0.6677, Valid Acc: 0.6719\n",
            "Epoch 18/20, Train Loss: 0.6052, Train Acc: 0.7422, Valid Loss: 0.6361, Valid Acc: 0.6875\n",
            "Epoch 19/20, Train Loss: 0.5614, Train Acc: 0.7578, Valid Loss: 0.6118, Valid Acc: 0.7188\n",
            "Epoch 20/20, Train Loss: 0.5520, Train Acc: 0.8125, Valid Loss: 0.5893, Valid Acc: 0.7031\n",
            "Best Validation Accuracy: 0.7188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class TemporalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=24):\n",
        "        super(TemporalPositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return self.pe[:seq_len, :].unsqueeze(0).expand(x.size(0), -1, -1)\n"
      ],
      "metadata": {
        "id": "qJUoBvFFpmdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_layers, num_heads, dim_feedforward, dropout, num_classes):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Temporal positional encoding (T-PE)\n",
        "        self.pos_encoding = TemporalPositionalEncoding(d_model=embedding_dim, max_len=input_timesteps)\n",
        "\n",
        "        # Transformer encoder layers\n",
        "        self.transformer_layers = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape input for Conv1D\n",
        "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, in_channels, seq_len)\n",
        "\n",
        "        # Apply patch embedding\n",
        "        x = self.patch_embedding(x)  # (batch_size, embedding_dim, seq_len // patch_size)\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len // patch_size, embedding_dim)\n",
        "\n",
        "        # Apply positional encoding\n",
        "        x = x + self.pos_encoding(x)\n",
        "\n",
        "        # Apply transformer layers\n",
        "        x = self.transformer_layers(x)\n",
        "\n",
        "        # Pool the sequence output and apply classifier\n",
        "        x = x.mean(dim=1)\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "AWFtUGHIprCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the temporal positional encoding\n",
        "temporal_pos_embed = TemporalPositionalEncoding(d_model=128, max_len=24)  # Sequence length is 24 (from your data)\n",
        "\n",
        "# Initialize the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=30,\n",
        "    in_channels=6,  # Since it's univariate\n",
        "    patch_size=8,  # This can be adjusted based on your experiments\n",
        "    embedding_dim=128,\n",
        "    num_layers=4,\n",
        "    num_heads=8,\n",
        "    dim_feedforward=512,\n",
        "    dropout=0.1,\n",
        "    num_classes= 4\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "wZUU3ZqrptrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and validation loop\n",
        "num_epochs = 20\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_preds = []\n",
        "    train_labels = []\n",
        "    for batch in train_loader:\n",
        "        inputs, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    valid_preds = []\n",
        "    valid_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            inputs, labels = batch\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item() * inputs.size(0)\n",
        "            valid_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "            valid_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    valid_loss /= len(valid_loader.dataset)\n",
        "    valid_acc = accuracy_score(valid_labels, valid_preds)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
        "          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MnRD8wqpwzi",
        "outputId": "46b458c6-1948-4f57-b8bc-044679105eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.6776, Train Acc: 0.2422, Valid Loss: 1.3279, Valid Acc: 0.2656\n",
            "Epoch 2/20, Train Loss: 1.2170, Train Acc: 0.3281, Valid Loss: 1.2223, Valid Acc: 0.2188\n",
            "Epoch 3/20, Train Loss: 1.1794, Train Acc: 0.2344, Valid Loss: 1.2292, Valid Acc: 0.1875\n",
            "Epoch 4/20, Train Loss: 1.1614, Train Acc: 0.3047, Valid Loss: 1.0804, Valid Acc: 0.5000\n",
            "Epoch 5/20, Train Loss: 1.0568, Train Acc: 0.5391, Valid Loss: 1.0625, Valid Acc: 0.4688\n",
            "Epoch 6/20, Train Loss: 1.0258, Train Acc: 0.5703, Valid Loss: 0.9830, Valid Acc: 0.4531\n",
            "Epoch 7/20, Train Loss: 0.9328, Train Acc: 0.5391, Valid Loss: 0.8661, Valid Acc: 0.5469\n",
            "Epoch 8/20, Train Loss: 0.7692, Train Acc: 0.7266, Valid Loss: 0.8112, Valid Acc: 0.5938\n",
            "Epoch 9/20, Train Loss: 0.6354, Train Acc: 0.7500, Valid Loss: 0.7668, Valid Acc: 0.6562\n",
            "Epoch 10/20, Train Loss: 0.5569, Train Acc: 0.7812, Valid Loss: 0.6809, Valid Acc: 0.6250\n",
            "Epoch 11/20, Train Loss: 0.5113, Train Acc: 0.7891, Valid Loss: 0.6201, Valid Acc: 0.6875\n",
            "Epoch 12/20, Train Loss: 0.4241, Train Acc: 0.7969, Valid Loss: 0.5976, Valid Acc: 0.7188\n",
            "Epoch 13/20, Train Loss: 0.3525, Train Acc: 0.8984, Valid Loss: 0.5317, Valid Acc: 0.7812\n",
            "Epoch 14/20, Train Loss: 0.2748, Train Acc: 0.9375, Valid Loss: 0.5169, Valid Acc: 0.7344\n",
            "Epoch 15/20, Train Loss: 0.2062, Train Acc: 0.9609, Valid Loss: 0.4938, Valid Acc: 0.7656\n",
            "Epoch 16/20, Train Loss: 0.1867, Train Acc: 0.9609, Valid Loss: 0.5190, Valid Acc: 0.7188\n",
            "Epoch 17/20, Train Loss: 0.1365, Train Acc: 0.9844, Valid Loss: 0.4862, Valid Acc: 0.7812\n",
            "Epoch 18/20, Train Loss: 0.1171, Train Acc: 0.9688, Valid Loss: 0.5436, Valid Acc: 0.7500\n",
            "Epoch 19/20, Train Loss: 0.0947, Train Acc: 0.9922, Valid Loss: 0.5566, Valid Acc: 0.7969\n",
            "Epoch 20/20, Train Loss: 0.0712, Train Acc: 1.0000, Valid Loss: 0.5773, Valid Acc: 0.8438\n",
            "Best Validation Accuracy: 0.8438\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}