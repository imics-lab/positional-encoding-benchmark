{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Directory where datasets will be downloaded and extracted\n",
        "DATA_DIR = 'datasets'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def download_dataset(dataset_name, url):\n",
        "    \"\"\"\n",
        "    Downloads and extracts a zip file containing the dataset.\n",
        "    \"\"\"\n",
        "    zip_path = os.path.join(DATA_DIR, f\"{dataset_name}.zip\")\n",
        "    extract_path = os.path.join(DATA_DIR, dataset_name)\n",
        "\n",
        "    # Download the dataset\n",
        "    print(\"Starting download...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "    # Extract the zip file\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    # Remove the zip file after extraction\n",
        "    os.remove(zip_path)\n",
        "    print(\"Extraction complete.\")\n",
        "    return extract_path\n",
        "\n",
        "def load_emg_data(directory):\n",
        "    \"\"\"\n",
        "    Loads and cleans EMG data by reading files, skipping metadata rows, and selecting numeric data.\n",
        "    Assumes that each file is in a plain text format with space-delimited values.\n",
        "    \"\"\"\n",
        "    data_frames = []\n",
        "\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file_name in files:\n",
        "            if file_name.endswith('.txt'):\n",
        "                file_path = os.path.join(root, file_name)\n",
        "                try:\n",
        "                    # Read the file, skipping rows with metadata or headers\n",
        "                    df = pd.read_csv(file_path, sep='\\s+', header=None, skiprows=3)\n",
        "\n",
        "                    # Check if the DataFrame has valid numeric content\n",
        "                    if df.empty or not pd.to_numeric(df.iloc[:, 1], errors='coerce').notna().all():\n",
        "                        continue\n",
        "\n",
        "                    # Convert all data to numeric\n",
        "                    df = df.apply(pd.to_numeric, errors='coerce')  # Ensure all data is numeric\n",
        "                    data_frames.append(df)\n",
        "                except pd.errors.ParserError:\n",
        "                    continue\n",
        "\n",
        "    if not data_frames:\n",
        "        raise ValueError(\"No valid numeric data files found for concatenation.\")\n",
        "\n",
        "    # Concatenate and reset index\n",
        "    full_df = pd.concat(data_frames, axis=0).reset_index(drop=True)\n",
        "    full_df.columns = [f'feature_{i}' for i in range(full_df.shape[1] - 1)] + ['label']  # Assign last column as label\n",
        "\n",
        "    print(\"Data loaded successfully.\")\n",
        "    return full_df\n",
        "\n",
        "def preprocess_data(df, batch_size=64, time_steps=30):\n",
        "    \"\"\"\n",
        "    Preprocesses the EMG data:\n",
        "    - Reshapes the dataset into sequences with time steps and features.\n",
        "    - Splits into train, validation, and test sets.\n",
        "    - Normalizes the features.\n",
        "    - Converts them into PyTorch tensors.\n",
        "    - Creates DataLoaders for supervised tasks (with labels).\n",
        "    \"\"\"\n",
        "    print(\"Starting preprocessing...\")\n",
        "    # Assume the last column is the label and the rest are features\n",
        "    X = df.iloc[:, :-1]  # All columns except the last\n",
        "    y = df.iloc[:, -1]   # Last column as labels\n",
        "    num_features = X.shape[1]\n",
        "\n",
        "    # Adjust the data to make it divisible by the number of time steps\n",
        "    total_samples = (len(X) // time_steps) * time_steps\n",
        "    X, y = X.iloc[:total_samples], y.iloc[:total_samples]\n",
        "\n",
        "    # Reshape X into (num_sequences, time_steps, num_features)\n",
        "    X = X.values.reshape(-1, time_steps, num_features)\n",
        "    y = y.values.reshape(-1, time_steps)[:, 0]  # Use the first label of each sequence as the label for that sequence\n",
        "\n",
        "    # Split into training, validation, and test sets\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train.reshape(-1, num_features)).reshape(-1, time_steps, num_features)\n",
        "    X_valid = scaler.transform(X_valid.reshape(-1, num_features)).reshape(-1, time_steps, num_features)\n",
        "    X_test = scaler.transform(X_test.reshape(-1, num_features)).reshape(-1, time_steps, num_features)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.long)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    # Print the shapes of the datasets\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    print(f\"\\nEach set details:\")\n",
        "    print(f\"- Training set: {X_train.shape[0]} sequences, {time_steps} time steps, {num_features} features per step.\")\n",
        "    print(f\"- Validation set: {X_valid.shape[0]} sequences, {time_steps} time steps, {num_features} features per step.\")\n",
        "    print(f\"- Test set: {X_test.shape[0]} sequences, {time_steps} time steps, {num_features} features per step.\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test\n",
        "\n",
        "def main():\n",
        "    dataset_name = 'EMG_Gestures'\n",
        "    dataset_url = 'https://archive.ics.uci.edu/static/public/481/emg+data+for+gestures.zip'\n",
        "\n",
        "    # Step 1: Download and Extract\n",
        "    extract_path = download_dataset(dataset_name, dataset_url)\n",
        "\n",
        "    # Step 2: Load Data\n",
        "    df = load_emg_data(extract_path)  # Ensure labels are in the last column\n",
        "\n",
        "    # Step 3: Preprocess Data into Training, Validation, and Testing sets\n",
        "    train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test = preprocess_data(df)\n",
        "\n",
        "    # Step 4: Output the Number of Classes (If Labels are present)\n",
        "    n_classes = len(torch.unique(y_train))\n",
        "    print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhY7xL6OHwPb",
        "outputId": "1fa0b5f6-2c10-4dd6-e8a4-e3f7b44e1573"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting download...\n",
            "Download complete.\n",
            "Extracting dataset...\n",
            "Extraction complete.\n",
            "Data loaded successfully.\n",
            "Starting preprocessing...\n",
            "X_train shape: torch.Size([84754, 30, 9]), y_train shape: torch.Size([84754])\n",
            "X_valid shape: torch.Size([28252, 30, 9]), y_valid shape: torch.Size([28252])\n",
            "X_test shape: torch.Size([28252, 30, 9]), y_test shape: torch.Size([28252])\n",
            "\n",
            "Each set details:\n",
            "- Training set: 84754 sequences, 30 time steps, 9 features per step.\n",
            "- Validation set: 28252 sequences, 30 time steps, 9 features per step.\n",
            "- Test set: 28252 sequences, 30 time steps, 9 features per step.\n",
            "Number of classes: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDT8m0OYWpQj",
        "outputId": "6a8071da-2e9f-4d71-d18e-c9123388fa8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QAP9J-zwWMbt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchinfo import summary\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j1tFm3BmWMbu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class TimeSeriesPatchEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Calculate the number of patches, adjusting for padding if necessary\n",
        "        # Ceiling division to account for padding\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "        self.padding = (\n",
        "            self.num_patches * patch_size\n",
        "        ) - input_timesteps  # Calculate padding length\n",
        "\n",
        "        self.conv_layer = nn.Conv1d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embedding_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "        )\n",
        "\n",
        "        self.class_token_embeddings = nn.Parameter(\n",
        "            torch.randn((1, 1, embedding_dim), requires_grad=True)\n",
        "        )\n",
        "        self.position_embeddings = PositionalEncoding(embedding_dim, dropout=0.1, max_len=input_timesteps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pad the input sequence if necessary\n",
        "        if self.padding > 0:\n",
        "            x = nn.functional.pad(x, (0, 0, 0, self.padding))  # Pad the second to last dimension, which is input_timesteps\n",
        "\n",
        "        # We use a Conv1d layer to generate the patch embeddings\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, timesteps)\n",
        "        conv_output = self.conv_layer(x)\n",
        "        conv_output = conv_output.permute(0, 2, 1)  # (batch, timesteps, features)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n",
        "        output = torch.cat((class_tokens, conv_output), dim=1)\n",
        "\n",
        "        output = self.position_embeddings(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def pos_encoding(self, q_len, d_model, normalize=True):\n",
        "        pe = torch.zeros(q_len, d_model)\n",
        "        position = torch.arange(0, q_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        if normalize:\n",
        "            pe = pe - pe.mean()\n",
        "            pe = pe / (pe.std() * 10)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_encoding(q_len = x.size(1), d_model = x.size(2))\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(LearnedPositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "def get_pos_encoder(pos_encoding):\n",
        "    if pos_encoding == 'fixed':\n",
        "        return PositionalEncoding\n",
        "    elif pos_encoding == 'learned':\n",
        "        return LearnedPositionalEncoding\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    dataset_name = 'EMG_Gestures'\n",
        "    dataset_url = 'https://archive.ics.uci.edu/static/public/481/emg+data+for+gestures.zip'\n",
        "\n",
        "    # Step 1: Download and Extract\n",
        "    extract_path = download_dataset(dataset_name, dataset_url)\n",
        "\n",
        "    # Step 2: Load Data\n",
        "    df = load_emg_data(extract_path)  # Ensure labels are in the last column\n",
        "\n",
        "    # Step 3: Preprocess Data into Training, Validation, and Testing sets\n",
        "    train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test = preprocess_data(df)\n",
        "\n",
        "    # Step 4: Output the Number of Classes (If Labels are present)\n",
        "    n_classes = len(torch.unique(y_train))\n",
        "    print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "    return train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test, n_classes\n",
        "\n",
        "# Call the main function and unpack returned values\n",
        "train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test, n_classes = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJPj55NHQRLN",
        "outputId": "e96a3e3d-315c-4eb5-e8e8-8ceda118d407"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting download...\n",
            "Download complete.\n",
            "Extracting dataset...\n",
            "Extraction complete.\n",
            "Data loaded successfully.\n",
            "Starting preprocessing...\n",
            "X_train shape: torch.Size([84754, 30, 9]), y_train shape: torch.Size([84754])\n",
            "X_valid shape: torch.Size([28252, 30, 9]), y_valid shape: torch.Size([28252])\n",
            "X_test shape: torch.Size([28252, 30, 9]), y_test shape: torch.Size([28252])\n",
            "\n",
            "Each set details:\n",
            "- Training set: 84754 sequences, 30 time steps, 9 features per step.\n",
            "- Validation set: 28252 sequences, 30 time steps, 9 features per step.\n",
            "- Test set: 28252 sequences, 30 time steps, 9 features per step.\n",
            "Number of classes: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8MKozUTWMbu",
        "outputId": "48ea0811-ad1e-4ae3-cb56-bf133e386824"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================================================================================================\n",
              "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
              "=================================================================================================================================================\n",
              "TimeSeriesPatchEmbeddingLayer (TimeSeriesPatchEmbeddingLayer)     [64, 30, 9]          [64, 5, 72]          72                   True\n",
              "├─Conv1d (conv_layer)                                             [64, 9, 32]          [64, 72, 4]          5,256                True\n",
              "├─PositionalEncoding (position_embeddings)                        [64, 5, 72]          [64, 5, 72]          --                   --\n",
              "│    └─Dropout (dropout)                                          [64, 5, 72]          [64, 5, 72]          --                   --\n",
              "=================================================================================================================================================\n",
              "Total params: 5,328\n",
              "Trainable params: 5,328\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.35\n",
              "=================================================================================================================================================\n",
              "Input size (MB): 0.07\n",
              "Forward/backward pass size (MB): 0.15\n",
              "Params size (MB): 0.02\n",
              "Estimated Total Size (MB): 0.24\n",
              "================================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "random_instances, random_labels = next(iter(train_loader))\n",
        "random_instance = random_instances[0]\n",
        "\n",
        "BATCH_SIZE = random_instances.shape[0]\n",
        "TIMESTEPS = random_instance.shape[0]\n",
        "CHANNELS = random_instance.shape[1]\n",
        "PATCH_SIZE = 8\n",
        "\n",
        "patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    embedding_dim=CHANNELS * PATCH_SIZE,\n",
        "    input_timesteps=TIMESTEPS,\n",
        ")\n",
        "\n",
        "patch_embeddings = patch_embedding_layer(random_instances)\n",
        "patch_embeddings.shape\n",
        "\n",
        "summary(\n",
        "    model=patch_embedding_layer,\n",
        "    # (batch_size, input_channels, input_timesteps)\n",
        "    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s30BKFOOWMbu"
      },
      "outputs": [],
      "source": [
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps)\n",
        "\n",
        "        # Calculate the number of patches\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        # Setting batch_first=True to accommodate inputs with batch dimension first\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n",
        "\n",
        "        # Feedforward layer\n",
        "        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n",
        "        # Classifier Head\n",
        "        self.classifier = nn.Linear(dim_feedforward, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, in_channels, input_timesteps)\n",
        "\n",
        "        # Get patch embeddings\n",
        "        x = self.patch_embedding(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n",
        "\n",
        "        # Apply Transformer Encoder with batch first\n",
        "        x = self.transformer_encoder(x)  # Output shape: (batch_size, num_patches + 1, embedding_dim)\n",
        "\n",
        "        # Use the output corresponding to the class token for classification\n",
        "        class_token_output = x[:, 0, :]  # Select the class token for each item in the batch\n",
        "\n",
        "        # Feedforward layer\n",
        "        x = self.ff_layer(class_token_output)\n",
        "\n",
        "        # Classifier head\n",
        "        output = self.classifier(x)  # Output shape: (batch_size, num_classes)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3tGq9kmWMbu",
        "outputId": "449d7672-9c42-43ed-a040-d7c1a17c32cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=======================================================================================================================================\n",
              "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
              "=======================================================================================================================================\n",
              "TimeSeriesTransformer (TimeSeriesTransformer)           [64, 30, 9]          [64, 8]              --                   True\n",
              "├─TimeSeriesPatchEmbeddingLayer (patch_embedding)       [64, 30, 9]          [64, 5, 32]          32                   True\n",
              "│    └─Conv1d (conv_layer)                              [64, 9, 32]          [64, 32, 4]          2,336                True\n",
              "│    └─PositionalEncoding (position_embeddings)         [64, 5, 32]          [64, 5, 32]          --                   --\n",
              "│    │    └─Dropout (dropout)                           [64, 5, 32]          [64, 5, 32]          --                   --\n",
              "├─TransformerEncoder (transformer_encoder)              [64, 5, 32]          [64, 5, 32]          --                   True\n",
              "│    └─ModuleList (layers)                              --                   --                   --                   True\n",
              "│    │    └─TransformerEncoderLayer (0)                 [64, 5, 32]          [64, 5, 32]          12,704               True\n",
              "│    │    └─TransformerEncoderLayer (1)                 [64, 5, 32]          [64, 5, 32]          12,704               True\n",
              "│    │    └─TransformerEncoderLayer (2)                 [64, 5, 32]          [64, 5, 32]          12,704               True\n",
              "│    │    └─TransformerEncoderLayer (3)                 [64, 5, 32]          [64, 5, 32]          12,704               True\n",
              "├─Linear (ff_layer)                                     [64, 32]             [64, 128]            4,224                True\n",
              "├─Linear (classifier)                                   [64, 128]            [64, 8]              1,032                True\n",
              "=======================================================================================================================================\n",
              "Total params: 58,440\n",
              "Trainable params: 58,440\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 3.11\n",
              "=======================================================================================================================================\n",
              "Input size (MB): 0.07\n",
              "Forward/backward pass size (MB): 2.43\n",
              "Params size (MB): 0.17\n",
              "Estimated Total Size (MB): 2.66\n",
              "======================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler to reduce the learning rate by the specified step size and factor (gamma) every step_size epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "summary(\n",
        "    model=model,\n",
        "    input_size=(BATCH_SIZE, TIMESTEPS, CHANNELS),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smirk3qsWMbu",
        "outputId": "57a7dbf1-3324-4c1e-83c8-62cd3f67af32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: New best model saved with validation accuracy: 0.6358\n",
            "Epoch 1, Train Loss: 0.9649, Train Acc: 0.6312, Val Loss: 0.8375, Val Acc: 0.6358\n",
            "Epoch 2: New best model saved with validation accuracy: 0.6451\n",
            "Epoch 2, Train Loss: 0.8504, Train Acc: 0.6329, Val Loss: 0.7736, Val Acc: 0.6451\n",
            "Epoch 3, Train Loss: 0.8088, Train Acc: 0.6383, Val Loss: 0.7702, Val Acc: 0.6322\n",
            "Epoch 4: New best model saved with validation accuracy: 0.6507\n",
            "Epoch 4, Train Loss: 0.7817, Train Acc: 0.6396, Val Loss: 0.7261, Val Acc: 0.6507\n",
            "Epoch 5, Train Loss: 0.7636, Train Acc: 0.6417, Val Loss: 0.7153, Val Acc: 0.6455\n",
            "Epoch 6, Train Loss: 0.7525, Train Acc: 0.6414, Val Loss: 0.7165, Val Acc: 0.6317\n",
            "Epoch 7: New best model saved with validation accuracy: 0.6547\n",
            "Epoch 7, Train Loss: 0.7409, Train Acc: 0.6441, Val Loss: 0.6965, Val Acc: 0.6547\n",
            "Epoch 8, Train Loss: 0.7332, Train Acc: 0.6453, Val Loss: 0.6925, Val Acc: 0.6471\n",
            "Epoch 9, Train Loss: 0.7298, Train Acc: 0.6450, Val Loss: 0.6969, Val Acc: 0.6536\n",
            "Epoch 10, Train Loss: 0.7238, Train Acc: 0.6470, Val Loss: 0.6868, Val Acc: 0.6440\n",
            "Epoch 11: New best model saved with validation accuracy: 0.6565\n",
            "Epoch 11, Train Loss: 0.7188, Train Acc: 0.6478, Val Loss: 0.6908, Val Acc: 0.6565\n",
            "Epoch 12, Train Loss: 0.7145, Train Acc: 0.6485, Val Loss: 0.7019, Val Acc: 0.6404\n",
            "Epoch 13, Train Loss: 0.7111, Train Acc: 0.6497, Val Loss: 0.6846, Val Acc: 0.6460\n",
            "Epoch 14, Train Loss: 0.7094, Train Acc: 0.6503, Val Loss: 0.6765, Val Acc: 0.6503\n",
            "Epoch 15: New best model saved with validation accuracy: 0.6626\n",
            "Epoch 15, Train Loss: 0.7086, Train Acc: 0.6481, Val Loss: 0.6760, Val Acc: 0.6626\n",
            "Epoch 16, Train Loss: 0.7072, Train Acc: 0.6477, Val Loss: 0.6727, Val Acc: 0.6518\n",
            "Epoch 17, Train Loss: 0.7026, Train Acc: 0.6483, Val Loss: 0.6637, Val Acc: 0.6598\n",
            "Epoch 18, Train Loss: 0.6989, Train Acc: 0.6505, Val Loss: 0.6714, Val Acc: 0.6438\n"
          ]
        }
      ],
      "source": [
        "# Model, loss function, and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Number of epochs\n",
        "n_epochs = 20\n",
        "\n",
        "# Initialize variables for tracking the best model\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    train_losses = []\n",
        "    train_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Training loop\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "        predictions = model(inputs)  # Forward pass\n",
        "        loss = criterion(predictions, labels)  # Calculate loss\n",
        "\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Count the number of correct predictions\n",
        "        train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = np.mean(train_losses)\n",
        "    train_acc = train_correct / total\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        validation_losses = []\n",
        "        validation_correct = 0\n",
        "        total_val = 0\n",
        "\n",
        "        for inputs, labels in valid_loader:\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            validation_losses.append(loss.item())\n",
        "\n",
        "            validation_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "        validation_loss = np.mean(validation_losses)\n",
        "        validation_acc = validation_correct / total_val\n",
        "\n",
        "    # Check if this is the best model so far\n",
        "    if validation_acc > best_validation_acc:\n",
        "        best_validation_acc = validation_acc\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {validation_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {validation_loss:.4f}, Val Acc: {validation_acc:.4f}')\n",
        "\n",
        "# Loading the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyotm-hrWMbv",
        "outputId": "4c1392ce-a0df-4083-e8ba-87e45de557ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.81      0.76     18209\n",
            "           1       0.55      0.45      0.50      1625\n",
            "           2       0.52      0.50      0.51      1633\n",
            "           3       0.52      0.36      0.42      1652\n",
            "           4       0.50      0.63      0.56      1680\n",
            "           5       0.62      0.03      0.06      1639\n",
            "           6       0.55      0.52      0.53      1715\n",
            "           7       0.50      0.08      0.14        99\n",
            "\n",
            "    accuracy                           0.67     28252\n",
            "   macro avg       0.56      0.42      0.43     28252\n",
            "weighted avg       0.66      0.67      0.64     28252\n",
            "\n",
            "Confusion matrix:\n",
            "[[14679   598   719   512  1009    27   659     6]\n",
            " [  887   738     0     0     0     0     0     0]\n",
            " [  797     0   811    16     5     0     4     0]\n",
            " [ 1033     0    12   588     6     1    12     0]\n",
            " [  612     0     2     2  1059     0     5     0]\n",
            " [ 1531     0     1     0    47    48    12     0]\n",
            " [  797     0    24     3     3     1   885     2]\n",
            " [   65     0     2     3     1     0    20     8]]\n"
          ]
        }
      ],
      "source": [
        "# Prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    Y_pred_prob = model(X_test)\n",
        "\n",
        "Y_pred = Y_pred_prob.argmax(1)\n",
        "\n",
        "print(classification_report(y_test, Y_pred))\n",
        "confusion = confusion_matrix(y_test, Y_pred)\n",
        "print(f\"Confusion matrix:\\n{confusion}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "LhQeXQWnWMbv",
        "outputId": "e3778b13-52d2-46b7-8e49-7f8ac65a50ec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Visualize the confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Prediction\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encodings"
      ],
      "metadata": {
        "id": "PHiMwsYvVAzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rwyM_sG-KTN",
        "outputId": "11fbe508-9200-47ee-fe51-38bb0b93daa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "import pandas as pd\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Scl(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.relative_bias_table = nn.Parameter(torch.zeros((2 * self.seq_len - 1), num_heads))\n",
        "        coords = torch.meshgrid((torch.arange(1), torch.arange(self.seq_len)))\n",
        "        coords = torch.flatten(torch.stack(coords), 1)\n",
        "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
        "        relative_coords[1] += self.seq_len - 1\n",
        "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
        "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
        "        self.register_buffer(\"relative_index\", relative_index)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, 8))\n",
        "        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=1 * self.seq_len, w=1 * self.seq_len)\n",
        "        attn = attn + relative_bias\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Vec(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.Er = nn.Parameter(torch.randn(self.seq_len, int(emb_size / num_heads)))\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.tril(torch.ones(self.seq_len, self.seq_len))\n",
        "                .unsqueeze(0).unsqueeze(0)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        QEr = torch.matmul(q, self.Er.transpose(0, 1))\n",
        "        Srel = self.skew(QEr)\n",
        "\n",
        "        attn = torch.matmul(q, k)\n",
        "        attn = (attn + Srel) * self.scale\n",
        "\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "    def skew(self, QEr):\n",
        "        padded = nn.functional.pad(QEr, (1, 0))\n",
        "        batch_size, num_heads, num_rows, num_cols = padded.shape\n",
        "        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
        "        Srel = reshaped[:, :, 1:, :]\n",
        "        return Srel\n"
      ],
      "metadata": {
        "id": "yD7K5F3D-Hji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TUPEConfig:\n",
        "    num_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    d_model: int = 128\n",
        "    d_head: int = 0\n",
        "    max_len: int = 256\n",
        "    dropout: float = 0.1\n",
        "    expansion_factor: int = 1\n",
        "    relative_bias: bool = True\n",
        "    bidirectional_bias: bool = True\n",
        "    num_buckets: int = 32\n",
        "    max_distance: int = 128\n",
        "\n",
        "    def __post_init__(self):\n",
        "        d_head, remainder = divmod(self.d_model, self.num_heads)\n",
        "        assert remainder == 0, \"`d_model` should be divisible by `num_heads`\"\n",
        "        self.d_head = d_head"
      ],
      "metadata": {
        "id": "B2e6n_Yj_38p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_relative_position_bucket(\n",
        "    relative_position, bidirectional, num_buckets, max_distance\n",
        "):\n",
        "    \"\"\"\n",
        "    from https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/modeling_t5.py\n",
        "    \"\"\"\n",
        "    relative_buckets = 0\n",
        "    if bidirectional:\n",
        "        num_buckets //= 2\n",
        "        relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n",
        "        relative_position = torch.abs(relative_position)\n",
        "    else:\n",
        "        relative_position = -torch.min(\n",
        "            relative_position, torch.zeros_like(relative_position)\n",
        "        )\n",
        "    # now relative_position is in the range [0, inf)\n",
        "\n",
        "    # half of the buckets are for exact increments in positions\n",
        "    max_exact = num_buckets // 2\n",
        "    is_small = relative_position < max_exact\n",
        "\n",
        "    # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n",
        "    relative_postion_if_large = max_exact + (\n",
        "        torch.log(relative_position.float() / max_exact)\n",
        "        / math.log(max_distance / max_exact)\n",
        "        * (num_buckets - max_exact)\n",
        "    ).to(torch.long)\n",
        "    relative_postion_if_large = torch.min(\n",
        "        relative_postion_if_large,\n",
        "        torch.full_like(relative_postion_if_large, num_buckets - 1),\n",
        "    )\n",
        "\n",
        "    relative_buckets += torch.where(\n",
        "        is_small, relative_position, relative_postion_if_large\n",
        "    )\n",
        "    return relative_buckets\n",
        "\n",
        "\n",
        "def get_relative_positions(\n",
        "    seq_len, bidirectional=True, num_buckets=32, max_distance=128\n",
        "):\n",
        "    x = torch.arange(seq_len)[None, :]\n",
        "    y = torch.arange(seq_len)[:, None]\n",
        "    relative_positions = _get_relative_position_bucket(\n",
        "        x - y, bidirectional, num_buckets, max_distance\n",
        "    )\n",
        "    return relative_positions"
      ],
      "metadata": {
        "id": "1QbW7ihvBByl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TUPEMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config: TUPEConfig, pos_embed: nn.Module) -> None:\n",
        "        super().__init__()\n",
        "        self.max_len = config.max_len\n",
        "        self.num_heads = config.num_heads\n",
        "        self.num_buckets = config.num_buckets\n",
        "        self.max_distance = config.max_distance\n",
        "        self.bidirectional = config.bidirectional_bias\n",
        "        self.scale = math.sqrt(2 * config.d_head)\n",
        "\n",
        "        self.pos_embed = pos_embed\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # kqv in one pass\n",
        "        self.pos_kq = nn.Linear(config.d_model, 2 * config.d_model, bias=False)\n",
        "        self.tok_kqv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n",
        "\n",
        "        self.relative_bias = config.relative_bias\n",
        "        if config.relative_bias:\n",
        "            self.bias = nn.Embedding(config.max_len * 2, config.num_heads)\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        pos_embed = self.pos_embed(seq_len).repeat(batch_size, 1, 1)\n",
        "        # pos_embed.shape == (batch_size, seq_len, d_model)\n",
        "        pos_key, pos_query = self.pos_kq(pos_embed).chunk(2, dim=-1)\n",
        "        pos_key = pos_key.view(batch_size, seq_len, self.num_heads, -1).permute(\n",
        "            0, 2, 3, 1\n",
        "        )\n",
        "        # pos_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        pos_query = pos_query.view(batch_size, seq_len, self.num_heads, -1).transpose(\n",
        "            1, 2\n",
        "        )\n",
        "        # pos_query.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        pos_attn = torch.matmul(pos_query, pos_key)\n",
        "        # pos_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        tok_key, tok_query, tok_value = self.tok_kqv(x).chunk(3, dim=-1)\n",
        "        tok_key = tok_key.view(batch_size, seq_len, self.num_heads, -1).permute(\n",
        "            0, 2, 3, 1\n",
        "        )\n",
        "        # tok_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        tok_query = tok_query.view(batch_size, seq_len, self.num_heads, -1).transpose(\n",
        "            1, 2\n",
        "        )\n",
        "        tok_value = tok_value.view(batch_size, seq_len, self.num_heads, -1).transpose(\n",
        "            1, 2\n",
        "        )\n",
        "        # tok_qv.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        tok_attn = torch.matmul(tok_query, tok_key)\n",
        "        # tok_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        attn = (tok_attn + pos_attn) / self.scale\n",
        "        if self.relative_bias:\n",
        "            relative_positions = get_relative_positions(\n",
        "                seq_len, self.bidirectional, self.num_buckets, self.max_distance\n",
        "            ).to(attn.device)\n",
        "            # relative_positions.shape == (seq_len, seq_len)\n",
        "            bias = self.bias(relative_positions + self.max_len)\n",
        "            # bias.shape == (seq_len, seq_len, num_heads)\n",
        "            bias = bias.permute(2, 0, 1).unsqueeze(0)\n",
        "            # bias.shape == (1, num_heads, seq_len, seq_len)\n",
        "            attn = attn + bias\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "        out = torch.matmul(attn, tok_value)\n",
        "        # out.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
        "        # out.shape == (batch_size, seq_len, d_model)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "9uW0SNXu_oda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import math\n",
        "from torchsummary import summary\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def pos_encoding(self, q_len, d_model, normalize=True):\n",
        "        pe = torch.zeros(q_len, d_model)\n",
        "        position = torch.arange(0, q_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        if normalize:\n",
        "            pe = pe - pe.mean()\n",
        "            pe = pe / (pe.std() * 10)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_encoding(q_len=x.size(1), d_model=x.size(2))\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Learned Positional Encoding\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(LearnedPositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Time Series Patch Embedding Layer\n",
        "class TimeSeriesPatchEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size, embedding_dim, input_timesteps, pos_encoding='fixed'):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "        self.padding = (self.num_patches * patch_size) - input_timesteps\n",
        "\n",
        "        self.conv_layer = nn.Conv1d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=embedding_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "        )\n",
        "\n",
        "        self.class_token_embeddings = nn.Parameter(\n",
        "            torch.randn((1, 1, embedding_dim), requires_grad=True)\n",
        "        )\n",
        "\n",
        "        # Instantiate the positional encoding\n",
        "        pos_encoder_class = get_pos_encoder(pos_encoding)\n",
        "        self.position_embeddings = pos_encoder_class(embedding_dim, dropout=0.1, max_len=input_timesteps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.padding > 0:\n",
        "            x = nn.functional.pad(x, (0, 0, 0, self.padding))\n",
        "\n",
        "        x = x.permute(0, 2, 1)\n",
        "        conv_output = self.conv_layer(x)\n",
        "        conv_output = conv_output.permute(0, 2, 1)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        class_tokens = self.class_token_embeddings.expand(batch_size, -1, -1)\n",
        "        output = torch.cat((class_tokens, conv_output), dim=1)\n",
        "\n",
        "        output = self.position_embeddings(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Get Positional Encoding\n",
        "def get_pos_encoder(pos_encoding):\n",
        "    if pos_encoding == 'fixed':\n",
        "        return PositionalEncoding\n",
        "    elif pos_encoding == 'learned':\n",
        "        return LearnedPositionalEncoding\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, pos_encoding='fixed', num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2, attention_type='absolute'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embedding = TimeSeriesPatchEmbeddingLayer(in_channels, patch_size, embedding_dim, input_timesteps, pos_encoding)\n",
        "        self.num_patches = -(-input_timesteps // patch_size)\n",
        "\n",
        "        if attention_type == 'relative_scl':\n",
        "            self.attention_layer = Attention_Rel_Scl(embedding_dim, num_heads, self.num_patches + 1, dropout)\n",
        "        elif attention_type == 'relative_vec':\n",
        "            self.attention_layer = Attention_Rel_Vec(embedding_dim, num_heads, self.num_patches + 1, dropout)\n",
        "        elif attention_type == 'tupe':\n",
        "            self.attention_layer = TUPEMultiHeadAttention(tupe_config, pos_embed)\n",
        "        else:\n",
        "            self.attention_layer = Attention(embedding_dim, num_heads, dropout)\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n",
        "\n",
        "        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n",
        "        self.classifier = nn.Linear(dim_feedforward, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        x = self.attention_layer(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        class_token_output = x[:, 0, :]\n",
        "        x = self.ff_layer(class_token_output)\n",
        "        output = self.classifier(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "random_instances, random_labels = next(iter(train_loader))\n",
        "random_instance = random_instances[0]\n",
        "\n",
        "BATCH_SIZE = random_instances.shape[0]\n",
        "TIMESTEPS = random_instance.shape[0]\n",
        "CHANNELS = random_instance.shape[1]\n",
        "PATCH_SIZE = 8\n",
        "\n",
        "patch_embedding_layer = TimeSeriesPatchEmbeddingLayer(\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    embedding_dim=CHANNELS * PATCH_SIZE,\n",
        "    input_timesteps=TIMESTEPS,\n",
        ")\n",
        "\n",
        "patch_embeddings = patch_embedding_layer(random_instances)\n",
        "patch_embeddings.shape\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    pos_encoding='fixed',\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "    val_loss = total_loss / len(valid_loader)\n",
        "    val_acc = correct / total_val\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n",
        "    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n",
        "\n",
        "    if val_acc > best_validation_acc:\n",
        "        best_validation_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeNs_BKxfx0i",
        "outputId": "92649eaa-637c-41d3-bc09-60636e9d9a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1.4569, Train Acc: 0.2266\n",
            "Epoch 1: New best model saved with validation accuracy: 0.2656\n",
            "Epoch 1, Val Loss: 1.3812, Val Acc: 0.2656\n",
            "Epoch 1, Train Loss: 1.3724, Train Acc: 0.3125\n",
            "Epoch 2: New best model saved with validation accuracy: 0.3750\n",
            "Epoch 2, Val Loss: 1.3303, Val Acc: 0.3750\n",
            "Epoch 1, Train Loss: 1.3081, Train Acc: 0.3750\n",
            "Epoch 3: New best model saved with validation accuracy: 0.4688\n",
            "Epoch 3, Val Loss: 1.2838, Val Acc: 0.4688\n",
            "Epoch 1, Train Loss: 1.2549, Train Acc: 0.4766\n",
            "Epoch 4, Val Loss: 1.2554, Val Acc: 0.4531\n",
            "Epoch 1, Train Loss: 1.1858, Train Acc: 0.4922\n",
            "Epoch 5: New best model saved with validation accuracy: 0.5000\n",
            "Epoch 5, Val Loss: 1.2337, Val Acc: 0.5000\n",
            "Epoch 1, Train Loss: 1.1196, Train Acc: 0.5703\n",
            "Epoch 6, Val Loss: 1.1852, Val Acc: 0.4844\n",
            "Epoch 1, Train Loss: 1.0403, Train Acc: 0.6250\n",
            "Epoch 7, Val Loss: 1.1052, Val Acc: 0.4844\n",
            "Epoch 1, Train Loss: 1.0115, Train Acc: 0.5703\n",
            "Epoch 8: New best model saved with validation accuracy: 0.5156\n",
            "Epoch 8, Val Loss: 1.0460, Val Acc: 0.5156\n",
            "Epoch 1, Train Loss: 0.9153, Train Acc: 0.6875\n",
            "Epoch 9: New best model saved with validation accuracy: 0.5781\n",
            "Epoch 9, Val Loss: 1.0105, Val Acc: 0.5781\n",
            "Epoch 1, Train Loss: 0.8802, Train Acc: 0.6484\n",
            "Epoch 10, Val Loss: 0.9813, Val Acc: 0.5781\n",
            "Loaded best model for testing or further use.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-b06036bd6b7c>:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "import pandas as pd\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Scl(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.relative_bias_table = nn.Parameter(torch.zeros((2 * self.seq_len - 1), num_heads))\n",
        "        coords = torch.meshgrid((torch.arange(1), torch.arange(self.seq_len)))\n",
        "        coords = torch.flatten(torch.stack(coords), 1)\n",
        "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
        "        relative_coords[1] += self.seq_len - 1\n",
        "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
        "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
        "        self.register_buffer(\"relative_index\", relative_index)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        attn = torch.matmul(q, k) * self.scale\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "\n",
        "        relative_bias = self.relative_bias_table.gather(0, self.relative_index.repeat(1, 8))\n",
        "        relative_bias = rearrange(relative_bias, '(h w) c -> 1 c h w', h=1 * self.seq_len, w=1 * self.seq_len)\n",
        "        attn = attn + relative_bias\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Attention_Rel_Vec(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, seq_len, dropout):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = emb_size ** -0.5\n",
        "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
        "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
        "\n",
        "        self.Er = nn.Parameter(torch.randn(self.seq_len, int(emb_size / num_heads)))\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.tril(torch.ones(self.seq_len, self.seq_len))\n",
        "                .unsqueeze(0).unsqueeze(0)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        QEr = torch.matmul(q, self.Er.transpose(0, 1))\n",
        "        Srel = self.skew(QEr)\n",
        "\n",
        "        attn = torch.matmul(q, k)\n",
        "        attn = (attn + Srel) * self.scale\n",
        "\n",
        "        attn = nn.functional.softmax(attn, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2)\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "    def skew(self, QEr):\n",
        "        padded = nn.functional.pad(QEr, (1, 0))\n",
        "        batch_size, num_heads, num_rows, num_cols = padded.shape\n",
        "        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
        "        Srel = reshaped[:, :, 1:, :]\n",
        "        return Srel\n"
      ],
      "metadata": {
        "id": "ESOXdCAqaM_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class tAPE(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n",
        "        super(tAPE, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin((position * div_term) * (d_model / max_len))\n",
        "        pe[:, 1::2] = torch.cos((position * div_term) * (d_model / max_len))\n",
        "        pe = scale_factor * pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class AbsolutePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n",
        "        super(AbsolutePositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = scale_factor * pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1024):\n",
        "        super(LearnablePositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.pe = nn.Parameter(torch.empty(max_len, d_model))\n",
        "        nn.init.uniform_(self.pe, -0.02, 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(1), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "dplhsyvE-jR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_encoder(pos_encoding):\n",
        "    if pos_encoding == 'fixed':\n",
        "        return PositionalEncoding\n",
        "    elif pos_encoding == 'learned':\n",
        "        return LearnedPositionalEncoding\n",
        "    elif pos_encoding == 'tAPE':\n",
        "        return tAPE\n",
        "    elif pos_encoding == 'absolute':\n",
        "        return AbsolutePositionalEncoding\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown positional encoding type: {pos_encoding}\")\n"
      ],
      "metadata": {
        "id": "q9sb14F6-kc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    pos_encoding='tAPE',\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "    val_loss = total_loss / len(valid_loader)\n",
        "    val_acc = correct / total_val\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 20\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n",
        "    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n",
        "\n",
        "    if val_acc > best_validation_acc:\n",
        "        best_validation_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egoVdYVX-mq4",
        "outputId": "153161d4-30bf-4814-d416-16b129503be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1.4311, Train Acc: 0.2266\n",
            "Epoch 1: New best model saved with validation accuracy: 0.3281\n",
            "Epoch 1, Val Loss: 1.3790, Val Acc: 0.3281\n",
            "Epoch 1, Train Loss: 1.3741, Train Acc: 0.2891\n",
            "Epoch 2: New best model saved with validation accuracy: 0.3438\n",
            "Epoch 2, Val Loss: 1.3855, Val Acc: 0.3438\n",
            "Epoch 1, Train Loss: 1.3881, Train Acc: 0.2812\n",
            "Epoch 3, Val Loss: 1.3670, Val Acc: 0.3281\n",
            "Epoch 1, Train Loss: 1.3506, Train Acc: 0.2969\n",
            "Epoch 4, Val Loss: 1.3426, Val Acc: 0.2969\n",
            "Epoch 1, Train Loss: 1.3434, Train Acc: 0.3125\n",
            "Epoch 5: New best model saved with validation accuracy: 0.5312\n",
            "Epoch 5, Val Loss: 1.3242, Val Acc: 0.5312\n",
            "Epoch 1, Train Loss: 1.2763, Train Acc: 0.4688\n",
            "Epoch 6, Val Loss: 1.3288, Val Acc: 0.3125\n",
            "Epoch 1, Train Loss: 1.2578, Train Acc: 0.3984\n",
            "Epoch 7, Val Loss: 1.2999, Val Acc: 0.3438\n",
            "Epoch 1, Train Loss: 1.2230, Train Acc: 0.4219\n",
            "Epoch 8, Val Loss: 1.2241, Val Acc: 0.4375\n",
            "Epoch 1, Train Loss: 1.1781, Train Acc: 0.5078\n",
            "Epoch 9: New best model saved with validation accuracy: 0.5625\n",
            "Epoch 9, Val Loss: 1.1763, Val Acc: 0.5625\n",
            "Epoch 1, Train Loss: 1.1084, Train Acc: 0.5000\n",
            "Epoch 10, Val Loss: 1.1406, Val Acc: 0.5625\n",
            "Epoch 1, Train Loss: 1.0463, Train Acc: 0.5859\n",
            "Epoch 11: New best model saved with validation accuracy: 0.5781\n",
            "Epoch 11, Val Loss: 1.1462, Val Acc: 0.5781\n",
            "Epoch 1, Train Loss: 1.0315, Train Acc: 0.5938\n",
            "Epoch 12, Val Loss: 1.0759, Val Acc: 0.5781\n",
            "Epoch 1, Train Loss: 0.9647, Train Acc: 0.5469\n",
            "Epoch 13: New best model saved with validation accuracy: 0.5938\n",
            "Epoch 13, Val Loss: 0.9922, Val Acc: 0.5938\n",
            "Epoch 1, Train Loss: 0.9757, Train Acc: 0.5781\n",
            "Epoch 14: New best model saved with validation accuracy: 0.6406\n",
            "Epoch 14, Val Loss: 0.9370, Val Acc: 0.6406\n",
            "Epoch 1, Train Loss: 0.9089, Train Acc: 0.6406\n",
            "Epoch 15, Val Loss: 0.9249, Val Acc: 0.6406\n",
            "Epoch 1, Train Loss: 0.9021, Train Acc: 0.6562\n",
            "Epoch 16, Val Loss: 0.9153, Val Acc: 0.6250\n",
            "Epoch 1, Train Loss: 0.8903, Train Acc: 0.6172\n",
            "Epoch 17, Val Loss: 0.8661, Val Acc: 0.6250\n",
            "Epoch 1, Train Loss: 0.8292, Train Acc: 0.6328\n",
            "Epoch 18, Val Loss: 0.8550, Val Acc: 0.5938\n",
            "Epoch 1, Train Loss: 0.8129, Train Acc: 0.6172\n",
            "Epoch 19, Val Loss: 0.8455, Val Acc: 0.6250\n",
            "Epoch 1, Train Loss: 0.7899, Train Acc: 0.6797\n",
            "Epoch 20, Val Loss: 0.7958, Val Acc: 0.6250\n",
            "Loaded best model for testing or further use.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-f4ea5e1fc41b>:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=32,\n",
        "    pos_encoding='fixed',\n",
        "    num_transformer_layers=4,\n",
        "    num_heads=4,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.2,\n",
        "    num_classes=n_classes,\n",
        "    attention_type= 'relative_vec',\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            train_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_acc = train_correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "    val_loss = total_loss / len(valid_loader)\n",
        "    val_acc = correct / total_val\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 20\n",
        "best_validation_acc = 0.0\n",
        "best_model_path = 'best_model_v2.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_model(model, train_loader, loss_fn, optimizer, num_epochs=1)\n",
        "    val_loss, val_acc = evaluate_model(model, valid_loader, loss_fn)\n",
        "\n",
        "    if val_acc > best_validation_acc:\n",
        "        best_validation_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Epoch {epoch+1}: New best model saved with validation accuracy: {val_acc:.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "    lr_scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print('Loaded best model for testing or further use.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzFfH3xX8fE_",
        "outputId": "1664db50-327c-4398-b12b-42ac5221b14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1.4358, Train Acc: 0.2422\n",
            "Epoch 1: New best model saved with validation accuracy: 0.4062\n",
            "Epoch 1, Val Loss: 1.3170, Val Acc: 0.4062\n",
            "Epoch 1, Train Loss: 1.3419, Train Acc: 0.3359\n",
            "Epoch 2: New best model saved with validation accuracy: 0.4844\n",
            "Epoch 2, Val Loss: 1.2702, Val Acc: 0.4844\n",
            "Epoch 1, Train Loss: 1.2761, Train Acc: 0.3984\n",
            "Epoch 3, Val Loss: 1.1982, Val Acc: 0.4688\n",
            "Epoch 1, Train Loss: 1.2040, Train Acc: 0.4609\n",
            "Epoch 4: New best model saved with validation accuracy: 0.5625\n",
            "Epoch 4, Val Loss: 1.1222, Val Acc: 0.5625\n",
            "Epoch 1, Train Loss: 1.1163, Train Acc: 0.5156\n",
            "Epoch 5: New best model saved with validation accuracy: 0.6406\n",
            "Epoch 5, Val Loss: 1.0442, Val Acc: 0.6406\n",
            "Epoch 1, Train Loss: 1.0287, Train Acc: 0.5391\n",
            "Epoch 6, Val Loss: 0.9654, Val Acc: 0.5781\n",
            "Epoch 1, Train Loss: 0.9441, Train Acc: 0.5938\n",
            "Epoch 7, Val Loss: 0.9070, Val Acc: 0.5781\n",
            "Epoch 1, Train Loss: 0.9253, Train Acc: 0.6328\n",
            "Epoch 8, Val Loss: 0.8725, Val Acc: 0.6094\n",
            "Epoch 1, Train Loss: 0.8254, Train Acc: 0.6641\n",
            "Epoch 9, Val Loss: 0.8457, Val Acc: 0.5625\n",
            "Epoch 1, Train Loss: 0.8502, Train Acc: 0.6406\n",
            "Epoch 10, Val Loss: 0.8050, Val Acc: 0.5781\n",
            "Epoch 1, Train Loss: 0.7219, Train Acc: 0.6641\n",
            "Epoch 11, Val Loss: 0.7731, Val Acc: 0.6406\n",
            "Epoch 1, Train Loss: 0.7538, Train Acc: 0.6562\n",
            "Epoch 12: New best model saved with validation accuracy: 0.7031\n",
            "Epoch 12, Val Loss: 0.7465, Val Acc: 0.7031\n",
            "Epoch 1, Train Loss: 0.7807, Train Acc: 0.6719\n",
            "Epoch 13, Val Loss: 0.7470, Val Acc: 0.6562\n",
            "Epoch 1, Train Loss: 0.6735, Train Acc: 0.7266\n",
            "Epoch 14, Val Loss: 0.7429, Val Acc: 0.6406\n",
            "Epoch 1, Train Loss: 0.6290, Train Acc: 0.7500\n",
            "Epoch 15, Val Loss: 0.7430, Val Acc: 0.6719\n",
            "Epoch 1, Train Loss: 0.6027, Train Acc: 0.7500\n",
            "Epoch 16, Val Loss: 0.7474, Val Acc: 0.6875\n",
            "Epoch 1, Train Loss: 0.6253, Train Acc: 0.7422\n",
            "Epoch 17, Val Loss: 0.7569, Val Acc: 0.7031\n",
            "Epoch 1, Train Loss: 0.5292, Train Acc: 0.8047\n",
            "Epoch 18, Val Loss: 0.7459, Val Acc: 0.7031\n",
            "Epoch 1, Train Loss: 0.6126, Train Acc: 0.7188\n",
            "Epoch 19: New best model saved with validation accuracy: 0.7188\n",
            "Epoch 19, Val Loss: 0.7281, Val Acc: 0.7188\n",
            "Epoch 1, Train Loss: 0.4622, Train Acc: 0.8203\n",
            "Epoch 20, Val Loss: 0.7186, Val Acc: 0.7031\n",
            "Loaded best model for testing or further use.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-014626430eba>:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TUPE"
      ],
      "metadata": {
        "id": "ntF6fok4KzMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class TUPEConfig:\n",
        "    def __init__(self):\n",
        "        self.d_model = 128\n",
        "        self.num_heads = 8\n",
        "        self.dim_feedforward = 512\n",
        "        self.dropout = 0.1\n",
        "        self.max_len = 5000\n",
        "        self.num_buckets = 32\n",
        "        self.max_distance = 128\n",
        "        self.relative_bias = True\n",
        "        self.bidirectional_bias = True\n",
        "\n",
        "class TUPEMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config: TUPEConfig, pos_embed: nn.Module) -> None:\n",
        "        super().__init__()\n",
        "        self.max_len = config.max_len\n",
        "        self.num_heads = config.num_heads\n",
        "        self.num_buckets = config.num_buckets\n",
        "        self.max_distance = config.max_distance\n",
        "        self.bidirectional = config.bidirectional_bias\n",
        "        self.scale = math.sqrt(2 * config.d_model // config.num_heads)\n",
        "\n",
        "        self.pos_embed = pos_embed\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # kqv in one pass\n",
        "        self.pos_kq = nn.Linear(config.d_model, 2 * config.d_model, bias=False)\n",
        "        self.tok_kqv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n",
        "\n",
        "        self.relative_bias = config.relative_bias\n",
        "        if config.relative_bias:\n",
        "            self.bias = nn.Embedding(config.max_len * 2, config.num_heads)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        pos_embed = self.pos_embed(seq_len).repeat(batch_size, 1, 1)\n",
        "        # pos_embed.shape == (batch_size, seq_len, d_model)\n",
        "        pos_key, pos_query = self.pos_kq(pos_embed).chunk(2, dim=-1)\n",
        "        pos_key = pos_key.view(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        # pos_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        pos_query = pos_query.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        # pos_query.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        pos_attn = torch.matmul(pos_query, pos_key)\n",
        "        # pos_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        tok_key, tok_query, tok_value = self.tok_kqv(x).chunk(3, dim=-1)\n",
        "        tok_key = tok_key.view(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "        # tok_key.shape == (batch_size, num_heads, d_head, seq_len)\n",
        "        tok_query = tok_query.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        tok_value = tok_value.view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        # tok_qv.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        tok_attn = torch.matmul(tok_query, tok_key)\n",
        "        # tok_attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        attn = (tok_attn + pos_attn) / self.scale\n",
        "        if self.relative_bias:\n",
        "            relative_positions = self.get_relative_positions(seq_len)\n",
        "            # relative_positions.shape == (seq_len, seq_len)\n",
        "            bias = self.bias(relative_positions + self.max_len)\n",
        "            # bias.shape == (seq_len, seq_len, num_heads)\n",
        "            bias = bias.permute(2, 0, 1).unsqueeze(0)\n",
        "            # bias.shape == (1, num_heads, seq_len, seq_len)\n",
        "            attn = attn + bias\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        # attn.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "        out = torch.matmul(attn, tok_value)\n",
        "        # out.shape == (batch_size, num_heads, seq_len, d_head)\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
        "        # out.shape == (batch_size, seq_len, d_model)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "    def get_relative_positions(self, seq_len):\n",
        "        # Generate relative position encodings\n",
        "        range_vec = torch.arange(seq_len)\n",
        "        distance_mat = range_vec.unsqueeze(1) - range_vec.unsqueeze(0)\n",
        "        distance_mat_clipped = torch.clamp(distance_mat, -self.max_distance, self.max_distance)\n",
        "        final_mat = distance_mat_clipped + self.max_distance\n",
        "        return final_mat\n"
      ],
      "metadata": {
        "id": "LWu8jg32ORdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(FixedPositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)  # (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        return self.encoding[:, :seq_len, :].detach()\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_layers, num_heads, dim_feedforward, dropout, num_classes, attention_type, pos_encoding):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.attention_type = attention_type\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.pos_encoding = pos_encoding\n",
        "\n",
        "        # Transformer layers with TUPEMultiHeadAttention\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TUPEMultiHeadAttention(\n",
        "                config=TUPEConfig(),\n",
        "                pos_embed=FixedPositionalEncoding(embedding_dim, max_len=input_timesteps)\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(f\"Input to model: {x.shape}\")\n",
        "\n",
        "        # Patch embedding\n",
        "        x = x.permute(0, 2, 1)  # Change shape from (batch_size, seq_len, in_channels) to (batch_size, in_channels, seq_len)\n",
        "        #print(f\"Input to Conv1d: {x.shape}\")\n",
        "\n",
        "        x = self.patch_embedding(x)  # Apply Conv1d\n",
        "        #print(f\"Patch embedding output: {x.shape}\")\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, embedding_dim)\n",
        "        #print(f\"Patch embedding output permuted: {x.shape}\")\n",
        "\n",
        "        # Positional encoding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pos_encoding(seq_len).to(x.device)\n",
        "        #print(f\"Positional encoding output: {x.shape}\")\n",
        "\n",
        "        # Transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "            #print(f\"Transformer layer output: {x.shape}\")\n",
        "\n",
        "        # Classification head\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.classifier(x)\n",
        "        #print(f\"Classifier output: {x.shape}\")\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "TIMESTEPS = 30\n",
        "CHANNELS = 6\n",
        "n_classes = 44\n",
        "\n",
        "# Initialize positional encoding\n",
        "pos_embed = FixedPositionalEncoding(d_model=128, max_len=TIMESTEPS)\n",
        "\n",
        "# Define the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=8,\n",
        "    embedding_dim=128,\n",
        "    num_layers=4,\n",
        "    num_heads=8,\n",
        "    dim_feedforward=512,\n",
        "    dropout=0.1,\n",
        "    num_classes=n_classes,\n",
        "    attention_type='tupe',\n",
        "    pos_encoding=pos_embed,\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training and validation loop\n",
        "num_epochs = 20\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_preds = []\n",
        "    train_labels = []\n",
        "    for batch in train_loader:\n",
        "        inputs, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    valid_preds = []\n",
        "    valid_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            inputs, labels = batch\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item() * inputs.size(0)\n",
        "            valid_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "            valid_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    valid_loss /= len(valid_loader.dataset)\n",
        "    valid_acc = accuracy_score(valid_labels, valid_preds)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
        "          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgP2Ge01OTt0",
        "outputId": "5a4c7414-9752-4535-9ff4-43d8752a1a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 3.1447, Train Acc: 0.1406, Valid Loss: 2.9109, Valid Acc: 0.3594\n",
            "Epoch 2/20, Train Loss: 2.8448, Train Acc: 0.3438, Valid Loss: 2.4550, Valid Acc: 0.3750\n",
            "Epoch 3/20, Train Loss: 2.3032, Train Acc: 0.3281, Valid Loss: 1.7242, Valid Acc: 0.2969\n",
            "Epoch 4/20, Train Loss: 1.5748, Train Acc: 0.3359, Valid Loss: 1.2022, Valid Acc: 0.4219\n",
            "Epoch 5/20, Train Loss: 1.2348, Train Acc: 0.2969, Valid Loss: 1.0896, Valid Acc: 0.4375\n",
            "Epoch 6/20, Train Loss: 1.1160, Train Acc: 0.3750, Valid Loss: 1.1139, Valid Acc: 0.2344\n",
            "Epoch 7/20, Train Loss: 1.1004, Train Acc: 0.3594, Valid Loss: 1.0432, Valid Acc: 0.5781\n",
            "Epoch 8/20, Train Loss: 1.0629, Train Acc: 0.4531, Valid Loss: 0.9556, Valid Acc: 0.5625\n",
            "Epoch 9/20, Train Loss: 0.9763, Train Acc: 0.4922, Valid Loss: 0.9714, Valid Acc: 0.4062\n",
            "Epoch 10/20, Train Loss: 0.9298, Train Acc: 0.4766, Valid Loss: 0.8266, Valid Acc: 0.5312\n",
            "Epoch 11/20, Train Loss: 0.7606, Train Acc: 0.6016, Valid Loss: 0.7787, Valid Acc: 0.6094\n",
            "Epoch 12/20, Train Loss: 0.7598, Train Acc: 0.5703, Valid Loss: 0.7638, Valid Acc: 0.6094\n",
            "Epoch 13/20, Train Loss: 0.6748, Train Acc: 0.6562, Valid Loss: 0.7068, Valid Acc: 0.6094\n",
            "Epoch 14/20, Train Loss: 0.5889, Train Acc: 0.7031, Valid Loss: 0.6520, Valid Acc: 0.6719\n",
            "Epoch 15/20, Train Loss: 0.4999, Train Acc: 0.7578, Valid Loss: 0.6087, Valid Acc: 0.6875\n",
            "Epoch 16/20, Train Loss: 0.4634, Train Acc: 0.7734, Valid Loss: 0.6138, Valid Acc: 0.6875\n",
            "Epoch 17/20, Train Loss: 0.4516, Train Acc: 0.7734, Valid Loss: 0.5685, Valid Acc: 0.6875\n",
            "Epoch 18/20, Train Loss: 0.3962, Train Acc: 0.7891, Valid Loss: 0.5337, Valid Acc: 0.7188\n",
            "Epoch 19/20, Train Loss: 0.3772, Train Acc: 0.8125, Valid Loss: 0.5571, Valid Acc: 0.7031\n",
            "Epoch 20/20, Train Loss: 0.3137, Train Acc: 0.8516, Valid Loss: 0.5213, Valid Acc: 0.7344\n",
            "Best Validation Accuracy: 0.7344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_layers, num_heads, dim_feedforward, dropout, num_classes, pos_encoding_type, spe_params):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Positional encoding\n",
        "        if pos_encoding_type == 'sine':\n",
        "            self.pos_encoding = SineSPE(num_heads=num_heads, in_features=embedding_dim // num_heads, **spe_params)\n",
        "        elif pos_encoding_type == 'conv':\n",
        "            self.pos_encoding = ConvSPE(ndim=1, num_heads=num_heads, in_features=embedding_dim // num_heads, **spe_params)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid positional encoding type\")\n",
        "\n",
        "        # Transformer layers with TUPEMultiHeadAttention\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TUPEMultiHeadAttention(\n",
        "                config=TUPEConfig(),\n",
        "                pos_embed=self.pos_encoding\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch embedding\n",
        "        x = x.permute(0, 2, 1)  # Change shape from (batch_size, seq_len, in_channels) to (batch_size, in_channels, seq_len)\n",
        "        x = self.patch_embedding(x)  # Apply Conv1d\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Positional encoding\n",
        "        seq_len = x.size(1)\n",
        "        pos_enc = self.pos_encoding(seq_len).to(x.device)\n",
        "        x = x + pos_enc\n",
        "\n",
        "        # Transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "pMTQ5_2nyjht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SineSPE(nn.Module):\n",
        "    def __init__(self, num_heads, in_features, max_len=512):\n",
        "        super(SineSPE, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.in_features = in_features\n",
        "        self.max_len = max_len\n",
        "        self.position = nn.Parameter(torch.zeros(1, max_len, in_features))\n",
        "        self.register_buffer('sine', self._generate_sine_encoding())\n",
        "\n",
        "    def _generate_sine_encoding(self):\n",
        "        position = torch.arange(self.max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, self.in_features, 2).float() * -(math.log(10000.0) / self.in_features))\n",
        "        encoding = torch.zeros(self.max_len, self.in_features)\n",
        "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        return encoding\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        return self.sine[:seq_len, :].unsqueeze(0).repeat(self.num_heads, 1, 1)\n",
        "\n",
        "class ConvSPE(nn.Module):\n",
        "    def __init__(self, ndim, num_heads, in_features, kernel_size=7, padding=3):\n",
        "        super(ConvSPE, self).__init__()\n",
        "        self.conv = nn.Conv1d(in_features, in_features, kernel_size=kernel_size, padding=padding)\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        # Generate sinusoidal encoding\n",
        "        x = torch.arange(seq_len).unsqueeze(0).float()\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.conv(x)\n",
        "        x = x.squeeze(0).permute(1, 0).unsqueeze(0).repeat(self.num_heads, 1, 1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_qHLmzj7yl1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import math\n",
        "\n",
        "# Example of positional encoding classes\n",
        "class SineSPE(nn.Module):\n",
        "    def __init__(self, in_features, max_len=512):\n",
        "        super(SineSPE, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.max_len = max_len\n",
        "        self.position = nn.Parameter(torch.zeros(1, max_len, in_features))\n",
        "        self.register_buffer('sine', self._generate_sine_encoding())\n",
        "\n",
        "    def _generate_sine_encoding(self):\n",
        "        position = torch.arange(self.max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, self.in_features, 2).float() * -(math.log(10000.0) / self.in_features))\n",
        "        encoding = torch.zeros(self.max_len, self.in_features)\n",
        "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        return encoding\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        return self.sine[:seq_len, :].unsqueeze(0)  # Shape: (1, seq_len, in_features)\n",
        "\n",
        "\n",
        "class ConvSPE(nn.Module):\n",
        "    def __init__(self, num_heads, in_features, kernel_size=3, num_realizations=1):\n",
        "        super(ConvSPE, self).__init__()\n",
        "        padding = kernel_size // 2  # This ensures that the output size matches the input size if stride=1\n",
        "\n",
        "        # Define a 1D convolutional layer\n",
        "        self.conv = nn.Conv1d(in_features, in_features, kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.in_features = in_features\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_realizations = num_realizations\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x should be of shape (batch_size, seq_len, in_features)\n",
        "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, in_features, seq_len)\n",
        "        x = self.conv(x)  # Apply convolution\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len, in_features)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model Definition\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_transformer_layers=6, num_heads=8, dim_feedforward=128, dropout=0.1, num_classes=2, pos_encoding_type='sine', spe_params={}):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Positional encoding\n",
        "        if pos_encoding_type == 'sineSPE':\n",
        "            self.pos_encoding = SineSPE(embedding_dim, **spe_params)\n",
        "        elif pos_encoding_type == 'convSPE':\n",
        "            self.pos_encoding = ConvSPE(num_heads=num_heads, in_features=embedding_dim, **spe_params)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid positional encoding type\")\n",
        "\n",
        "        # Calculate the number of patches\n",
        "        self.num_patches = -(-input_timesteps // patch_size)  # Ceiling division\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_transformer_layers)\n",
        "\n",
        "        # Feedforward layer\n",
        "        self.ff_layer = nn.Linear(embedding_dim, dim_feedforward)\n",
        "        # Classifier Head\n",
        "        self.classifier = nn.Linear(dim_feedforward, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, in_channels, input_timesteps)\n",
        "\n",
        "        # Get patch embeddings\n",
        "        x = x.permute(0, 2, 1)  # Change shape from (batch_size, in_channels, input_timesteps) to (batch_size, input_timesteps, in_channels)\n",
        "        x = self.patch_embedding(x)  # Apply Conv1d\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, num_patches, embedding_dim)\n",
        "\n",
        "        # Apply positional encoding\n",
        "        seq_len = x.size(1)\n",
        "        pos_enc = self.pos_encoding(seq_len).to(x.device)\n",
        "        x = x + pos_enc\n",
        "\n",
        "        # Apply Transformer Encoder\n",
        "        x = self.transformer_encoder(x)  # Output shape: (batch_size, num_patches, embedding_dim)\n",
        "\n",
        "        # Use the output corresponding to the class token for classification\n",
        "        class_token_output = x[:, 0, :]  # Select the class token for each item in the batch\n",
        "\n",
        "        # Feedforward layer\n",
        "        x = self.ff_layer(class_token_output)\n",
        "\n",
        "        # Classifier head\n",
        "        output = self.classifier(x)  # Output shape: (batch_size, num_classes)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "3dIJpcJ3ypDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "TIMESTEPS = 30\n",
        "CHANNELS = 6\n",
        "patch_size = 7\n",
        "embedding_dim = 128\n",
        "num_transformer_layers = 6\n",
        "num_heads = 8\n",
        "dim_feedforward = 128\n",
        "dropout = 0.1\n",
        "num_classes = 4\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=patch_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_transformer_layers=num_transformer_layers,\n",
        "    num_heads=num_heads,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    dropout=dropout,\n",
        "    num_classes=num_classes,\n",
        "    pos_encoding_type='sineSPE',\n",
        "    spe_params={'max_len': TIMESTEPS}\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for inputs, labels in data_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss = running_loss / len(data_loader.dataset)\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    valid_loss = running_loss / len(data_loader.dataset)\n",
        "    valid_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return valid_loss, valid_acc\n",
        "\n",
        "# Training and Evaluation Loop\n",
        "num_epochs = 20\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
        "          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if valid_accuracy > best_valid_acc:\n",
        "        best_valid_acc = valid_accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlTvPqd6zvXw",
        "outputId": "48f5dbf9-aa11-4814-8da2-880899d8dbb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.2272, Train Acc: 0.2109, Valid Loss: 1.1188, Valid Acc: 0.4375\n",
            "Epoch 2/20, Train Loss: 1.1498, Train Acc: 0.3359, Valid Loss: 1.1045, Valid Acc: 0.3594\n",
            "Epoch 3/20, Train Loss: 1.1307, Train Acc: 0.3438, Valid Loss: 1.0889, Valid Acc: 0.4219\n",
            "Epoch 4/20, Train Loss: 1.0989, Train Acc: 0.4141, Valid Loss: 1.0616, Valid Acc: 0.4531\n",
            "Epoch 5/20, Train Loss: 1.0594, Train Acc: 0.4297, Valid Loss: 1.0348, Valid Acc: 0.4375\n",
            "Epoch 6/20, Train Loss: 1.0368, Train Acc: 0.4766, Valid Loss: 1.0130, Valid Acc: 0.5000\n",
            "Epoch 7/20, Train Loss: 1.0146, Train Acc: 0.5234, Valid Loss: 0.9864, Valid Acc: 0.5781\n",
            "Epoch 8/20, Train Loss: 0.9775, Train Acc: 0.6172, Valid Loss: 0.9599, Valid Acc: 0.6406\n",
            "Epoch 9/20, Train Loss: 0.9468, Train Acc: 0.6250, Valid Loss: 0.9311, Valid Acc: 0.6562\n",
            "Epoch 10/20, Train Loss: 0.8976, Train Acc: 0.6953, Valid Loss: 0.9022, Valid Acc: 0.6875\n",
            "Epoch 11/20, Train Loss: 0.8603, Train Acc: 0.7031, Valid Loss: 0.8717, Valid Acc: 0.6875\n",
            "Epoch 12/20, Train Loss: 0.8045, Train Acc: 0.7266, Valid Loss: 0.8409, Valid Acc: 0.7031\n",
            "Epoch 13/20, Train Loss: 0.7918, Train Acc: 0.7656, Valid Loss: 0.8110, Valid Acc: 0.7344\n",
            "Epoch 14/20, Train Loss: 0.7073, Train Acc: 0.7812, Valid Loss: 0.7802, Valid Acc: 0.7031\n",
            "Epoch 15/20, Train Loss: 0.6928, Train Acc: 0.7734, Valid Loss: 0.7468, Valid Acc: 0.7031\n",
            "Epoch 16/20, Train Loss: 0.6734, Train Acc: 0.7891, Valid Loss: 0.7122, Valid Acc: 0.7500\n",
            "Epoch 17/20, Train Loss: 0.6264, Train Acc: 0.8047, Valid Loss: 0.6785, Valid Acc: 0.7656\n",
            "Epoch 18/20, Train Loss: 0.6000, Train Acc: 0.7891, Valid Loss: 0.6488, Valid Acc: 0.7656\n",
            "Epoch 19/20, Train Loss: 0.5691, Train Acc: 0.8281, Valid Loss: 0.6253, Valid Acc: 0.7656\n",
            "Epoch 20/20, Train Loss: 0.5421, Train Acc: 0.8516, Valid Loss: 0.6050, Valid Acc: 0.7500\n",
            "Best Validation Accuracy: 0.7656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "TIMESTEPS = 30\n",
        "CHANNELS = 6\n",
        "patch_size = 7\n",
        "embedding_dim = 128\n",
        "num_transformer_layers = 6\n",
        "num_heads = 8\n",
        "dim_feedforward = 128\n",
        "dropout = 0.1\n",
        "num_classes = 4\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=patch_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_transformer_layers=num_transformer_layers,\n",
        "    num_heads=num_heads,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    dropout=dropout,\n",
        "    num_classes=num_classes,\n",
        "    pos_encoding_type='sineSPE',\n",
        "    spe_params={'max_len': TIMESTEPS}\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for inputs, labels in data_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss = running_loss / len(data_loader.dataset)\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    valid_loss = running_loss / len(data_loader.dataset)\n",
        "    valid_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return valid_loss, valid_acc\n",
        "\n",
        "# Training and Evaluation Loop\n",
        "num_epochs = 20  # Adjust as needed\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
        "          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if valid_accuracy > best_valid_acc:\n",
        "        best_valid_acc = valid_accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34GXwW8DBnzG",
        "outputId": "83de774e-ec27-401b-a9a6-709f64899856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.2174, Train Acc: 0.2500, Valid Loss: 1.1347, Valid Acc: 0.3750\n",
            "Epoch 2/20, Train Loss: 1.1618, Train Acc: 0.3516, Valid Loss: 1.1233, Valid Acc: 0.2812\n",
            "Epoch 3/20, Train Loss: 1.1508, Train Acc: 0.2969, Valid Loss: 1.1194, Valid Acc: 0.2969\n",
            "Epoch 4/20, Train Loss: 1.1149, Train Acc: 0.3438, Valid Loss: 1.1112, Valid Acc: 0.3125\n",
            "Epoch 5/20, Train Loss: 1.1081, Train Acc: 0.3359, Valid Loss: 1.0925, Valid Acc: 0.3125\n",
            "Epoch 6/20, Train Loss: 1.0875, Train Acc: 0.4141, Valid Loss: 1.0654, Valid Acc: 0.5000\n",
            "Epoch 7/20, Train Loss: 1.0427, Train Acc: 0.5547, Valid Loss: 1.0390, Valid Acc: 0.5938\n",
            "Epoch 8/20, Train Loss: 1.0257, Train Acc: 0.5391, Valid Loss: 1.0139, Valid Acc: 0.5781\n",
            "Epoch 9/20, Train Loss: 0.9856, Train Acc: 0.6016, Valid Loss: 0.9838, Valid Acc: 0.5625\n",
            "Epoch 10/20, Train Loss: 0.9490, Train Acc: 0.6562, Valid Loss: 0.9451, Valid Acc: 0.5781\n",
            "Epoch 11/20, Train Loss: 0.8928, Train Acc: 0.6875, Valid Loss: 0.9071, Valid Acc: 0.6250\n",
            "Epoch 12/20, Train Loss: 0.8673, Train Acc: 0.6875, Valid Loss: 0.8741, Valid Acc: 0.6250\n",
            "Epoch 13/20, Train Loss: 0.8148, Train Acc: 0.6875, Valid Loss: 0.8370, Valid Acc: 0.6406\n",
            "Epoch 14/20, Train Loss: 0.7449, Train Acc: 0.7734, Valid Loss: 0.8035, Valid Acc: 0.6406\n",
            "Epoch 15/20, Train Loss: 0.7219, Train Acc: 0.7422, Valid Loss: 0.7749, Valid Acc: 0.6250\n",
            "Epoch 16/20, Train Loss: 0.6885, Train Acc: 0.7578, Valid Loss: 0.7499, Valid Acc: 0.6250\n",
            "Epoch 17/20, Train Loss: 0.6406, Train Acc: 0.7734, Valid Loss: 0.7213, Valid Acc: 0.6406\n",
            "Epoch 18/20, Train Loss: 0.6127, Train Acc: 0.7656, Valid Loss: 0.6971, Valid Acc: 0.6406\n",
            "Epoch 19/20, Train Loss: 0.5805, Train Acc: 0.7656, Valid Loss: 0.6737, Valid Acc: 0.6875\n",
            "Epoch 20/20, Train Loss: 0.5650, Train Acc: 0.7891, Valid Loss: 0.6531, Valid Acc: 0.6719\n",
            "Best Validation Accuracy: 0.6875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "TIMESTEPS = 30\n",
        "CHANNELS = 6\n",
        "patch_size = 7\n",
        "embedding_dim = 128\n",
        "num_transformer_layers = 6\n",
        "num_heads = 8\n",
        "dim_feedforward = 128\n",
        "dropout = 0.1\n",
        "num_classes = 4\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Instantiate model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=TIMESTEPS,\n",
        "    in_channels=CHANNELS,\n",
        "    patch_size=patch_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_transformer_layers=num_transformer_layers,\n",
        "    num_heads=num_heads,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    dropout=dropout,\n",
        "    num_classes=num_classes,\n",
        "    pos_encoding_type='sineSPE',\n",
        "    spe_params={'max_len': TIMESTEPS}\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for inputs, labels in data_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update metrics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss = running_loss / len(data_loader.dataset)\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    valid_loss = running_loss / len(data_loader.dataset)\n",
        "    valid_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return valid_loss, valid_acc\n",
        "\n",
        "# Training and Evaluation Loop\n",
        "num_epochs = 20\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
        "          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if valid_accuracy > best_valid_acc:\n",
        "        best_valid_acc = valid_accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wu69Z0LcAeT",
        "outputId": "c07c3e3a-b7be-4fb9-de27-80df114a9a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.2058, Train Acc: 0.2344, Valid Loss: 1.1697, Valid Acc: 0.2344\n",
            "Epoch 2/20, Train Loss: 1.1803, Train Acc: 0.2344, Valid Loss: 1.1434, Valid Acc: 0.2188\n",
            "Epoch 3/20, Train Loss: 1.1436, Train Acc: 0.3047, Valid Loss: 1.1222, Valid Acc: 0.2500\n",
            "Epoch 4/20, Train Loss: 1.1292, Train Acc: 0.3281, Valid Loss: 1.0970, Valid Acc: 0.3438\n",
            "Epoch 5/20, Train Loss: 1.0987, Train Acc: 0.4219, Valid Loss: 1.0694, Valid Acc: 0.4375\n",
            "Epoch 6/20, Train Loss: 1.0743, Train Acc: 0.5078, Valid Loss: 1.0399, Valid Acc: 0.5156\n",
            "Epoch 7/20, Train Loss: 1.0430, Train Acc: 0.5547, Valid Loss: 1.0088, Valid Acc: 0.5938\n",
            "Epoch 8/20, Train Loss: 0.9965, Train Acc: 0.6484, Valid Loss: 0.9759, Valid Acc: 0.6094\n",
            "Epoch 9/20, Train Loss: 0.9623, Train Acc: 0.6250, Valid Loss: 0.9420, Valid Acc: 0.6406\n",
            "Epoch 10/20, Train Loss: 0.9373, Train Acc: 0.6875, Valid Loss: 0.9087, Valid Acc: 0.6719\n",
            "Epoch 11/20, Train Loss: 0.8977, Train Acc: 0.6328, Valid Loss: 0.8719, Valid Acc: 0.6719\n",
            "Epoch 12/20, Train Loss: 0.8450, Train Acc: 0.6641, Valid Loss: 0.8333, Valid Acc: 0.6562\n",
            "Epoch 13/20, Train Loss: 0.8001, Train Acc: 0.7031, Valid Loss: 0.7958, Valid Acc: 0.6562\n",
            "Epoch 14/20, Train Loss: 0.7577, Train Acc: 0.7578, Valid Loss: 0.7622, Valid Acc: 0.6562\n",
            "Epoch 15/20, Train Loss: 0.7406, Train Acc: 0.7422, Valid Loss: 0.7306, Valid Acc: 0.6719\n",
            "Epoch 16/20, Train Loss: 0.6677, Train Acc: 0.7656, Valid Loss: 0.7001, Valid Acc: 0.6562\n",
            "Epoch 17/20, Train Loss: 0.6426, Train Acc: 0.7500, Valid Loss: 0.6677, Valid Acc: 0.6719\n",
            "Epoch 18/20, Train Loss: 0.6052, Train Acc: 0.7422, Valid Loss: 0.6361, Valid Acc: 0.6875\n",
            "Epoch 19/20, Train Loss: 0.5614, Train Acc: 0.7578, Valid Loss: 0.6118, Valid Acc: 0.7188\n",
            "Epoch 20/20, Train Loss: 0.5520, Train Acc: 0.8125, Valid Loss: 0.5893, Valid Acc: 0.7031\n",
            "Best Validation Accuracy: 0.7188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class TemporalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=24):\n",
        "        super(TemporalPositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return self.pe[:seq_len, :].unsqueeze(0).expand(x.size(0), -1, -1)\n"
      ],
      "metadata": {
        "id": "qJUoBvFFpmdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, input_timesteps, in_channels, patch_size, embedding_dim, num_layers, num_heads, dim_feedforward, dropout, num_classes):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embedding = nn.Conv1d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Temporal positional encoding (T-PE)\n",
        "        self.pos_encoding = TemporalPositionalEncoding(d_model=embedding_dim, max_len=input_timesteps)\n",
        "\n",
        "        # Transformer encoder layers\n",
        "        self.transformer_layers = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape input for Conv1D\n",
        "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, in_channels, seq_len)\n",
        "\n",
        "        # Apply patch embedding\n",
        "        x = self.patch_embedding(x)  # (batch_size, embedding_dim, seq_len // patch_size)\n",
        "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, seq_len // patch_size, embedding_dim)\n",
        "\n",
        "        # Apply positional encoding\n",
        "        x = x + self.pos_encoding(x)\n",
        "\n",
        "        # Apply transformer layers\n",
        "        x = self.transformer_layers(x)\n",
        "\n",
        "        # Pool the sequence output and apply classifier\n",
        "        x = x.mean(dim=1)\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "AWFtUGHIprCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the temporal positional encoding\n",
        "temporal_pos_embed = TemporalPositionalEncoding(d_model=128, max_len=24)  # Sequence length is 24 (from your data)\n",
        "\n",
        "# Initialize the model\n",
        "model = TimeSeriesTransformer(\n",
        "    input_timesteps=30,\n",
        "    in_channels=6,  # Since it's univariate\n",
        "    patch_size=8,  # This can be adjusted based on your experiments\n",
        "    embedding_dim=128,\n",
        "    num_layers=4,\n",
        "    num_heads=8,\n",
        "    dim_feedforward=512,\n",
        "    dropout=0.1,\n",
        "    num_classes= 4\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "wZUU3ZqrptrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and validation loop\n",
        "num_epochs = 20\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_preds = []\n",
        "    train_labels = []\n",
        "    for batch in train_loader:\n",
        "        inputs, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    valid_preds = []\n",
        "    valid_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            inputs, labels = batch\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item() * inputs.size(0)\n",
        "            valid_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "            valid_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    valid_loss /= len(valid_loader.dataset)\n",
        "    valid_acc = accuracy_score(valid_labels, valid_preds)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
        "          f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f'Best Validation Accuracy: {best_valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MnRD8wqpwzi",
        "outputId": "46b458c6-1948-4f57-b8bc-044679105eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.6776, Train Acc: 0.2422, Valid Loss: 1.3279, Valid Acc: 0.2656\n",
            "Epoch 2/20, Train Loss: 1.2170, Train Acc: 0.3281, Valid Loss: 1.2223, Valid Acc: 0.2188\n",
            "Epoch 3/20, Train Loss: 1.1794, Train Acc: 0.2344, Valid Loss: 1.2292, Valid Acc: 0.1875\n",
            "Epoch 4/20, Train Loss: 1.1614, Train Acc: 0.3047, Valid Loss: 1.0804, Valid Acc: 0.5000\n",
            "Epoch 5/20, Train Loss: 1.0568, Train Acc: 0.5391, Valid Loss: 1.0625, Valid Acc: 0.4688\n",
            "Epoch 6/20, Train Loss: 1.0258, Train Acc: 0.5703, Valid Loss: 0.9830, Valid Acc: 0.4531\n",
            "Epoch 7/20, Train Loss: 0.9328, Train Acc: 0.5391, Valid Loss: 0.8661, Valid Acc: 0.5469\n",
            "Epoch 8/20, Train Loss: 0.7692, Train Acc: 0.7266, Valid Loss: 0.8112, Valid Acc: 0.5938\n",
            "Epoch 9/20, Train Loss: 0.6354, Train Acc: 0.7500, Valid Loss: 0.7668, Valid Acc: 0.6562\n",
            "Epoch 10/20, Train Loss: 0.5569, Train Acc: 0.7812, Valid Loss: 0.6809, Valid Acc: 0.6250\n",
            "Epoch 11/20, Train Loss: 0.5113, Train Acc: 0.7891, Valid Loss: 0.6201, Valid Acc: 0.6875\n",
            "Epoch 12/20, Train Loss: 0.4241, Train Acc: 0.7969, Valid Loss: 0.5976, Valid Acc: 0.7188\n",
            "Epoch 13/20, Train Loss: 0.3525, Train Acc: 0.8984, Valid Loss: 0.5317, Valid Acc: 0.7812\n",
            "Epoch 14/20, Train Loss: 0.2748, Train Acc: 0.9375, Valid Loss: 0.5169, Valid Acc: 0.7344\n",
            "Epoch 15/20, Train Loss: 0.2062, Train Acc: 0.9609, Valid Loss: 0.4938, Valid Acc: 0.7656\n",
            "Epoch 16/20, Train Loss: 0.1867, Train Acc: 0.9609, Valid Loss: 0.5190, Valid Acc: 0.7188\n",
            "Epoch 17/20, Train Loss: 0.1365, Train Acc: 0.9844, Valid Loss: 0.4862, Valid Acc: 0.7812\n",
            "Epoch 18/20, Train Loss: 0.1171, Train Acc: 0.9688, Valid Loss: 0.5436, Valid Acc: 0.7500\n",
            "Epoch 19/20, Train Loss: 0.0947, Train Acc: 0.9922, Valid Loss: 0.5566, Valid Acc: 0.7969\n",
            "Epoch 20/20, Train Loss: 0.0712, Train Acc: 1.0000, Valid Loss: 0.5773, Valid Acc: 0.8438\n",
            "Best Validation Accuracy: 0.8438\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}